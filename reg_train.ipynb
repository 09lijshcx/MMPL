{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce267ab1-f9e5-4da5-a703-dcd447a67fa6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import gc\n",
    "from sklearn import metrics\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "# torch.autograd.set_detect_anomaly(True)\n",
    "CUDA_LAUNCH_BLOCKING=1\n",
    "\n",
    "\n",
    "# fixed random seed for reproduction\n",
    "seed = 10\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.enabled = False\n",
    "print('Random seed :', seed)\n",
    "\n",
    "from collections import OrderedDict\n",
    "@torch.no_grad()\n",
    "def update_ema(ema_model, model, decay=0.9999):\n",
    "    \"\"\"\n",
    "    Step the EMA model towards the current model.\n",
    "    \"\"\"\n",
    "    ema_params = OrderedDict(ema_model.named_parameters())\n",
    "    model_params = OrderedDict(model.named_parameters())\n",
    "\n",
    "    for name, param in model_params.items():\n",
    "        # TODO: Consider applying only to params that require_grad to avoid small numerical changes of pos_embed\n",
    "        ema_params[name].mul_(decay).add_(param.data, alpha=1 - decay)\n",
    "\n",
    "def requires_grad(model, flag=True):\n",
    "    \"\"\"\n",
    "    Set requires_grad flag for all parameters in a model.\n",
    "    \"\"\"\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = flag\n",
    "\n",
    "from sklearn import metrics\n",
    "def compute_AUC(y, pred, n_class=1):\n",
    "    # compute one score\n",
    "    if n_class == 1:\n",
    "        auc = metrics.roc_auc_score(y, pred)\n",
    "\n",
    "    # compute two-class\n",
    "    elif n_class == 2:\n",
    "        # pos = pred[:, 1]\n",
    "        auc = metrics.roc_auc_score(y, pred)\n",
    "    return auc\n",
    "\n",
    "pad_len = 150 # best\n",
    "conf = 10 # best# \n",
    "\n",
    "##########################################\n",
    "#########  construct dataloader  ######### \n",
    "##########################################\n",
    "from reg_lmdb_datapipeline import LMDBDataset \n",
    "lmdb_file = './results/logd_train.lmdb'\n",
    "train_dataset = LMDBDataset(lmdb_file, conf=conf, pad_len=pad_len, mode=\"train\")\n",
    "train_set = DataLoader(train_dataset,\n",
    "                    batch_size=16,  # 16 best\n",
    "                    drop_last=True,\n",
    "                    shuffle=True,\n",
    "                    pin_memory=True,\n",
    "                    num_workers=4,\n",
    "                    # collate_fn=train_dataset.collate_fn,\n",
    "                    worker_init_fn=train_dataset.worker_init_fn\n",
    "                    )\n",
    "\n",
    "lmdb_file = './results/logd_test.lmdb'\n",
    "val_dataset = LMDBDataset(lmdb_file, conf=conf, pad_len=pad_len, mode=\"test\")\n",
    "val_set = DataLoader(val_dataset,\n",
    "                    batch_size=32,\n",
    "                    drop_last=False,\n",
    "                    shuffle=False,\n",
    "                    pin_memory=True,\n",
    "                    num_workers=4,\n",
    "                    # collate_fn=train_dataset.collate_fn,\n",
    "                    worker_init_fn=val_dataset.worker_init_fn\n",
    "                    )\n",
    "\n",
    "\n",
    "##########################################\n",
    "######  build model and optimizer  ####### \n",
    "##########################################\n",
    "dev = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "from CLIP import clip\n",
    "clip_model, preprocess = clip.load(name=\"ViT-B/16\", device=\"cpu\", download_root=\"/home/jovyan/clip_download_root\")\n",
    "from model_zoo import CLIP_Protein\n",
    "model = CLIP_Protein(clip_model, conf, pad_len=pad_len).to(dev)\n",
    "\n",
    "# path = \"/home/jovyan/prompts_learning/trained_weight/A_10_7_cliP_Epoch13_val_auc_0.90789.pth\"\n",
    "# sd = torch.load(path)\n",
    "# model.load_state_dict(sd)\n",
    "# print(\"pre-trained weights loaded...\")\n",
    "\n",
    "from copy import deepcopy\n",
    "ema = deepcopy(model).to(dev)  # Create an EMA of the model for use after training\n",
    "update_ema(ema, model, decay=0)  # Ensure EMA is initialized with synced weights\n",
    "requires_grad(ema, False)\n",
    "ema.eval()\n",
    "\n",
    "# best\n",
    "lr = 5e-5\n",
    "wd = 1e-4\n",
    "\n",
    "print(f'Set of Optimizer: lr:{lr}, weight_decay:{wd}')\n",
    "model_params = [# {'params': model.parameters(), 'lr': lr},\n",
    "                {'params': model.atom_encoder.parameters(), 'lr': lr},\n",
    "                {'params': model.coor_encoder.parameters(), 'lr': lr},\n",
    "                {'params': model.fusion_blocks.parameters(), 'lr': lr},\n",
    "                # {'params': model.head.parameters(), 'lr': 1e-3}, # best for cls task\n",
    "                {'params': model.head.parameters(), 'lr': 1e-4},\n",
    "                {'params': model.prompts_processor.parameters(), 'lr': lr},\n",
    "                {'params': model.ppim.parameters(), 'lr': lr},\n",
    "                # {'params': model.qformer.parameters(), 'lr':1e-5}\n",
    "               ]\n",
    "\n",
    "optims = 'adan'\n",
    "# optims = \"sgd\"\n",
    "if optims == 'adan':\n",
    "    from adan import Adan\n",
    "    optimizer = Adan(model_params, betas=(0.98, 0.92, 0.99),weight_decay=wd, max_grad_norm=0.)\n",
    "elif optims == 'sgd':\n",
    "    optimizer = optim.SGD(model_params, momentum=0.9, weight_decay=wd)\n",
    "elif optims == 'adamw':\n",
    "    optimizer = optim.AdamW(model_params, betas=(0.9, 0.999), weight_decay=wd)\n",
    "elif optims == 'adam':\n",
    "    optimizer = optim.Adam(model_params, betas=(0.9, 0.999), weight_decay=wd)\n",
    "print('Current Optimizer is', optims)\n",
    "\n",
    "\n",
    "###################################################\n",
    "########   build learning rate scheduler   ######## \n",
    "###################################################\n",
    "scheduler1 = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=1./3., total_iters=5) # best\n",
    "scheduler2 = torch.optim.lr_scheduler.ConstantLR(optimizer, factor=1., total_iters=5)\n",
    "scheduler3 = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=15, eta_min=1e-6)\n",
    "scheduler = torch.optim.lr_scheduler.SequentialLR(optimizer, schedulers=[scheduler1, scheduler2, scheduler3], milestones=[5, 10])\n",
    "cur_lr = scheduler.get_last_lr() \n",
    "print(f\"Current learning rate is {cur_lr}.\")\n",
    "\n",
    "\n",
    "\n",
    "##########################################\n",
    "########   build loss criterion   ######## \n",
    "##########################################\n",
    "cri_mse = nn.MSELoss()\n",
    "cri_mae = nn.L1Loss()\n",
    "\n",
    "\n",
    "##########################################\n",
    "######## start training our model ######## \n",
    "##########################################\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "Epoch = 1000\n",
    "best_val = 9999.\n",
    "print(\"Let's start training!\")\n",
    "for e in range(0, Epoch):\n",
    "    start = time.time()\n",
    "    model.train()\n",
    "    for step_id, datas in enumerate(train_set):\n",
    "        atoms = datas[\"atoms\"].to(dev, non_blocking=True).long()\n",
    "        # coord = datas[\"coordinate\"].to(dev, non_blocking=True).float()\n",
    "        pair = datas[\"distance\"].to(dev, non_blocking=True).float()\n",
    "        spd = datas[\"SPD\"].to(dev, non_blocking=True).float()\n",
    "        edge = datas[\"edge\"].to(dev, non_blocking=True).float()\n",
    "        label = datas[\"label\"].to(dev, non_blocking=True).float()\n",
    "\n",
    "        pred = model(atoms, pair, spd, edge)\n",
    "\n",
    "        # loss = cri_mse(pred, label.unsqueeze(-1))\n",
    "        loss = cri_mae(pred, label.unsqueeze(-1))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        update_ema(ema, model, 0.997) # 0.997 best\n",
    "\n",
    "        if not (step_id+1) % 40:\n",
    "            print(f\"epoch: {e+1} / {Epoch},step {step_id} / {len(train_set)}, loss: {loss.detach().cpu().numpy():.4f}\")\n",
    "                \n",
    "    \n",
    "    ##########################################\n",
    "    ####### start evaluating our model #######\n",
    "    ##########################################\n",
    "    model.eval()\n",
    "    print(\"evaluating...\")\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0.\n",
    "        for step_id, datas in enumerate(val_set):\n",
    "            atoms = datas[\"atoms\"].to(dev, non_blocking=True).long()\n",
    "            # coord = datas[\"coordinate\"].to(dev, non_blocking=True).float()\n",
    "            pair = datas[\"distance\"].to(dev, non_blocking=True).float()\n",
    "            spd = datas[\"SPD\"].to(dev, non_blocking=True).float()\n",
    "            edge = datas[\"edge\"].to(dev, non_blocking=True).float()\n",
    "            label = datas[\"label\"].to(dev, non_blocking=True).float()\n",
    "\n",
    "            pred = model(atoms, pair, spd, edge)\n",
    "            \n",
    "            pred = pred * (9.13 - (-4.8)) + (-4.8)\n",
    "            label = label * (9.13 - (-4.8)) + (-4.8)\n",
    "            total_loss += cri_mae(pred, label.unsqueeze(-1))\n",
    "            \n",
    "        total_loss /= len(val_set)\n",
    "        total_loss = total_loss.detach().cpu().numpy()\n",
    "        print(f\"epoch: {e+1} / {Epoch}, test mae: {total_loss:.5f}\")\n",
    "    if total_loss < best_val:\n",
    "        best_val = total_loss\n",
    "        torch.save(model.state_dict(), f'./trained_weight/reg_logd_cliP_Epoch{e+1}_val_mae_{best_val:.5f}.pth')           \n",
    "\n",
    "\n",
    "    end = time.time()\n",
    "    print(f\"epoch: {e+1} end ; cost time: {(end - start)/60.:.4f} min\")\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222a024c-68ea-41c7-a53d-dfa0fcec14b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c895174-f20f-420c-8faf-03c2b63eddde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fcb",
   "language": "python",
   "name": "fcb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
