{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bac61f26-ff92-477b-96a0-836186307af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed : 10\n",
      "number of postive/negtive samples 4599/2042.\n",
      "{'Br': 0, 'C': 1, 'H': 2, 'N': 3, 'O': 4, 'F': 5, 'Cl': 6, 'S': 7, 'P': 8, 'I': 9, 'B': 10, 'Se': 11, 'Ar': 12, 'Kr': 13, 'Li': 14, 'Ne': 15, 'Xe': 16, 'Si': 17, 'Na': 18, 'Mask': 19, 'Pad': 20}\n",
      "train set is initialized successfully. The max length of the atom is 276. The number of dataset is 6641. Padding length is 150.\n",
      "number of postive/negtive samples 1120/540.\n",
      "{'Br': 0, 'C': 1, 'H': 2, 'N': 3, 'O': 4, 'F': 5, 'Cl': 6, 'S': 7, 'P': 8, 'I': 9, 'B': 10, 'Se': 11, 'Ar': 12, 'Kr': 13, 'Li': 14, 'Ne': 15, 'Xe': 16, 'Si': 17, 'Na': 18, 'Mask': 19, 'Pad': 20}\n",
      "test set is initialized successfully. The max length of the atom is 196. The number of dataset is 1660. Padding length is 150.\n",
      "Layer number of backbone is 12.\n",
      "The initial prompts are: ['The molecule has the X X X X X', 'The LogD of the molecule is A A A A', 'The LogP of the molecule is B B B B', 'The pKa of the molecule is C C C C', 'The pKb of the molecule is D D D D', 'The LogSol of the molecule is E E E E', 'The wLogSol of the molecule is F F F F']\n",
      "Set of Optimizer: lr:5e-05, weight_decay:0.0\n",
      "Current Optimizer is adan\n",
      "attribution loss is bce, and reduction method is mean.\n",
      "Let's start training!\n",
      "epoch: 1 / 1000,step 39 / 415, loss: 3.7837\n",
      "BBB cls: 0.0703; LogD: 0.6448; LogP: 0.6706; pKa: 0.5092; pKb: 0.5203; LogSol: 0.6838; wLogSol: 0.6848; \n",
      "epoch: 1 / 1000,step 79 / 415, loss: 3.6299\n",
      "BBB cls: 0.0757; LogD: 0.7157; LogP: 0.7123; pKa: 0.4751; pKb: 0.2870; LogSol: 0.6866; wLogSol: 0.6776; \n",
      "epoch: 1 / 1000,step 119 / 415, loss: 3.8396\n",
      "BBB cls: 0.0824; LogD: 0.6707; LogP: 0.6767; pKa: 0.5461; pKb: 0.4789; LogSol: 0.6920; wLogSol: 0.6929; \n",
      "epoch: 1 / 1000,step 159 / 415, loss: 3.0624\n",
      "BBB cls: 0.0538; LogD: 0.6903; LogP: 0.6866; pKa: 0.1556; pKb: 0.1399; LogSol: 0.6669; wLogSol: 0.6692; \n",
      "epoch: 1 / 1000,step 199 / 415, loss: 3.4141\n",
      "BBB cls: 0.0741; LogD: 0.6540; LogP: 0.6742; pKa: 0.3403; pKb: 0.3273; LogSol: 0.6720; wLogSol: 0.6722; \n",
      "epoch: 1 / 1000,step 239 / 415, loss: 3.1265\n",
      "BBB cls: 0.0392; LogD: 0.6379; LogP: 0.6578; pKa: 0.2565; pKb: 0.2341; LogSol: 0.6512; wLogSol: 0.6499; \n",
      "epoch: 1 / 1000,step 279 / 415, loss: 3.3539\n",
      "BBB cls: 0.0708; LogD: 0.6594; LogP: 0.6702; pKa: 0.3903; pKb: 0.2117; LogSol: 0.6779; wLogSol: 0.6735; \n",
      "epoch: 1 / 1000,step 319 / 415, loss: 3.2059\n",
      "BBB cls: 0.0813; LogD: 0.6486; LogP: 0.6538; pKa: 0.2622; pKb: 0.2401; LogSol: 0.6578; wLogSol: 0.6620; \n",
      "epoch: 1 / 1000,step 359 / 415, loss: 3.3548\n",
      "BBB cls: 0.0576; LogD: 0.6439; LogP: 0.6840; pKa: 0.3152; pKb: 0.2991; LogSol: 0.6786; wLogSol: 0.6765; \n",
      "epoch: 1 / 1000,step 399 / 415, loss: 3.3934\n",
      "BBB cls: 0.0739; LogD: 0.6251; LogP: 0.6409; pKa: 0.4046; pKb: 0.3702; LogSol: 0.6379; wLogSol: 0.6407; \n",
      "evaluating...\n",
      "epoch: 1 / 1000, test AUC: 0.86426\n",
      "epoch: 1 end ; cost time: 1.9246 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-7:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jovyan/.conda/fcb/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/jovyan/.conda/fcb/lib/python3.8/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/home/jovyan/.conda/fcb/lib/python3.8/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/jovyan/.conda/fcb/lib/python3.8/site-packages/torch/utils/data/_utils/pin_memory.py\", line 51, in _pin_memory_loop\n",
      "    do_one_step()\n",
      "  File \"/home/jovyan/.conda/fcb/lib/python3.8/site-packages/torch/utils/data/_utils/pin_memory.py\", line 28, in do_one_step\n",
      "    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/home/jovyan/.conda/fcb/lib/python3.8/multiprocessing/queues.py\", line 116, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/jovyan/.conda/fcb/lib/python3.8/site-packages/torch/multiprocessing/reductions.py\", line 307, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "  File \"/home/jovyan/.conda/fcb/lib/python3.8/multiprocessing/resource_sharer.py\", line 57, in detach\n",
      "    with _resource_sharer.get_connection(self._id) as conn:\n",
      "  File \"/home/jovyan/.conda/fcb/lib/python3.8/multiprocessing/resource_sharer.py\", line 87, in get_connection\n",
      "    c = Client(address, authkey=process.current_process().authkey)\n",
      "  File \"/home/jovyan/.conda/fcb/lib/python3.8/multiprocessing/connection.py\", line 502, in Client\n",
      "    c = SocketClient(address)\n",
      "  File \"/home/jovyan/.conda/fcb/lib/python3.8/multiprocessing/connection.py\", line 630, in SocketClient\n",
      "    s.connect(address)\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jovyan/.conda/fcb/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/jovyan/.conda/fcb/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/jovyan/.conda/fcb/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/jovyan/.conda/fcb/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import gc\n",
    "from sklearn import metrics\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "# torch.autograd.set_detect_anomaly(True)\n",
    "CUDA_LAUNCH_BLOCKING=1\n",
    "\n",
    "# fixed random seed for reproduction\n",
    "seed = 10\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.enabled = False\n",
    "print('Random seed :', seed)\n",
    "\n",
    "from collections import OrderedDict\n",
    "@torch.no_grad()\n",
    "def update_ema(ema_model, model, decay=0.9999):\n",
    "    \"\"\"\n",
    "    Step the EMA model towards the current model.\n",
    "    \"\"\"\n",
    "    ema_params = OrderedDict(ema_model.named_parameters())\n",
    "    model_params = OrderedDict(model.named_parameters())\n",
    "\n",
    "    for name, param in model_params.items():\n",
    "        # TODO: Consider applying only to params that require_grad to avoid small numerical changes of pos_embed\n",
    "        ema_params[name].mul_(decay).add_(param.data, alpha=1 - decay)\n",
    "\n",
    "def requires_grad(model, flag=True):\n",
    "    \"\"\"\n",
    "    Set requires_grad flag for all parameters in a model.\n",
    "    \"\"\"\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = flag\n",
    "\n",
    "from sklearn import metrics\n",
    "def compute_AUC(y, pred, n_class=1):\n",
    "    # compute one score\n",
    "    if n_class == 1:\n",
    "        auc = metrics.roc_auc_score(y, pred)\n",
    "\n",
    "    # compute two-class\n",
    "    elif n_class == 2:\n",
    "        # pos = pred[:, 1]\n",
    "        auc = metrics.roc_auc_score(y, pred)\n",
    "    return auc\n",
    "\n",
    "\n",
    "def print_loss(loss, loss_name):\n",
    "    print(f\"{loss_name}: {loss.detach().cpu().numpy():.4f}; \", end='', flush=True)\n",
    "    # print('\\r', end='', flush=True)\n",
    "\n",
    "    \n",
    "pad_len = 150 # best\n",
    "conf = 10 # best# \n",
    "\n",
    "##########################################\n",
    "#########  construct dataloader  ######### \n",
    "##########################################\n",
    "# from lmdb_datapipeline import LMDBDataset  \n",
    "from cls_lmdb_datapipeline import LMDBDataset \n",
    "# lmdb_file = './results/logd_train.lmdb'\n",
    "lmdb_file = './results/bbb_train.lmdb'\n",
    "train_dataset = LMDBDataset(lmdb_file, conf=conf, pad_len=pad_len, mode=\"train\")\n",
    "train_set = DataLoader(train_dataset,\n",
    "                    batch_size=16,  # 16 best\n",
    "                    drop_last=True,\n",
    "                    shuffle=True,\n",
    "                    pin_memory=True,\n",
    "                    num_workers=4,\n",
    "                    # collate_fn=train_dataset.collate_fn,\n",
    "                    worker_init_fn=train_dataset.worker_init_fn\n",
    "                    )\n",
    "\n",
    "# lmdb_file = './results/logd_test.lmdb'\n",
    "lmdb_file = './results/bbb_test.lmdb'\n",
    "val_dataset = LMDBDataset(lmdb_file, conf=conf, pad_len=pad_len, mode=\"test\")\n",
    "val_set = DataLoader(val_dataset,\n",
    "                    batch_size=32,\n",
    "                    drop_last=False,\n",
    "                    shuffle=False,\n",
    "                    pin_memory=True,\n",
    "                    num_workers=4,\n",
    "                    # collate_fn=train_dataset.collate_fn,\n",
    "                    worker_init_fn=val_dataset.worker_init_fn\n",
    "                    )\n",
    "\n",
    "\n",
    "##########################################\n",
    "######  build model and optimizer  ####### \n",
    "##########################################\n",
    "dev = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "from CLIP import clip\n",
    "clip_model, preprocess = clip.load(name=\"ViT-B/16\", device=\"cpu\", download_root=\"/home/jovyan/clip_download_root\")\n",
    "from model_zoo import CLIP_Protein\n",
    "model = CLIP_Protein(clip_model, conf, pad_len=pad_len).to(dev)\n",
    "\n",
    "# path = \"/home/jovyan/prompts_learning/trained_weight/cliP_multiP_Epoch5_val_auc_0.92254.pth\"\n",
    "# # path = \"/home/jovyan/prompts_learning/trained_weight/cliP_multiP_Epoch4_val_auc_0.92039.pth\"\n",
    "# sd = torch.load(path)\n",
    "# model.load_state_dict(sd)\n",
    "# print(\"pre-trained weights loaded...\")\n",
    "# exit(0)\n",
    "\n",
    "from copy import deepcopy\n",
    "ema = deepcopy(model).to(dev)  # Create an EMA of the model for use after training\n",
    "update_ema(ema, model, decay=0)  # Ensure EMA is initialized with synced weights\n",
    "requires_grad(ema, False)\n",
    "ema.eval()\n",
    "\n",
    "# best\n",
    "lr = 5e-5\n",
    "wd = 0.\n",
    "\n",
    "print(f'Set of Optimizer: lr:{lr}, weight_decay:{wd}')\n",
    "model_params = [\n",
    "                {'params': model.atom_encoder.parameters(), 'lr': lr, \"weight_decay\": wd},\n",
    "                {'params': model.coor_encoder.parameters(), 'lr': lr, \"weight_decay\": wd},\n",
    "                {'params': model.fusion_blocks.parameters(), 'lr': lr, \"weight_decay\": wd},\n",
    "                \n",
    "                {'params': model.head.parameters(), 'lr': 1e-3, \"weight_decay\": wd},\n",
    "                \n",
    "                {'params': model.prompts_processor.parameters(), 'lr': 5e-5, \"weight_decay\": wd},\n",
    "                # {'params': model.ppim.parameters(), 'lr': lr},\n",
    "                {'params': model.ppim.model.parameters(), 'lr': 5e-5, \"weight_decay\": wd},\n",
    "                {'params': model.ppim.atom_emb.parameters(), 'lr': 5e-5, \"weight_decay\": wd},\n",
    "                {'params': model.ppim.label_emb.parameters(), 'lr': 5e-5, \"weight_decay\": wd},\n",
    "                {'params': model.ppim.attr_heads.parameters(), 'lr': 5e-5, \"weight_decay\": wd},\n",
    "               ]\n",
    "\n",
    "\n",
    "optims = 'adan'\n",
    "# optims = \"sgd\"\n",
    "if optims == 'adan':\n",
    "    from adan import Adan\n",
    "    optimizer = Adan(model_params, betas=(0.98, 0.92, 0.99), max_grad_norm=0.)\n",
    "elif optims == 'sgd':\n",
    "    optimizer = optim.SGD(model_params, momentum=0.9, weight_decay=wd)\n",
    "elif optims == 'adamw':\n",
    "    optimizer = optim.AdamW(model_params, betas=(0.9, 0.999), weight_decay=wd)\n",
    "elif optims == 'adam':\n",
    "    optimizer = optim.Adam(model_params, betas=(0.9, 0.999), weight_decay=wd)\n",
    "print('Current Optimizer is', optims)\n",
    "\n",
    "\n",
    "###################################################\n",
    "########   build learning rate scheduler   ######## \n",
    "###################################################\n",
    "# scheduler1 = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=1./3., total_iters=5) # best\n",
    "# scheduler2 = torch.optim.lr_scheduler.ConstantLR(optimizer, factor=1., total_iters=5)\n",
    "# scheduler3 = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=15, eta_min=1e-6)\n",
    "# scheduler = torch.optim.lr_scheduler.SequentialLR(optimizer, schedulers=[scheduler1, scheduler2, scheduler3], milestones=[5, 10])\n",
    "\n",
    "# scheduler1 = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=1./3., total_iters=5) \n",
    "# scheduler2 = torch.optim.lr_scheduler.ConstantLR(optimizer, factor=1., total_iters=5)\n",
    "# scheduler3 = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=15, eta_min=1e-6)\n",
    "# scheduler = torch.optim.lr_scheduler.SequentialLR(optimizer, schedulers=[scheduler1, scheduler2, scheduler3], milestones=[5, 10])\n",
    "# cur_lr = scheduler.get_last_lr() \n",
    "# print(f\"Current learning rate is {cur_lr}.\")\n",
    "\n",
    "\n",
    "##########################################\n",
    "########   build loss criterion   ######## \n",
    "##########################################\n",
    "# attr_loss = 'l1'\n",
    "# attr_loss = 'mse'\n",
    "attr_loss = 'bce'\n",
    "# attr_loss = 'ce'\n",
    "\n",
    "# red = 'sum'\n",
    "red = 'mean'\n",
    "\n",
    "print(f\"attribution loss is {attr_loss}, and reduction method is {red}.\")\n",
    "if attr_loss == 'l1':\n",
    "    # cri_attr = nn.L1Loss(reduction=red)\n",
    "    cri_attr = nn.SmoothL1Loss(reduction=red)\n",
    "elif attr_loss == 'mse':\n",
    "    cri_attr = nn.MSELoss(reduction=red)\n",
    "elif attr_loss == 'bce':\n",
    "    cri_attr = nn.BCEWithLogitsLoss(reduction=red)\n",
    "elif attr_loss == 'ce':\n",
    "    cri_attr = nn.CrossEntropyLoss(reduction=red)\n",
    "\n",
    "\n",
    "##########################################\n",
    "######## start training our model ######## \n",
    "##########################################\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "Epoch = 1000\n",
    "best_val = 0.92\n",
    "print(\"Let's start training!\")\n",
    "\n",
    "for e in range(0, Epoch):\n",
    "    start = time.time()\n",
    "    model.train()\n",
    "    for step_id, datas in enumerate(train_set):\n",
    "            # break\n",
    "            # print(datas[\"atoms\"].shape)\n",
    "            # print(datas[\"coordinate\"].shape)\n",
    "            # print(datas[\"label\"])\n",
    "            atoms = datas[\"atoms\"].to(dev, non_blocking=True).long()\n",
    "            # coord = datas[\"coordinate\"].to(dev, non_blocking=True).float()\n",
    "            pair = datas[\"distance\"].to(dev, non_blocking=True).float()\n",
    "            spd = datas[\"SPD\"].to(dev, non_blocking=True).float()\n",
    "            edge = datas[\"edge\"].to(dev, non_blocking=True).float()\n",
    "            label = datas[\"label\"].to(dev, non_blocking=True).float()\n",
    "            \n",
    "            pka_gt = datas[\"pka\"].to(dev, non_blocking=True).float()\n",
    "            pkb_gt = datas[\"pkb\"].to(dev, non_blocking=True).float()\n",
    "            logd_gt = datas[\"logd\"].to(dev, non_blocking=True).float()\n",
    "            logp_gt = datas[\"logp\"].to(dev, non_blocking=True).float()\n",
    "            logsol_gt = datas[\"logsol\"].to(dev, non_blocking=True).float()\n",
    "            wlogsol_gt = datas[\"wlogsol\"].to(dev, non_blocking=True).float()\n",
    "            \n",
    "            # pka_gt = datas[\"pka\"].to(dev, non_blocking=True).long()\n",
    "            # pkb_gt = datas[\"pkb\"].to(dev, non_blocking=True).long()\n",
    "            # logd_gt = datas[\"logd\"].to(dev, non_blocking=True).long()\n",
    "            # logp_gt = datas[\"logp\"].to(dev, non_blocking=True).long()\n",
    "            # logsol_gt = datas[\"logsol\"].to(dev, non_blocking=True).long()\n",
    "            # wlogsol_gt = datas[\"wlogsol\"].to(dev, non_blocking=True).long()\n",
    "\n",
    "            \n",
    "            pred, attr_list = model(atoms, pair, spd, edge)\n",
    "            # pred = torch.tanh(pred)\n",
    "\n",
    "            # loss = cri_mae(pred, label.unsqueeze(-1))\n",
    "            \n",
    "            # weighted BCE loss\n",
    "            # loss = F.binary_cross_entropy_with_logits(pred, label.unsqueeze(-1))\n",
    "            loss = F.binary_cross_entropy_with_logits(pred, label.unsqueeze(-1), reduction='none')\n",
    "            lambda_ = 2.\n",
    "            for i, lab in enumerate(label):\n",
    "                if lab == 0:\n",
    "                    # loss[i] = loss[i] * (6641./2042.)\n",
    "                    loss[i] = loss[i] * (4599./6641)  # best\n",
    "                    # loss[i] = loss[i] * (4599./6641) * torch.pow(torch.sigmoid(pred[i]), lambda_)\n",
    "                else:\n",
    "                    # loss[i] = loss[i] * (6641./4599.)\n",
    "                    loss[i] = loss[i] * (2042./6641)\n",
    "                    # loss[i] = loss[i] * (2042./6641) * torch.pow(1. - torch.sigmoid(pred[i]), lambda_)# focal loss\n",
    "            if red == 'mean':\n",
    "                loss_cls = loss.mean()\n",
    "            elif red == 'sum':\n",
    "                loss_cls = loss.sum()\n",
    "            \n",
    "            \n",
    "            # loss_pka = F.binary_cross_entropy_with_logits(attr_list[2], pka_gt.unsqueeze(-1), reduction=red)\n",
    "            # loss_pkb = F.binary_cross_entropy_with_logits(attr_list[3], pkb_gt.unsqueeze(-1), reduction=red)\n",
    "            \n",
    "            loss_logd = cri_attr(attr_list[0], logd_gt.unsqueeze(-1))\n",
    "            loss_logp = cri_attr(attr_list[1], logp_gt.unsqueeze(-1))\n",
    "            loss_pka = cri_attr(attr_list[2], pka_gt.unsqueeze(-1))\n",
    "            loss_pkb = cri_attr(attr_list[3], pkb_gt.unsqueeze(-1))\n",
    "            loss_logsol = cri_attr(attr_list[4], logsol_gt.unsqueeze(-1))\n",
    "            loss_wlogsol = cri_attr(attr_list[5], wlogsol_gt.unsqueeze(-1))\n",
    "            \n",
    "            # loss_pka = cri_attr(attr_list[2], pka_gt)\n",
    "            # loss_pkb = cri_attr(attr_list[3], pkb_gt)\n",
    "            # loss_logd = cri_attr(attr_list[0], logd_gt)\n",
    "            # loss_logp = cri_attr(attr_list[1], logp_gt)\n",
    "            # loss_logsol = cri_attr(attr_list[4], logsol_gt)\n",
    "            # loss_wlogsol = cri_attr(attr_list[5], wlogsol_gt)\n",
    "            \n",
    "            loss_attr = loss_logd + loss_logp  + loss_logsol + loss_pka + loss_pkb + loss_wlogsol\n",
    "            \n",
    "            loss = loss_cls*1. + loss_attr # *(1./6.) # + loss_con*0.1\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            update_ema(ema, model, 0.997) # 0.997 best\n",
    "            # update_ema(ema, model, 0.999)\n",
    "\n",
    "            if not (step_id+1) % 40:\n",
    "                print(f\"epoch: {e+1} / {Epoch},step {step_id} / {len(train_set)}, loss: {loss.detach().cpu().numpy():.4f}\")\n",
    "                print_loss(loss_cls, \"BBB cls\")\n",
    "                print_loss(loss_logd, \"LogD\")\n",
    "                print_loss(loss_logp, \"LogP\")\n",
    "                print_loss(loss_pka, \"pKa\")\n",
    "                print_loss(loss_pkb, \"pKb\")\n",
    "                print_loss(loss_logsol, \"LogSol\")\n",
    "                print_loss(loss_wlogsol, \"wLogSol\")\n",
    "                # print_loss(loss_con, \"Contrastive\")\n",
    "                print()\n",
    "                \n",
    "    \n",
    "    ##########################################\n",
    "    ####### start evaluating our model #######\n",
    "    ##########################################\n",
    "    model.eval()\n",
    "    print(\"evaluating...\")\n",
    "    with torch.no_grad():\n",
    "        all_pred = None \n",
    "        all_lab = None\n",
    "        for step_id, datas in enumerate(val_set):\n",
    "                atoms = datas[\"atoms\"].to(dev, non_blocking=True).long()\n",
    "                # coord = datas[\"coordinate\"].to(dev, non_blocking=True).float()\n",
    "                pair = datas[\"distance\"].to(dev, non_blocking=True).float()\n",
    "                spd = datas[\"SPD\"].to(dev, non_blocking=True).float()\n",
    "                edge = datas[\"edge\"].to(dev, non_blocking=True).float()\n",
    "                label = datas[\"label\"].to(dev, non_blocking=True).float()\n",
    "                 \n",
    "                pred = ema(atoms, pair, spd, edge)\n",
    "                \n",
    "                pred = torch.sigmoid(pred)\n",
    "                # pred = torch.softmax(pred, dim=-1)[:, 1]\n",
    "                # total_loss += cri_mae(pred, label.unsqueeze(-1))\n",
    "                all_pred = pred if all_pred is None else torch.cat([all_pred, pred], dim=0)\n",
    "                all_lab = label if all_lab is None else torch.cat([all_lab, label], dim=0)\n",
    "    auc = compute_AUC(all_lab.cpu().detach(), all_pred.cpu().detach())\n",
    "    print(f\"epoch: {e+1} / {Epoch}, test AUC: {auc:.5f}\")\n",
    "    if auc > best_val:\n",
    "        best_val = auc\n",
    "        # torch.save(model.state_dict(), f'./trained_weight/cliP_Epoch{e+1}_val_auc_{best_val:.5f}.pth') \n",
    "        torch.save(ema.state_dict(), f'./trained_weight/cliP_multiP_Epoch{e+1}_val_auc_{best_val:.5f}.pth') \n",
    "\n",
    "\n",
    "    end = time.time()\n",
    "    print(f\"epoch: {e+1} end ; cost time: {(end - start)/60.:.4f} min\")\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    # scheduler.step()\n",
    "    # cur_lr = scheduler.get_last_lr() \n",
    "    # print(f\"Current learning rate is {cur_lr}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a929c24-3573-4523-8f87-e303fb3b3f79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c97b751-7d68-423a-91e1-d7b06a34a50f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fcb",
   "language": "python",
   "name": "fcb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
