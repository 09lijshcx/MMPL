{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbc1d38-a435-44da-aa79-fbd952629676",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import lmdb\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "# data = pd.read_csv(\"/home/jovyan/PharmaBench/data/final_datasets/logd_reg_final_data.csv\")\n",
    "data = pd.read_csv(\"/home/jovyan/PharmaBench/data/final_datasets/bbb_cls_final_data.csv\")\n",
    "scaffold_training = data[data['scaffold_train_test_label'] == 'train']\n",
    "scaffold_test = data[data['scaffold_train_test_label'] == 'test']\n",
    "scaffold_training = scaffold_training.reset_index()\n",
    "scaffold_test = scaffold_test.reset_index()\n",
    "\n",
    "\n",
    "class Balance_LMDBDataset(Dataset):\n",
    "    def __init__(self, db_path, conf, pad_len=200, mode=\"train\"):\n",
    "        self.db_path = db_path\n",
    "        assert os.path.isfile(self.db_path), \"{} not found\".format(\n",
    "            self.db_path\n",
    "        )\n",
    "        env = self.connect_db(self.db_path)\n",
    "        with env.begin() as txn:\n",
    "            self._keys = list(txn.cursor().iternext(values=False))\n",
    "        \n",
    "        # get correponding label from csv\n",
    "        self.value_list = []\n",
    "        self.pos_idx_list = []\n",
    "        self.neg_idx_list = []\n",
    "        self.max_len = 0\n",
    "        if mode == \"train\":\n",
    "            pd_data = scaffold_training\n",
    "        else:\n",
    "            pd_data = scaffold_test\n",
    "        \n",
    "        self.max_value = 0.\n",
    "        self.min_value = 999.\n",
    "        for i in range(len(self._keys)):\n",
    "            datapoint_pickled = env.begin().get(self._keys[i])\n",
    "            data = pickle.loads(datapoint_pickled)\n",
    "            current_len = len(data['atoms'])\n",
    "            if self.max_len < current_len:\n",
    "                self.max_len = current_len\n",
    "            idx = pd_data['Smiles_unify'][pd_data['Smiles_unify']==data['smi']].index[0]\n",
    "            self.value_list.append(pd_data['value'][idx])\n",
    "            \n",
    "            # to resample and balance pos/neg num.\n",
    "            if pd_data['value'][idx] == 0:\n",
    "                self.neg_idx_list.append(idx)\n",
    "            else:\n",
    "                self.pos_idx_list.append(idx)\n",
    "                \n",
    "            # for normalization of reprogression task\n",
    "            if self.max_value < pd_data['value'][idx]:\n",
    "                self.max_value = pd_data['value'][idx]\n",
    "            if self.min_value > pd_data['value'][idx]:\n",
    "                self.min_value = pd_data['value'][idx]\n",
    "        \n",
    "        # for classfication task, to balance number of postive/negtive samples\n",
    "        # num_p = 0\n",
    "        # num_n = 0\n",
    "        # for lab in self.value_list:\n",
    "        #     if lab == 1: num_p += 1\n",
    "        #     else: num_n += 1\n",
    "        print(f\"number of postive/negtive samples {len(self.pos_idx_list)}/{len(self.neg_idx_list)}.\")\n",
    "            \n",
    "        \n",
    "        # get word embedding index (atoms) ps:only use one time and fix the dict result \n",
    "        # self.atom_dict = {}\n",
    "        # idx = 0 \n",
    "        # for i in range(len(self._keys)):\n",
    "        #     datapoint_pickled = env.begin().get(self._keys[i])\n",
    "        #     data = pickle.loads(datapoint_pickled)\n",
    "        #     for a in data['atoms']:\n",
    "        #         if a not in self.atom_dict:\n",
    "        #             self.atom_dict.update({a:idx})\n",
    "        #             idx += 1\n",
    "        self.atom_dict = {'Br': 0, 'C': 1, 'H': 2, 'N': 3, 'O': 4, 'F': 5, 'Cl': 6, 'S': 7, 'P': 8, 'I': 9, 'B': 10, 'Se': 11, 'Ar': 12, 'Kr': 13, 'Li': 14, 'Ne': 15, 'Xe': 16, 'Si': 17}\n",
    "        print(self.atom_dict)\n",
    "        \n",
    "        self.pad_len = pad_len\n",
    "        self.conf = conf # conformation\n",
    "        self.mode = mode\n",
    "        print(f\"{mode} set is initialized successfully. The max length of the atom is {self.max_len}. The number of dataset is {len(self._keys)}. Padding length is {self.pad_len}.\")\n",
    "        \n",
    "                    \n",
    "\n",
    "    def connect_db(self, lmdb_path, save_to_self=False):\n",
    "        env = lmdb.open(\n",
    "            lmdb_path,\n",
    "            subdir=False,\n",
    "            readonly=True,\n",
    "            lock=False,\n",
    "            readahead=False,\n",
    "            meminit=False,\n",
    "            max_readers=256,\n",
    "        )\n",
    "        if not save_to_self:\n",
    "            return env\n",
    "        else:\n",
    "            self.env = env\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pos_idx_list)\n",
    "    \n",
    "    def min_max_norm(self, x):\n",
    "        _min = x.min()\n",
    "        _max = x.max()\n",
    "        x = (x - _min) / (_max - _min)\n",
    "        return x\n",
    "    def __getitem__(self, idx_p):\n",
    "        if not hasattr(self, 'env'):\n",
    "            self.connect_db(self.db_path, save_to_self=True)\n",
    "            \n",
    "        #################################    Get the positive sample    #################################\n",
    "        datapoint_pickled = self.env.begin().get(self._keys[idx_p])\n",
    "        data = pickle.loads(datapoint_pickled)\n",
    "        # random select conformation\n",
    "        conf_idx = torch.randperm(11)[:self.conf]\n",
    "        coordinates = torch.tensor(np.array(data['coordinates']), dtype=torch.float32)[conf_idx, :, :]\n",
    "        emb_idx = torch.tensor([self.atom_dict[atom] for atom in data['atoms']], dtype=torch.long)\n",
    "        cur_len = len(emb_idx)\n",
    "        \n",
    "        # shortest path distance\n",
    "        spd = torch.tensor(np.array(data[\"SPD\"]), dtype=torch.float32)\n",
    "        edge = torch.tensor(np.array(data[\"edge\"]), dtype=torch.float32) + torch.eye(cur_len)\n",
    "        \n",
    "        # random dropout atoms\n",
    "        if np.random.rand() < 0.5 and self.mode == \"train\" and cur_len > 100:\n",
    "            num_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "            num = random.choice(num_list)\n",
    "            emb_idx = emb_idx[-num:]\n",
    "            coordinates = coordinates[:, -num:,:]\n",
    "            spd = spd[-num:, -num:]\n",
    "            edge = edge[-num:, -num:]\n",
    "            cur_len = len(emb_idx)\n",
    "        \n",
    "        # padding\n",
    "        if cur_len < self.pad_len:\n",
    "            new_emb = torch.full(size=(self.pad_len,), fill_value=self.pad_len-1, dtype=torch.long)\n",
    "            new_emb[:cur_len] = emb_idx\n",
    "            \n",
    "            new_cor = torch.full(size=(self.conf, self.pad_len, 3), fill_value=0, dtype=torch.float32)\n",
    "            new_cor[:, :cur_len, :] = coordinates\n",
    "            \n",
    "            new_spd = torch.full(size=(self.pad_len, self.pad_len), fill_value=0, dtype=torch.float32)\n",
    "            new_spd[:cur_len, :cur_len] = spd\n",
    "            new_edge = torch.full(size=(self.pad_len, self.pad_len), fill_value=0, dtype=torch.float32)\n",
    "            new_edge[:cur_len, :cur_len] = edge\n",
    "        elif cur_len >= self.pad_len:\n",
    "            new_emb = emb_idx[:self.pad_len]\n",
    "            new_cor = coordinates[:, :self.pad_len, :]\n",
    "            new_spd = spd[:self.pad_len, :self.pad_len]\n",
    "            new_edge = edge[:self.pad_len, :self.pad_len]\n",
    "        \n",
    "        # Normalize and augment coordination\n",
    "        if self.mode == \"train\": # for random augmentation\n",
    "            weight_list = [0.1, 0.2, 0.3, 0.5, 0.7, 0.01, 0.001]\n",
    "            scale = random.choice(weight_list) \n",
    "            noise = scale * torch.randn_like(new_cor)\n",
    "            if np.random.rand() < 0.5:  # for add noise locally\n",
    "                mask = torch.randint_like(noise, 0, 2, dtype=torch.float32)\n",
    "                noise = noise * mask\n",
    "            new_cor = new_cor + noise\n",
    "            \n",
    "            # new_cor = new_cor + 1. * torch.rand_like(new_cor)\n",
    "        # new_cor = (self.min_max_norm(new_cor) - 0.5) / 0.5\n",
    "            \n",
    "        # to compute the pair relative distance\n",
    "        atom_expanded = new_cor.unsqueeze(2)  # shape (conf, pad_len, 1, 3)\n",
    "        coor_expanded = new_cor.unsqueeze(1)   # shape (conf, 1, pad_len, 3)\n",
    "        # distance = atom_expanded - coor_expanded\n",
    "        # distance = distance.permute(1, 0, 2, 3).reshape(-1, conf*pad_len*3)   # xyz \n",
    "        distance = torch.sqrt((atom_expanded - coor_expanded).pow(2).sum(dim=-1))   # x+y+z\n",
    "        distance = distance.permute(1, 0, 2).reshape(-1, self.conf*self.pad_len)\n",
    "        distance = (self.min_max_norm(distance) - 0.5) / 0.5\n",
    "        \n",
    "        # label = torch.tensor(self.value_list[idx], dtype=torch.float32)\n",
    "        # label = (label - self.min_value) / (self.max_value - self.min_value)\n",
    "        \n",
    "        new_cor = (self.min_max_norm(new_cor) - 0.5) / 0.5\n",
    "        new_cor = new_cor.permute(1, 0, 2).reshape(-1, self.conf*3)\n",
    "        # new_spd = (self.min_max_norm(new_spd) - 0.5) / 0.5\n",
    "        # new_edge = (self.min_max_norm(new_edge) - 0.5) / 0.5\n",
    "        \n",
    "        new_emb_p = np.array(new_emb)\n",
    "        new_cor_p = np.array(new_cor)\n",
    "        distance_p = np.array(distance)\n",
    "        new_spd_p = np.array(new_spd)\n",
    "        new_edge_p = np.array(new_edge)\n",
    "        \n",
    "        \n",
    "        #################################    Get the negtive sample    #################################\n",
    "        idx_n = np.random.randint(0, len(self.neg_idx_list))\n",
    "        datapoint_pickled = self.env.begin().get(self._keys[idx_n])\n",
    "        data = pickle.loads(datapoint_pickled)\n",
    "        # random select conformation\n",
    "        conf_idx = torch.randperm(11)[:self.conf]\n",
    "        coordinates = torch.tensor(np.array(data['coordinates']), dtype=torch.float32)[conf_idx, :, :]\n",
    "        emb_idx = torch.tensor([self.atom_dict[atom] for atom in data['atoms']], dtype=torch.long)\n",
    "        cur_len = len(emb_idx)\n",
    "        \n",
    "        # shortest path distance\n",
    "        spd = torch.tensor(np.array(data[\"SPD\"]), dtype=torch.float32)\n",
    "        edge = torch.tensor(np.array(data[\"edge\"]), dtype=torch.float32) + torch.eye(cur_len)\n",
    "        \n",
    "        # random dropout atoms\n",
    "        if np.random.rand() < 0.5 and self.mode == \"train\" and cur_len > 100:\n",
    "            num_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "            num = random.choice(num_list)\n",
    "            emb_idx = emb_idx[-num:]\n",
    "            coordinates = coordinates[:, -num:,:]\n",
    "            spd = spd[-num:, -num:]\n",
    "            edge = edge[-num:, -num:]\n",
    "            cur_len = len(emb_idx)\n",
    "        \n",
    "        # padding\n",
    "        if cur_len < self.pad_len:\n",
    "            new_emb = torch.full(size=(self.pad_len,), fill_value=self.pad_len-1, dtype=torch.long)\n",
    "            new_emb[:cur_len] = emb_idx\n",
    "            \n",
    "            new_cor = torch.full(size=(self.conf, self.pad_len, 3), fill_value=0, dtype=torch.float32)\n",
    "            new_cor[:, :cur_len, :] = coordinates\n",
    "            \n",
    "            new_spd = torch.full(size=(self.pad_len, self.pad_len), fill_value=0, dtype=torch.float32)\n",
    "            new_spd[:cur_len, :cur_len] = spd\n",
    "            new_edge = torch.full(size=(self.pad_len, self.pad_len), fill_value=0, dtype=torch.float32)\n",
    "            new_edge[:cur_len, :cur_len] = edge\n",
    "        elif cur_len >= self.pad_len:\n",
    "            new_emb = emb_idx[:self.pad_len]\n",
    "            new_cor = coordinates[:, :self.pad_len, :]\n",
    "            new_spd = spd[:self.pad_len, :self.pad_len]\n",
    "            new_edge = edge[:self.pad_len, :self.pad_len]\n",
    "        \n",
    "        # Normalize and augment coordination\n",
    "        if self.mode == \"train\": # for random augmentation\n",
    "            weight_list = [0.1, 0.2, 0.3, 0.5, 0.7, 0.01, 0.001]\n",
    "            scale = random.choice(weight_list) \n",
    "            noise = scale * torch.randn_like(new_cor)\n",
    "            if np.random.rand() < 0.5:  # for add noise locally\n",
    "                mask = torch.randint_like(noise, 0, 2, dtype=torch.float32)\n",
    "                noise = noise * mask\n",
    "            new_cor = new_cor + noise\n",
    "            \n",
    "            # new_cor = new_cor + 1. * torch.rand_like(new_cor)\n",
    "        # new_cor = (self.min_max_norm(new_cor) - 0.5) / 0.5\n",
    "            \n",
    "        # to compute the pair relative distance\n",
    "        atom_expanded = new_cor.unsqueeze(2)  # shape (conf, pad_len, 1, 3)\n",
    "        coor_expanded = new_cor.unsqueeze(1)   # shape (conf, 1, pad_len, 3)\n",
    "        # distance = atom_expanded - coor_expanded\n",
    "        # distance = distance.permute(1, 0, 2, 3).reshape(-1, conf*pad_len*3)   # xyz \n",
    "        distance = torch.sqrt((atom_expanded - coor_expanded).pow(2).sum(dim=-1))   # x+y+z\n",
    "        distance = distance.permute(1, 0, 2).reshape(-1, self.conf*self.pad_len)\n",
    "        distance = (self.min_max_norm(distance) - 0.5) / 0.5\n",
    "        \n",
    "        # label = torch.tensor(self.value_list[idx], dtype=torch.float32)\n",
    "        # label = (label - self.min_value) / (self.max_value - self.min_value)\n",
    "        \n",
    "        new_cor = (self.min_max_norm(new_cor) - 0.5) / 0.5\n",
    "        new_cor = new_cor.permute(1, 0, 2).reshape(-1, self.conf*3)\n",
    "        # new_spd = (self.min_max_norm(new_spd) - 0.5) / 0.5\n",
    "        # new_edge = (self.min_max_norm(new_edge) - 0.5) / 0.5\n",
    "        \n",
    "        new_emb_n = np.array(new_emb)\n",
    "        new_cor_n = np.array(new_cor)\n",
    "        distance_n = np.array(distance)\n",
    "        new_spd_n = np.array(new_spd)\n",
    "        new_edge_n = np.array(new_edge)\n",
    "       \n",
    "        return new_emb_p, new_emb_n, \\\n",
    "        new_cor_p, new_cor_n, \\\n",
    "        distance_p, distance_n, \\\n",
    "        new_spd_p, new_spd_n, \\\n",
    "        new_edge_p, new_edge_n\n",
    "        # return {\"atoms\": new_emb, \"coordinate\": new_cor, \"distance\": distance, \"SPD\": new_spd, \"edge\": new_edge, \"label\": label}\n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "        new_emb_p, new_emb_n, \\\n",
    "        new_cor_p, new_cor_n, \\\n",
    "        distance_p, distance_n, \\\n",
    "        new_spd_p, new_spd_n, \\\n",
    "        new_edge_p, new_edge_n = zip(*batch)\n",
    "        data={}\n",
    "\n",
    "        data['atoms']=torch.cat([torch.tensor(new_emb_p), torch.tensor(new_emb_n)], 0)\n",
    "        data['coordinate']=torch.cat([torch.tensor(new_cor_p).float(), torch.tensor(new_cor_n).float()], 0)\n",
    "        data['distance']=torch.cat([torch.tensor(distance_p).float(), torch.tensor(distance_n).float()], 0)\n",
    "        data['SPD']=torch.cat([torch.tensor(new_spd_p).float(), torch.tensor(new_spd_n).float()], 0)\n",
    "        data['edge']=torch.cat([torch.tensor(new_edge_p).float(), torch.tensor(new_edge_n).float()], 0)\n",
    "        data['label']=torch.tensor([1]*len(new_emb_p)+[0]*len(new_emb_n))\n",
    "        return data\n",
    "    \n",
    "    def worker_init_fn(self, worker_id):\n",
    "        np.random.seed(np.random.get_state()[1][0] + worker_id)\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    lmdb_file = './results/bbb_train.lmdb'\n",
    "    train_dataset = Balance_LMDBDataset(lmdb_file, conf=10, pad_len=150, mode=\"train\")\n",
    "    train_set = torch.utils.data.DataLoader(train_dataset,\n",
    "                                                batch_size=4,\n",
    "                                                shuffle=False,\n",
    "                                                pin_memory=True,\n",
    "                                                num_workers=0,\n",
    "                                                collate_fn=train_dataset.collate_fn,\n",
    "                                                worker_init_fn=train_dataset.worker_init_fn\n",
    "                                                )\n",
    "    for datas in train_set:\n",
    "        print(datas[\"atoms\"].shape)\n",
    "        # print(datas[\"coordinate\"].shape)\n",
    "        # print(datas[\"label\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2c3183-561b-4773-8088-4845e059fad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.randperm(11)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a6de1b-dea0-412f-a2eb-9a879253da89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lmdb\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def find_min_max(x_list):\n",
    "    x_np = np.array(x_list)\n",
    "    print(x_np.max(), x_np.min())\n",
    "    \n",
    "\n",
    "def min_max_norm(x_list, max_v, min_v):\n",
    "    x_np = np.array(x_list)\n",
    "    x_np = (x_np - min_v) / (max_v - min_v)\n",
    "    x_list = np.tolist(x_np)\n",
    "    return x_list\n",
    "\n",
    "\n",
    "class HERG_Multi_Class_LMDBDataset(Dataset):\n",
    "    def __init__(self, conf, pad_len=200, mode=\"train\"):\n",
    "        if mode == \"train\":\n",
    "            db_path = \"/home/jovyan/prompts_learning/results/herg_cls_train.lmdb\"\n",
    "            csv_path = \"/home/jovyan/prompts_learning/herg_dataset/hERGDB_cls_train_data.csv\"\n",
    "        elif mode == \"val\":\n",
    "            db_path = \"/home/jovyan/prompts_learning/results/herg_cls_val.lmdb\"\n",
    "            csv_path = \"/home/jovyan/prompts_learning/herg_dataset/hERGDB_cls_valid_data.csv\"\n",
    "        elif mode == \"week1\":\n",
    "            db_path = \"/home/jovyan/prompts_learning/results/herg_cls_week1.lmdb\"\n",
    "            csv_path = \"/home/jovyan/prompts_learning/herg_dataset/hERGDB_cls_week1_1201.csv\"\n",
    "        elif mode == \"week2\":\n",
    "            db_path = \"/home/jovyan/prompts_learning/results/herg_cls_week2.lmdb\"\n",
    "            csv_path = \"/home/jovyan/prompts_learning/herg_dataset/hERGDB_cls_week2_1201.csv\"\n",
    "        elif mode == \"week3\":\n",
    "            db_path = \"/home/jovyan/prompts_learning/results/herg_cls_week3.lmdb\"\n",
    "            csv_path = \"/home/jovyan/prompts_learning/herg_dataset/hERGDB_cls_week3_1201.csv\"\n",
    "        elif mode == \"week4\":\n",
    "            db_path = \"/home/jovyan/prompts_learning/results/herg_cls_week4.lmdb\"\n",
    "            csv_path = \"/home/jovyan/prompts_learning/herg_dataset/hERGDB_cls_week4_1201.csv\"\n",
    "        pd_data = pd.read_csv(csv_path)\n",
    "        \n",
    "        self.db_path = db_path\n",
    "        assert os.path.isfile(self.db_path), \"{} not found\".format(\n",
    "            self.db_path\n",
    "        )\n",
    "        env = self.connect_db(self.db_path)\n",
    "        with env.begin() as txn:\n",
    "            self._keys = list(txn.cursor().iternext(values=False))\n",
    "        \n",
    "        # get correponding label from csv\n",
    "        self.value_list = []\n",
    "        self.logd_list = []\n",
    "        self.logp_list = []\n",
    "        self.pka_list = []\n",
    "        self.pkb_list = []\n",
    "        self.logsol_list = []\n",
    "        self.wlogsol_list = []\n",
    "        self.max_len = 0\n",
    "        \n",
    "        self.max_value = 0.\n",
    "        self.min_value = 999.\n",
    "        for i in range(len(self._keys)):\n",
    "            datapoint_pickled = env.begin().get(self._keys[i])\n",
    "            data = pickle.loads(datapoint_pickled)\n",
    "            current_len = len(data['atoms'])\n",
    "            if self.max_len < current_len:\n",
    "                self.max_len = current_len\n",
    "            idx = pd_data['smiles'][pd_data['smiles']==data['smi']].index[0]\n",
    "            self.value_list.append(pd_data['class'][idx])\n",
    "            \n",
    "            self.logd_list.append(pd_data['LogD_pred'][idx])\n",
    "            self.logp_list.append(pd_data['LogP_pred'][idx])\n",
    "            self.pka_list.append(pd_data['pKa_class_pred'][idx])\n",
    "            self.pkb_list.append(pd_data['pKb_class_pred'][idx])\n",
    "            self.logsol_list.append(pd_data['LogSol_pred'][idx])\n",
    "            self.wlogsol_list.append(pd_data['wLogSol_pred'][idx])\n",
    "        \n",
    "        # find_min_max(self.logd_list)\n",
    "        # find_min_max(self.logp_list)\n",
    "        # find_min_max(self.logsol_list)\n",
    "        # find_min_max(self.wlogsol_list)\n",
    "        # 5.75390625 -0.93310546875\n",
    "        # 7.89453125 -1.91796875\n",
    "        # 2.880859375 -0.54931640625\n",
    "        # 3.5625 -9.6171875\n",
    "        min_max_norm()\n",
    "        \n",
    "        \n",
    "        # for classfication task, to balance number of postive/negtive samples\n",
    "        num_p = 0\n",
    "        num_n = 0\n",
    "        for lab in self.value_list:\n",
    "            if lab == 1: num_p += 1\n",
    "            else: num_n += 1\n",
    "        print(f\"number of positive/negative samples {num_p}/{num_n}.\")\n",
    "            \n",
    "        \n",
    "        # get word embedding index (atoms) ps:only use one time and fix the dict result \n",
    "        # self.atom_dict = {}\n",
    "        # idx = 0 \n",
    "        # for i in range(len(self._keys)):\n",
    "        #     datapoint_pickled = env.begin().get(self._keys[i])\n",
    "        #     data = pickle.loads(datapoint_pickled)\n",
    "        #     for a in data['atoms']:\n",
    "        #         if a not in self.atom_dict:\n",
    "        #             self.atom_dict.update({a:idx})\n",
    "        #             idx += 1\n",
    "        self.atom_dict = {'Br': 0, 'C': 1, 'H': 2, 'N': 3, 'O': 4, 'F': 5, 'Cl': 6, 'S': 7, 'P': 8, 'I': 9, 'B': 10, 'Se': 11, 'Ar': 12, 'Kr': 13, 'Li': 14, 'Ne': 15, 'Xe': 16, 'Si': 17, 'Na': 18}\n",
    "        print(self.atom_dict)\n",
    "        \n",
    "        self.pad_len = pad_len\n",
    "        self.conf = conf # conformation\n",
    "        self.mode = mode\n",
    "        print(f\"{mode} set is initialized successfully. The max length of the atom is {self.max_len}. The number of dataset is {len(self._keys)}. Padding length is {self.pad_len}.\")\n",
    "        \n",
    "                    \n",
    "\n",
    "    def connect_db(self, lmdb_path, save_to_self=False):\n",
    "        env = lmdb.open(\n",
    "            lmdb_path,\n",
    "            subdir=False,\n",
    "            readonly=True,\n",
    "            lock=False,\n",
    "            readahead=False,\n",
    "            meminit=False,\n",
    "            max_readers=256,\n",
    "        )\n",
    "        if not save_to_self:\n",
    "            return env\n",
    "        else:\n",
    "            self.env = env\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._keys)\n",
    "    \n",
    "    def min_max_norm(self, x):\n",
    "        _min = x.min()\n",
    "        _max = x.max()\n",
    "        x = (x - _min) / (_max - _min)\n",
    "        return x\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if not hasattr(self, 'env'):\n",
    "            self.connect_db(self.db_path, save_to_self=True)\n",
    "        datapoint_pickled = self.env.begin().get(self._keys[idx])\n",
    "        data = pickle.loads(datapoint_pickled)\n",
    "        coordinates = torch.tensor(np.array(data['coordinates']), dtype=torch.float32)[:self.conf, :, :]\n",
    "        emb_idx = torch.tensor([self.atom_dict[atom] for atom in data['atoms']], dtype=torch.long)\n",
    "        cur_len = len(emb_idx)\n",
    "        \n",
    "        # shortest path distance\n",
    "        spd = torch.tensor(np.array(data[\"SPD\"]), dtype=torch.float32)\n",
    "        edge = torch.tensor(np.array(data[\"edge\"]), dtype=torch.float32) + torch.eye(cur_len)\n",
    "        \n",
    "        # random dropout atoms\n",
    "        if np.random.rand() < 0.5 and self.mode == \"train\" and cur_len > 100:\n",
    "            num_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "            num = random.choice(num_list)\n",
    "            emb_idx = emb_idx[-num:]\n",
    "            coordinates = coordinates[:, -num:,:]\n",
    "            spd = spd[-num:, -num:]\n",
    "            edge = edge[-num:, -num:]\n",
    "            cur_len = len(emb_idx)\n",
    "        \n",
    "        # padding\n",
    "        if cur_len < self.pad_len:\n",
    "            new_emb = torch.full(size=(self.pad_len,), fill_value=self.pad_len-1, dtype=torch.long)\n",
    "            new_emb[:cur_len] = emb_idx\n",
    "            \n",
    "            new_cor = torch.full(size=(self.conf, self.pad_len, 3), fill_value=0, dtype=torch.float32)\n",
    "            new_cor[:, :cur_len, :] = coordinates\n",
    "            \n",
    "            new_spd = torch.full(size=(self.pad_len, self.pad_len), fill_value=0, dtype=torch.float32)\n",
    "            new_spd[:cur_len, :cur_len] = spd\n",
    "            new_edge = torch.full(size=(self.pad_len, self.pad_len), fill_value=0, dtype=torch.float32)\n",
    "            new_edge[:cur_len, :cur_len] = edge\n",
    "        elif cur_len >= self.pad_len:\n",
    "            new_emb = emb_idx[:self.pad_len]\n",
    "            new_cor = coordinates[:, :self.pad_len, :]\n",
    "            new_spd = spd[:self.pad_len, :self.pad_len]\n",
    "            new_edge = edge[:self.pad_len, :self.pad_len]\n",
    "        \n",
    "        # Normalize and augment coordination\n",
    "        if self.mode == \"train\": # for random augmentation\n",
    "            weight_list = [0.1, 0.2, 0.3, 0.5, 0.7, 0.01, 0.001]\n",
    "            scale = random.choice(weight_list) \n",
    "            noise = scale * torch.randn_like(new_cor)\n",
    "            if np.random.rand() < 0.5:  # for add noise locally\n",
    "                mask = torch.randint_like(noise, 0, 2, dtype=torch.float32)\n",
    "                noise = noise * mask\n",
    "            new_cor = new_cor + noise\n",
    "            \n",
    "            # new_cor = new_cor + 1. * torch.rand_like(new_cor)\n",
    "        # new_cor = (self.min_max_norm(new_cor) - 0.5) / 0.5\n",
    "            \n",
    "        # to compute the pair relative distance\n",
    "        atom_expanded = new_cor.unsqueeze(2)  # shape (conf, pad_len, 1, 3)\n",
    "        coor_expanded = new_cor.unsqueeze(1)   # shape (conf, 1, pad_len, 3)\n",
    "        # distance = atom_expanded - coor_expanded\n",
    "        # distance = distance.permute(1, 0, 2, 3).reshape(-1, conf*pad_len*3)   # xyz \n",
    "        distance = torch.sqrt((atom_expanded - coor_expanded).pow(2).sum(dim=-1))   # x+y+z\n",
    "        distance = distance.permute(1, 0, 2).reshape(-1, self.conf*self.pad_len)\n",
    "        distance = (self.min_max_norm(distance) - 0.5) / 0.5\n",
    "        \n",
    "        label = torch.tensor(self.value_list[idx], dtype=torch.float32)\n",
    "        # label = (label - self.min_value) / (self.max_value - self.min_value)\n",
    "        \n",
    "        new_cor = (self.min_max_norm(new_cor) - 0.5) / 0.5\n",
    "        new_cor = new_cor.permute(1, 0, 2).reshape(-1, self.conf*3)\n",
    "        # new_spd = (self.min_max_norm(new_spd) - 0.5) / 0.5\n",
    "        # new_edge = (self.min_max_norm(new_edge) - 0.5) / 0.5\n",
    "       \n",
    "        \n",
    "        return {\"atoms\": new_emb, \"coordinate\": new_cor, \"distance\": distance, \"SPD\": new_spd, \"edge\": new_edge, \"label\": label}\n",
    "        \n",
    "\n",
    "    def worker_init_fn(self, worker_id):\n",
    "        np.random.seed(np.random.get_state()[1][0] + worker_id)\n",
    "\n",
    "\n",
    "class HERG_LMDBDataset(Dataset):\n",
    "    def __init__(self, conf, pad_len=200, mode=\"train\"):\n",
    "        if mode == \"train\":\n",
    "            db_path = \"/home/jovyan/prompts_learning/results/herg_cls_train.lmdb\"\n",
    "            csv_path = \"/home/jovyan/prompts_learning/herg_dataset/hERGDB_cls_train_data.csv\"\n",
    "        elif mode == \"val\":\n",
    "            db_path = \"/home/jovyan/prompts_learning/results/herg_cls_val.lmdb\"\n",
    "            csv_path = \"/home/jovyan/prompts_learning/herg_dataset/hERGDB_cls_valid_data.csv\"\n",
    "        elif mode == \"week1\":\n",
    "            db_path = \"/home/jovyan/prompts_learning/results/herg_cls_week1.lmdb\"\n",
    "            csv_path = \"/home/jovyan/prompts_learning/herg_dataset/hERGDB_cls_week1_1201.csv\"\n",
    "        elif mode == \"week2\":\n",
    "            db_path = \"/home/jovyan/prompts_learning/results/herg_cls_week2.lmdb\"\n",
    "            csv_path = \"/home/jovyan/prompts_learning/herg_dataset/hERGDB_cls_week2_1201.csv\"\n",
    "        elif mode == \"week3\":\n",
    "            db_path = \"/home/jovyan/prompts_learning/results/herg_cls_week3.lmdb\"\n",
    "            csv_path = \"/home/jovyan/prompts_learning/herg_dataset/hERGDB_cls_week3_1201.csv\"\n",
    "        elif mode == \"week4\":\n",
    "            db_path = \"/home/jovyan/prompts_learning/results/herg_cls_week4.lmdb\"\n",
    "            csv_path = \"/home/jovyan/prompts_learning/herg_dataset/hERGDB_cls_week4_1201.csv\"\n",
    "        pd_data = pd.read_csv(csv_path)\n",
    "        \n",
    "        self.db_path = db_path\n",
    "        assert os.path.isfile(self.db_path), \"{} not found\".format(\n",
    "            self.db_path\n",
    "        )\n",
    "        env = self.connect_db(self.db_path)\n",
    "        with env.begin() as txn:\n",
    "            self._keys = list(txn.cursor().iternext(values=False))\n",
    "        \n",
    "        # get correponding label from csv\n",
    "        self.value_list = []\n",
    "        self.max_len = 0\n",
    "        \n",
    "        self.max_value = 0.\n",
    "        self.min_value = 999.\n",
    "        for i in range(len(self._keys)):\n",
    "            datapoint_pickled = env.begin().get(self._keys[i])\n",
    "            data = pickle.loads(datapoint_pickled)\n",
    "            current_len = len(data['atoms'])\n",
    "            if self.max_len < current_len:\n",
    "                self.max_len = current_len\n",
    "            idx = pd_data['smiles'][pd_data['smiles']==data['smi']].index[0]\n",
    "            self.value_list.append(pd_data['class'][idx])\n",
    "            \n",
    "            # for normalization of reprogression task\n",
    "            # if self.max_value < pd_data['value'][idx]:\n",
    "            #     self.max_value = pd_data['value'][idx]\n",
    "            # if self.min_value > pd_data['value'][idx]:\n",
    "            #     self.min_value = pd_data['value'][idx]\n",
    "        \n",
    "        # for classfication task, to balance number of postive/negtive samples\n",
    "        num_p = 0\n",
    "        num_n = 0\n",
    "        for lab in self.value_list:\n",
    "            if lab == 1: num_p += 1\n",
    "            else: num_n += 1\n",
    "        print(f\"number of positive/negative samples {num_p}/{num_n}.\")\n",
    "            \n",
    "        \n",
    "        # get word embedding index (atoms) ps:only use one time and fix the dict result \n",
    "        # self.atom_dict = {}\n",
    "        # idx = 0 \n",
    "        # for i in range(len(self._keys)):\n",
    "        #     datapoint_pickled = env.begin().get(self._keys[i])\n",
    "        #     data = pickle.loads(datapoint_pickled)\n",
    "        #     for a in data['atoms']:\n",
    "        #         if a not in self.atom_dict:\n",
    "        #             self.atom_dict.update({a:idx})\n",
    "        #             idx += 1\n",
    "        self.atom_dict = {'Br': 0, 'C': 1, 'H': 2, 'N': 3, 'O': 4, 'F': 5, 'Cl': 6, 'S': 7, 'P': 8, 'I': 9, 'B': 10, 'Se': 11, 'Ar': 12, 'Kr': 13, 'Li': 14, 'Ne': 15, 'Xe': 16, 'Si': 17, 'Na': 18}\n",
    "        print(self.atom_dict)\n",
    "        \n",
    "        self.pad_len = pad_len\n",
    "        self.conf = conf # conformation\n",
    "        self.mode = mode\n",
    "        print(f\"{mode} set is initialized successfully. The max length of the atom is {self.max_len}. The number of dataset is {len(self._keys)}. Padding length is {self.pad_len}.\")\n",
    "        \n",
    "                    \n",
    "\n",
    "    def connect_db(self, lmdb_path, save_to_self=False):\n",
    "        env = lmdb.open(\n",
    "            lmdb_path,\n",
    "            subdir=False,\n",
    "            readonly=True,\n",
    "            lock=False,\n",
    "            readahead=False,\n",
    "            meminit=False,\n",
    "            max_readers=256,\n",
    "        )\n",
    "        if not save_to_self:\n",
    "            return env\n",
    "        else:\n",
    "            self.env = env\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._keys)\n",
    "    \n",
    "    def min_max_norm(self, x):\n",
    "        _min = x.min()\n",
    "        _max = x.max()\n",
    "        x = (x - _min) / (_max - _min)\n",
    "        return x\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if not hasattr(self, 'env'):\n",
    "            self.connect_db(self.db_path, save_to_self=True)\n",
    "        datapoint_pickled = self.env.begin().get(self._keys[idx])\n",
    "        data = pickle.loads(datapoint_pickled)\n",
    "        coordinates = torch.tensor(np.array(data['coordinates']), dtype=torch.float32)[:self.conf, :, :]\n",
    "        emb_idx = torch.tensor([self.atom_dict[atom] for atom in data['atoms']], dtype=torch.long)\n",
    "        cur_len = len(emb_idx)\n",
    "        \n",
    "        # shortest path distance\n",
    "        spd = torch.tensor(np.array(data[\"SPD\"]), dtype=torch.float32)\n",
    "        edge = torch.tensor(np.array(data[\"edge\"]), dtype=torch.float32) + torch.eye(cur_len)\n",
    "        \n",
    "        # random dropout atoms\n",
    "        if np.random.rand() < 0.5 and self.mode == \"train\" and cur_len > 100:\n",
    "            num_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "            num = random.choice(num_list)\n",
    "            emb_idx = emb_idx[-num:]\n",
    "            coordinates = coordinates[:, -num:,:]\n",
    "            spd = spd[-num:, -num:]\n",
    "            edge = edge[-num:, -num:]\n",
    "            cur_len = len(emb_idx)\n",
    "        \n",
    "        # padding\n",
    "        if cur_len < self.pad_len:\n",
    "            new_emb = torch.full(size=(self.pad_len,), fill_value=self.pad_len-1, dtype=torch.long)\n",
    "            new_emb[:cur_len] = emb_idx\n",
    "            \n",
    "            new_cor = torch.full(size=(self.conf, self.pad_len, 3), fill_value=0, dtype=torch.float32)\n",
    "            new_cor[:, :cur_len, :] = coordinates\n",
    "            \n",
    "            new_spd = torch.full(size=(self.pad_len, self.pad_len), fill_value=0, dtype=torch.float32)\n",
    "            new_spd[:cur_len, :cur_len] = spd\n",
    "            new_edge = torch.full(size=(self.pad_len, self.pad_len), fill_value=0, dtype=torch.float32)\n",
    "            new_edge[:cur_len, :cur_len] = edge\n",
    "        elif cur_len >= self.pad_len:\n",
    "            new_emb = emb_idx[:self.pad_len]\n",
    "            new_cor = coordinates[:, :self.pad_len, :]\n",
    "            new_spd = spd[:self.pad_len, :self.pad_len]\n",
    "            new_edge = edge[:self.pad_len, :self.pad_len]\n",
    "        \n",
    "        # Normalize and augment coordination\n",
    "        if self.mode == \"train\": # for random augmentation\n",
    "            weight_list = [0.1, 0.2, 0.3, 0.5, 0.7, 0.01, 0.001]\n",
    "            scale = random.choice(weight_list) \n",
    "            noise = scale * torch.randn_like(new_cor)\n",
    "            if np.random.rand() < 0.5:  # for add noise locally\n",
    "                mask = torch.randint_like(noise, 0, 2, dtype=torch.float32)\n",
    "                noise = noise * mask\n",
    "            new_cor = new_cor + noise\n",
    "            \n",
    "            # new_cor = new_cor + 1. * torch.rand_like(new_cor)\n",
    "        # new_cor = (self.min_max_norm(new_cor) - 0.5) / 0.5\n",
    "            \n",
    "        # to compute the pair relative distance\n",
    "        atom_expanded = new_cor.unsqueeze(2)  # shape (conf, pad_len, 1, 3)\n",
    "        coor_expanded = new_cor.unsqueeze(1)   # shape (conf, 1, pad_len, 3)\n",
    "        # distance = atom_expanded - coor_expanded\n",
    "        # distance = distance.permute(1, 0, 2, 3).reshape(-1, conf*pad_len*3)   # xyz \n",
    "        distance = torch.sqrt((atom_expanded - coor_expanded).pow(2).sum(dim=-1))   # x+y+z\n",
    "        distance = distance.permute(1, 0, 2).reshape(-1, self.conf*self.pad_len)\n",
    "        distance = (self.min_max_norm(distance) - 0.5) / 0.5\n",
    "        \n",
    "        label = torch.tensor(self.value_list[idx], dtype=torch.float32)\n",
    "        # label = (label - self.min_value) / (self.max_value - self.min_value)\n",
    "        \n",
    "        new_cor = (self.min_max_norm(new_cor) - 0.5) / 0.5\n",
    "        new_cor = new_cor.permute(1, 0, 2).reshape(-1, self.conf*3)\n",
    "        # new_spd = (self.min_max_norm(new_spd) - 0.5) / 0.5\n",
    "        # new_edge = (self.min_max_norm(new_edge) - 0.5) / 0.5\n",
    "       \n",
    "        \n",
    "        return {\"atoms\": new_emb, \"coordinate\": new_cor, \"distance\": distance, \"SPD\": new_spd, \"edge\": new_edge, \"label\": label}\n",
    "        \n",
    "\n",
    "    def worker_init_fn(self, worker_id):\n",
    "        np.random.seed(np.random.get_state()[1][0] + worker_id)\n",
    "\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    # train_dataset = HERG_LMDBDataset(conf=10, pad_len=150, mode=\"train\")\n",
    "    train_dataset = HERG_Multi_Class_LMDBDataset(conf=10, pad_len=150, mode=\"train\")\n",
    "    train_set = torch.utils.data.DataLoader(train_dataset,\n",
    "                                                batch_size=2,\n",
    "                                                shuffle=False,\n",
    "                                                pin_memory=True,\n",
    "                                                num_workers=0,\n",
    "                                                # collate_fn=train_dataset.collate_fn,\n",
    "                                                worker_init_fn=train_dataset.worker_init_fn\n",
    "                                                )\n",
    "    # for datas in train_set:\n",
    "    #     print(datas[\"atoms\"].shape)\n",
    "    #     # print(datas[\"coordinate\"].shape)\n",
    "    #     # print(datas[\"label\"])\n",
    "    #     # exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc4b423-4df6-4369-a41c-e3c17d33abb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lmdb\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "data = pd.read_csv(\"/home/jovyan/prompts_learning/bbb_cls_final_data_multi_class.csv\")\n",
    "# data = pd.read_csv(\"/home/jovyan/PharmaBench/data/final_datasets/bbb_cls_final_data.csv\")\n",
    "scaffold_training = data[data['scaffold_train_test_label'] == 'train']\n",
    "scaffold_test = data[data['scaffold_train_test_label'] == 'test']\n",
    "scaffold_training = scaffold_training.reset_index()\n",
    "scaffold_test = scaffold_test.reset_index()\n",
    "\n",
    "\n",
    "def find_min_max(x_list):\n",
    "    x_np = np.array(x_list)\n",
    "    print(x_np.max(), x_np.min())\n",
    "    \n",
    "\n",
    "def list_min_max_norm(x_list, max_v, min_v):\n",
    "    x_np = np.array(x_list)\n",
    "    x_np = (x_np - min_v) / (max_v - min_v)\n",
    "    print(x_np.min(), x_np.max(0))\n",
    "    x_list = x_np.tolist()\n",
    "    return x_list\n",
    "\n",
    "\n",
    "class LMDBDataset(Dataset):\n",
    "    def __init__(self, db_path, conf, pad_len=200, mode=\"train\"):\n",
    "        self.db_path = db_path\n",
    "        assert os.path.isfile(self.db_path), \"{} not found\".format(\n",
    "            self.db_path\n",
    "        )\n",
    "        env = self.connect_db(self.db_path)\n",
    "        with env.begin() as txn:\n",
    "            self._keys = list(txn.cursor().iternext(values=False))\n",
    "        \n",
    "        # get correponding label from csv\n",
    "        self.value_list = []\n",
    "        self.logd_list = []\n",
    "        self.logp_list = []\n",
    "        self.pka_list = []\n",
    "        self.pkb_list = []\n",
    "        self.logsol_list = []\n",
    "        self.wlogsol_list = []\n",
    "        self.max_len = 0\n",
    "        if mode == \"train\":\n",
    "            pd_data = scaffold_training\n",
    "        else:\n",
    "            pd_data = scaffold_test\n",
    "        \n",
    "        self.max_value = 0.\n",
    "        self.min_value = 999.\n",
    "        for i in range(len(self._keys)):\n",
    "            datapoint_pickled = env.begin().get(self._keys[i])\n",
    "            data = pickle.loads(datapoint_pickled)\n",
    "            current_len = len(data['atoms'])\n",
    "            if self.max_len < current_len:\n",
    "                self.max_len = current_len\n",
    "            idx = pd_data['Smiles_unify'][pd_data['Smiles_unify']==data['smi']].index[0]\n",
    "            self.value_list.append(pd_data['value'][idx])\n",
    "            \n",
    "            self.logd_list.append(pd_data['LogD_pred'][idx])\n",
    "            self.logp_list.append(pd_data['LogP_pred'][idx])\n",
    "            self.pka_list.append(pd_data['pKa_class_pred'][idx])\n",
    "            self.pkb_list.append(pd_data['pKb_class_pred'][idx])\n",
    "            self.logsol_list.append(pd_data['LogSol_pred'][idx])\n",
    "            self.wlogsol_list.append(pd_data['wLogSol_pred'][idx])\n",
    "            \n",
    "            # # for normalization of reprogression task\n",
    "            # if self.max_value < pd_data['value'][idx]:\n",
    "            #     self.max_value = pd_data['value'][idx]\n",
    "            # if self.min_value > pd_data['value'][idx]:\n",
    "            #     self.min_value = pd_data['value'][idx]\n",
    "        \n",
    "        \n",
    "        # find_min_max(self.logd_list)\n",
    "        # find_min_max(self.logp_list)\n",
    "        # find_min_max(self.logsol_list)\n",
    "        # find_min_max(self.wlogsol_list)\n",
    "        # 8.7890625 -3.41796875\n",
    "        # 12.796875 -5.53125\n",
    "        # 3.017578125 -0.51806640625\n",
    "        # 3.92578125 -12.3984375\n",
    "        self.logd_list = list_min_max_norm(self.logd_list, 8.7890625, -3.41796875)\n",
    "        self.logp_list = list_min_max_norm(self.logp_list, 12.796875, -5.53125)\n",
    "        self.logsol_list = list_min_max_norm(self.logsol_list, 3.017578125, -0.51806640625)\n",
    "        self.wlogsol_list = list_min_max_norm(self.wlogsol_list, 3.92578125, -12.3984375)\n",
    "        \n",
    "        # for classfication task, to balance number of postive/negtive samples\n",
    "        num_p = 0\n",
    "        num_n = 0\n",
    "        for lab in self.value_list:\n",
    "            if lab == 1: num_p += 1\n",
    "            else: num_n += 1\n",
    "        print(f\"number of postive/negtive samples {num_p}/{num_n}.\")\n",
    "            \n",
    "        \n",
    "        # get word embedding index (atoms) ps:only use one time and fix the dict result \n",
    "        # self.atom_dict = {}\n",
    "        # idx = 0 \n",
    "        # for i in range(len(self._keys)):\n",
    "        #     datapoint_pickled = env.begin().get(self._keys[i])\n",
    "        #     data = pickle.loads(datapoint_pickled)\n",
    "        #     for a in data['atoms']:\n",
    "        #         if a not in self.atom_dict:\n",
    "        #             self.atom_dict.update({a:idx})\n",
    "        #             idx += 1\n",
    "        # self.atom_dict = {'Br': 0, 'C': 1, 'H': 2, 'N': 3, 'O': 4, 'F': 5, 'Cl': 6, 'S': 7, 'P': 8, 'I': 9, 'B': 10, 'Se': 11, 'Ar': 12, 'Kr': 13, 'Li': 14, 'Ne': 15, 'Xe': 16, 'Si': 17}\n",
    "        \n",
    "        self.atom_dict = {'Br': 0, 'C': 1, 'H': 2, 'N': 3, 'O': 4, 'F': 5, 'Cl': 6, 'S': 7, 'P': 8, 'I': 9, 'B': 10, 'Se': 11, 'Ar': 12, 'Kr': 13, 'Li': 14, 'Ne': 15, 'Xe': 16, 'Si': 17, 'Na': 18}\n",
    "        print(self.atom_dict)\n",
    "        \n",
    "        self.pad_len = pad_len\n",
    "        self.conf = conf # conformation\n",
    "        self.mode = mode\n",
    "        print(f\"{mode} set is initialized successfully. The max length of the atom is {self.max_len}. The number of dataset is {len(self._keys)}. Padding length is {self.pad_len}.\")\n",
    "        \n",
    "                    \n",
    "\n",
    "    def connect_db(self, lmdb_path, save_to_self=False):\n",
    "        env = lmdb.open(\n",
    "            lmdb_path,\n",
    "            subdir=False,\n",
    "            readonly=True,\n",
    "            lock=False,\n",
    "            readahead=False,\n",
    "            meminit=False,\n",
    "            max_readers=256,\n",
    "        )\n",
    "        if not save_to_self:\n",
    "            return env\n",
    "        else:\n",
    "            self.env = env\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._keys)\n",
    "    \n",
    "    def min_max_norm(self, x):\n",
    "        _min = x.min()\n",
    "        _max = x.max()\n",
    "        x = (x - _min) / (_max - _min)\n",
    "        return x\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if not hasattr(self, 'env'):\n",
    "            self.connect_db(self.db_path, save_to_self=True)\n",
    "        datapoint_pickled = self.env.begin().get(self._keys[idx])\n",
    "        data = pickle.loads(datapoint_pickled)\n",
    "        coordinates = torch.tensor(np.array(data['coordinates']), dtype=torch.float32)[:self.conf, :, :]\n",
    "        emb_idx = torch.tensor([self.atom_dict[atom] for atom in data['atoms']], dtype=torch.long)\n",
    "        cur_len = len(emb_idx)\n",
    "        \n",
    "        # shortest path distance\n",
    "        spd = torch.tensor(np.array(data[\"SPD\"]), dtype=torch.float32)\n",
    "        edge = torch.tensor(np.array(data[\"edge\"]), dtype=torch.float32) + torch.eye(cur_len)\n",
    "        \n",
    "        # random dropout atoms\n",
    "        if np.random.rand() < 0.5 and self.mode == \"train\" and cur_len > 100:\n",
    "            num_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "            num = random.choice(num_list)\n",
    "            emb_idx = emb_idx[-num:]\n",
    "            coordinates = coordinates[:, -num:,:]\n",
    "            spd = spd[-num:, -num:]\n",
    "            edge = edge[-num:, -num:]\n",
    "            cur_len = len(emb_idx)\n",
    "        \n",
    "        # padding\n",
    "        if cur_len < self.pad_len:\n",
    "            new_emb = torch.full(size=(self.pad_len,), fill_value=self.pad_len-1, dtype=torch.long)\n",
    "            new_emb[:cur_len] = emb_idx\n",
    "            \n",
    "            new_cor = torch.full(size=(self.conf, self.pad_len, 3), fill_value=0, dtype=torch.float32)\n",
    "            new_cor[:, :cur_len, :] = coordinates\n",
    "            \n",
    "            new_spd = torch.full(size=(self.pad_len, self.pad_len), fill_value=0, dtype=torch.float32)\n",
    "            new_spd[:cur_len, :cur_len] = spd\n",
    "            new_edge = torch.full(size=(self.pad_len, self.pad_len), fill_value=0, dtype=torch.float32)\n",
    "            new_edge[:cur_len, :cur_len] = edge\n",
    "        elif cur_len >= self.pad_len:\n",
    "            new_emb = emb_idx[:self.pad_len]\n",
    "            new_cor = coordinates[:, :self.pad_len, :]\n",
    "            new_spd = spd[:self.pad_len, :self.pad_len]\n",
    "            new_edge = edge[:self.pad_len, :self.pad_len]\n",
    "        \n",
    "        # Normalize and augment coordination\n",
    "        if self.mode == \"train\": # for random augmentation\n",
    "            weight_list = [0.1, 0.2, 0.3, 0.5, 0.7, 0.01, 0.001]\n",
    "            scale = random.choice(weight_list) \n",
    "            noise = scale * torch.randn_like(new_cor)\n",
    "            if np.random.rand() < 0.5:  # for add noise locally\n",
    "                mask = torch.randint_like(noise, 0, 2, dtype=torch.float32)\n",
    "                noise = noise * mask\n",
    "            new_cor = new_cor + noise\n",
    "            \n",
    "            # new_cor = new_cor + 1. * torch.rand_like(new_cor)\n",
    "        # new_cor = (self.min_max_norm(new_cor) - 0.5) / 0.5\n",
    "            \n",
    "        # to compute the pair relative distance\n",
    "        atom_expanded = new_cor.unsqueeze(2)  # shape (conf, pad_len, 1, 3)\n",
    "        coor_expanded = new_cor.unsqueeze(1)   # shape (conf, 1, pad_len, 3)\n",
    "        # distance = atom_expanded - coor_expanded\n",
    "        # distance = distance.permute(1, 0, 2, 3).reshape(-1, conf*pad_len*3)   # xyz \n",
    "        distance = torch.sqrt((atom_expanded - coor_expanded).pow(2).sum(dim=-1))   # x+y+z\n",
    "        distance = distance.permute(1, 0, 2).reshape(-1, self.conf*self.pad_len)\n",
    "        distance = (self.min_max_norm(distance) - 0.5) / 0.5\n",
    "        \n",
    "        label = torch.tensor(self.value_list[idx], dtype=torch.float32)\n",
    "        logd = torch.tensor(self.logd_list[idx], dtype=torch.float32)\n",
    "        logp = torch.tensor(self.logp_list[idx], dtype=torch.float32)\n",
    "        pka = torch.tensor(self.pka_list[idx], dtype=torch.float32)\n",
    "        pkb = torch.tensor(self.pkb_list[idx], dtype=torch.float32)\n",
    "        logsol = torch.tensor(self.logsol_list[idx], dtype=torch.float32)\n",
    "        wlogsol = torch.tensor(self.wlogsol_list[idx], dtype=torch.float32)\n",
    "        # label = (label - self.min_value) / (self.max_value - self.min_value)\n",
    "        \n",
    "        new_cor = (self.min_max_norm(new_cor) - 0.5) / 0.5\n",
    "        new_cor = new_cor.permute(1, 0, 2).reshape(-1, self.conf*3)\n",
    "        # new_spd = (self.min_max_norm(new_spd) - 0.5) / 0.5\n",
    "        # new_edge = (self.min_max_norm(new_edge) - 0.5) / 0.5\n",
    "       \n",
    "        \n",
    "        return {\"atoms\": new_emb, \"coordinate\": new_cor, \"distance\": distance, \"SPD\": new_spd, \"edge\": new_edge, \"label\": label,\\\n",
    "               \"logd\": logd, \"logp\": logp, \"pka\": pka, \"pkb\": pkb, \"logsol\": wlogsol, \"wlogsol\": wlogsol}\n",
    "        # return {\"atoms\": new_emb, \"coordinate\": new_cor, \"distance\": distance, \"SPD\": new_spd, \"edge\": new_edge, \"label\": label}\n",
    "        \n",
    "\n",
    "    def worker_init_fn(self, worker_id):\n",
    "        np.random.seed(np.random.get_state()[1][0] + worker_id)\n",
    "\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    lmdb_file = './results/bbb_train.lmdb'\n",
    "    train_dataset = LMDBDataset(lmdb_file, conf=10, pad_len=150, mode=\"train\")\n",
    "    train_set = torch.utils.data.DataLoader(train_dataset,\n",
    "                                                batch_size=2,\n",
    "                                                shuffle=False,\n",
    "                                                pin_memory=True,\n",
    "                                                num_workers=0,\n",
    "                                                # collate_fn=train_dataset.collate_fn,\n",
    "                                                worker_init_fn=train_dataset.worker_init_fn\n",
    "                                                )\n",
    "    for datas in train_set:\n",
    "        print(datas[\"atoms\"].shape)\n",
    "        # print(datas[\"coordinate\"].shape)\n",
    "        # print(datas[\"label\"])\n",
    "        exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cf3806-5bcf-4bde-ae41-8a9aad688c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lmdb\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "data = pd.read_csv(\"/home/jovyan/prompts_learning/bbb_cls_final_data_multi_class.csv\")\n",
    "# data = pd.read_csv(\"/home/jovyan/PharmaBench/data/final_datasets/bbb_cls_final_data.csv\")\n",
    "scaffold_training = data[data['scaffold_train_test_label'] == 'train']\n",
    "scaffold_test = data[data['scaffold_train_test_label'] == 'test']\n",
    "scaffold_training = scaffold_training.reset_index()\n",
    "scaffold_test = scaffold_test.reset_index()\n",
    "\n",
    "\n",
    "\n",
    "class LMDBDataset(Dataset):\n",
    "    def __init__(self, db_path, conf, pad_len=200, mode=\"train\"):\n",
    "        self.db_path = db_path\n",
    "        assert os.path.isfile(self.db_path), \"{} not found\".format(\n",
    "            self.db_path\n",
    "        )\n",
    "        env = self.connect_db(self.db_path)\n",
    "        with env.begin() as txn:\n",
    "            self._keys = list(txn.cursor().iternext(values=False))\n",
    "        \n",
    "        # get correponding label from csv\n",
    "        self.value_list = []\n",
    "        self.max_len = 0\n",
    "        if mode == \"train\":\n",
    "            pd_data = scaffold_training\n",
    "        else:\n",
    "            pd_data = scaffold_test\n",
    "        \n",
    "        self.max_value = 0.\n",
    "        self.min_value = 999.\n",
    "        for i in range(len(self._keys)):\n",
    "            datapoint_pickled = env.begin().get(self._keys[i])\n",
    "            data = pickle.loads(datapoint_pickled)\n",
    "            current_len = len(data['atoms'])\n",
    "            if self.max_len < current_len:\n",
    "                self.max_len = current_len\n",
    "            idx = pd_data['Smiles_unify'][pd_data['Smiles_unify']==data['smi']].index[0]\n",
    "            self.value_list.append(pd_data['value'][idx])\n",
    "            \n",
    "  \n",
    "\n",
    "            \n",
    "        self.atom_dict = {'Br': 0, 'C': 1, 'H': 2, 'N': 3, 'O': 4, 'F': 5, 'Cl': 6, 'S': 7, 'P': 8, 'I': 9, 'B': 10, 'Se': 11, 'Ar': 12, 'Kr': 13, 'Li': 14, 'Ne': 15, 'Xe': 16, 'Si': 17, 'Na': 18, 'Mask': 19}\n",
    "        print(self.atom_dict)\n",
    "        self.mask_token_id = self.atom_dict['Mask']\n",
    "        \n",
    "        self.pad_len = pad_len\n",
    "        self.conf = conf # conformation\n",
    "        self.mode = mode\n",
    "        print(f\"{mode} set is initialized successfully. The max length of the atom is {self.max_len}. The number of dataset is {len(self._keys)}. Padding length is {self.pad_len}.\")\n",
    "        \n",
    "                    \n",
    "\n",
    "    def connect_db(self, lmdb_path, save_to_self=False):\n",
    "        env = lmdb.open(\n",
    "            lmdb_path,\n",
    "            subdir=False,\n",
    "            readonly=True,\n",
    "            lock=False,\n",
    "            readahead=False,\n",
    "            meminit=False,\n",
    "            max_readers=256,\n",
    "        )\n",
    "        if not save_to_self:\n",
    "            return env\n",
    "        else:\n",
    "            self.env = env\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._keys)\n",
    "    \n",
    "    def min_max_norm(self, x):\n",
    "        _min = x.min()\n",
    "        _max = x.max()\n",
    "        x = (x - _min) / (_max - _min)\n",
    "        return x\n",
    "    \n",
    "    def random_mask(self, atoms, mask_ratio=0.2):\n",
    "        print(\"before\", atoms)\n",
    "        len_masked = int(len(atoms) * mask_ratio)\n",
    "        noise = np.random.rand(len(atoms))  # noise in [0, 1]\n",
    "        \n",
    "        # sort noise for each sample\n",
    "        ids_masked = np.argsort(noise)[:len_masked]  # ascend: small is keep, large is remove\n",
    "        output_label = []\n",
    "        \n",
    "        output_label = atoms[ids_masked]\n",
    "        atoms[ids_masked] = self.mask_token_id\n",
    "        print(\"masked\", atoms)\n",
    "        print(\"lab\", output_label)\n",
    "\n",
    "        return atoms, output_label\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if not hasattr(self, 'env'):\n",
    "            self.connect_db(self.db_path, save_to_self=True)\n",
    "        datapoint_pickled = self.env.begin().get(self._keys[idx])\n",
    "        data = pickle.loads(datapoint_pickled)\n",
    "        coordinates = torch.tensor(np.array(data['coordinates']), dtype=torch.float32)[:self.conf, :, :]\n",
    "        emb_idx = torch.tensor([self.atom_dict[atom] for atom in data['atoms']], dtype=torch.long)\n",
    "        cur_len = len(emb_idx)\n",
    "        \n",
    "        # shortest path distance\n",
    "        spd = torch.tensor(np.array(data[\"SPD\"]), dtype=torch.float32)\n",
    "        edge = torch.tensor(np.array(data[\"edge\"]), dtype=torch.float32) + torch.eye(cur_len)\n",
    "        \n",
    "        # random dropout atoms\n",
    "        if np.random.rand() < 0.5 and self.mode == \"train\" and cur_len > 100:\n",
    "            num_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "            num = random.choice(num_list)\n",
    "            emb_idx = emb_idx[-num:]\n",
    "            coordinates = coordinates[:, -num:,:]\n",
    "            spd = spd[-num:, -num:]\n",
    "            edge = edge[-num:, -num:]\n",
    "            cur_len = len(emb_idx)\n",
    "            \n",
    "        \n",
    "        # padding\n",
    "        if cur_len < self.pad_len:\n",
    "            new_emb = torch.full(size=(self.pad_len,), fill_value=self.pad_len-1, dtype=torch.long)\n",
    "            new_emb[:cur_len] = emb_idx\n",
    "            \n",
    "            new_cor = torch.full(size=(self.conf, self.pad_len, 3), fill_value=0, dtype=torch.float32)\n",
    "            new_cor[:, :cur_len, :] = coordinates\n",
    "            \n",
    "            new_spd = torch.full(size=(self.pad_len, self.pad_len), fill_value=0, dtype=torch.float32)\n",
    "            new_spd[:cur_len, :cur_len] = spd\n",
    "            new_edge = torch.full(size=(self.pad_len, self.pad_len), fill_value=0, dtype=torch.float32)\n",
    "            new_edge[:cur_len, :cur_len] = edge\n",
    "        elif cur_len >= self.pad_len:\n",
    "            new_emb = emb_idx[:self.pad_len]\n",
    "            new_cor = coordinates[:, :self.pad_len, :]\n",
    "            new_spd = spd[:self.pad_len, :self.pad_len]\n",
    "            new_edge = edge[:self.pad_len, :self.pad_len]\n",
    "        \n",
    "        # Normalize and augment coordination\n",
    "        if self.mode == \"train\": # for random augmentation\n",
    "            weight_list = [0.1, 0.2, 0.3, 0.5, 0.7, 0.01, 0.001]\n",
    "            # weight_list = [0.1, 0.2, 0.3, 0.01, 0.001]\n",
    "            scale = random.choice(weight_list) \n",
    "            noise = scale * torch.randn_like(new_cor)\n",
    "            if np.random.rand() < 0.5:  # for add noise locally\n",
    "                mask = torch.randint_like(noise, 0, 2, dtype=torch.float32)\n",
    "                noise = noise * mask\n",
    "            new_cor = new_cor + noise\n",
    "            \n",
    "            \n",
    "        # to compute the pair relative distance\n",
    "        atom_expanded = new_cor.unsqueeze(2)  # shape (conf, pad_len, 1, 3)\n",
    "        coor_expanded = new_cor.unsqueeze(1)   # shape (conf, 1, pad_len, 3)\n",
    "        # distance = atom_expanded - coor_expanded\n",
    "        # distance = distance.permute(1, 0, 2, 3).reshape(-1, conf*pad_len*3)   # xyz \n",
    "        distance = torch.sqrt((atom_expanded - coor_expanded).pow(2).sum(dim=-1))   # x+y+z\n",
    "        distance = distance.permute(1, 0, 2).reshape(-1, self.conf*self.pad_len)\n",
    "        distance = (self.min_max_norm(distance) - 0.5) / 0.5\n",
    "        \n",
    "        label = torch.tensor(self.value_list[idx], dtype=torch.float32)\n",
    "        \n",
    "        new_cor = (self.min_max_norm(new_cor) - 0.5) / 0.5\n",
    "        new_cor = new_cor.permute(1, 0, 2).reshape(-1, self.conf*3)\n",
    "        \n",
    "        # for pre-training task 1: atom prediction\n",
    "        new_emb, masked_label = self.random_mask(new_emb)\n",
    "        # for pre-training task 2: noised distance prediction\n",
    "        weight_list = [0.1, 0.2, 0.3, 0.5, 0.7, 0.9, 0.01, 0.001]\n",
    "        scale = random.choice(weight_list) \n",
    "        noise_gt = scale * torch.randn_like(distance)\n",
    "        if np.random.rand() < 0.5:  # for add noise locally\n",
    "            mask = torch.randint_like(noise, 0, 2, dtype=torch.float32)\n",
    "            noise_gt = noise_gt * mask\n",
    "        distance = distance + noise_gt\n",
    "       \n",
    "        return {\"atoms\": new_emb, \"coordinate\": new_cor, \"distance\": distance, \"SPD\": new_spd, \"edge\": new_edge, \"label\": label,\\\n",
    "               \"masked_label\": masked_label, \"noise_gt\": noise_gt}\n",
    "\n",
    "        \n",
    "\n",
    "    def worker_init_fn(self, worker_id):\n",
    "        np.random.seed(np.random.get_state()[1][0] + worker_id)\n",
    "\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    lmdb_file = './results/bbb_train.lmdb'\n",
    "    train_dataset = LMDBDataset(lmdb_file, conf=10, pad_len=150, mode=\"train\")\n",
    "    train_set = torch.utils.data.DataLoader(train_dataset,\n",
    "                                                batch_size=2,\n",
    "                                                shuffle=False,\n",
    "                                                pin_memory=True,\n",
    "                                                num_workers=0,\n",
    "                                                # collate_fn=train_dataset.collate_fn,\n",
    "                                                worker_init_fn=train_dataset.worker_init_fn\n",
    "                                                )\n",
    "    for datas in train_set:\n",
    "        print(datas[\"atoms\"].shape)\n",
    "        # print(datas[\"coordinate\"].shape)\n",
    "        # print(datas[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74be4800-694b-4abc-94e1-b5247eebd03a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Br': 0, 'C': 1, 'H': 2, 'N': 3, 'O': 4, 'F': 5, 'Cl': 6, 'S': 7, 'P': 8, 'I': 9, 'B': 10, 'Se': 11, 'Ar': 12, 'Kr': 13, 'Li': 14, 'Ne': 15, 'Xe': 16, 'Si': 17, 'Na': 18, 'Mask': 19, 'Pad': 20}\n",
      "bbb_test set is initialized successfully. The max length of the atom is 196. The number of dataset is 1660. Padding length is 150.\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([2, 150])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([2, 150])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([2, 150])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([2, 150])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([2, 150])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([2, 150])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([2, 150])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([2, 150])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([2, 150])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([2, 150])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([2, 150])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([2, 150])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([2, 150])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([2, 150])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([2, 150])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([2, 150])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([2, 150])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([2, 150])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([2, 150])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([2, 150])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([2, 150])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([2, 150])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([2, 150])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([2, 150])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([2, 150])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([2, 150])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([2, 150])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([2, 150])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([2, 150])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([2, 150])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([2, 150])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([2, 150])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([2, 150])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([2, 150])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([2, 150])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([2, 150])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([2, 150])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([2, 150])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([2, 150])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([2, 150])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([2, 150])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([2, 150])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([2, 150])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([2, 150])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([2, 150])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([2, 150])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([2, 150])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([2, 150])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([2, 150])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([2, 150])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([2, 150])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([2, 150])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([2, 150])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([2, 150])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([2, 150])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([2, 150])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([2, 150])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([2, 150])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([2, 150])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([2, 150])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([2, 150])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([2, 150])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([2, 150])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([2, 150])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([2, 150])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([2, 150])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([2, 150])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([2, 150])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([2, 150])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([2, 150])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([2, 150])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([2, 150])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([2, 150])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([2, 150])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([2, 150])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([2, 150])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([150, 1500])\n",
      "torch.Size([2, 150])\n",
      "torch.Size([150, 1500])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import lmdb\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "data = pd.read_csv(\"/home/jovyan/prompts_learning/bbb_cls_final_data_multi_class.csv\")\n",
    "# data = pd.read_csv(\"/home/jovyan/PharmaBench/data/final_datasets/bbb_cls_final_data.csv\")\n",
    "scaffold_training = data[data['scaffold_train_test_label'] == 'train']\n",
    "scaffold_test = data[data['scaffold_train_test_label'] == 'test']\n",
    "scaffold_training = scaffold_training.reset_index()\n",
    "scaffold_test = scaffold_test.reset_index()\n",
    "\n",
    "\n",
    "\n",
    "class Pretrain_LMDBDataset(Dataset):\n",
    "    def __init__(self, conf, pad_len=150, mode=\"herg\"):\n",
    "        if mode == \"herg\":\n",
    "            db_path = \"/home/jovyan/prompts_learning/results/herg_cls_train.lmdb\"\n",
    "            csv_path = \"/home/jovyan/prompts_learning/herg_dataset/hERGDB_cls_train_data.csv\"\n",
    "            pd_data = pd.read_csv(csv_path)\n",
    "        elif mode == \"bbb\":\n",
    "            db_path = \"/home/jovyan/prompts_learning/results/bbb_train.lmdb\"\n",
    "            csv_path = \"/home/jovyan/prompts_learning/bbb_cls_final_data_multi_class.csv\"\n",
    "            data = pd.read_csv(csv_path)\n",
    "            scaffold_training = data[data['scaffold_train_test_label'] == 'train']\n",
    "            scaffold_training = scaffold_training.reset_index()\n",
    "            pd_data = scaffold_training\n",
    "        elif mode == \"logd\":\n",
    "            db_path = \"/home/jovyan/prompts_learning/results/logd_train.lmdb\"\n",
    "            csv_path = \"/home/jovyan/PharmaBench/data/final_datasets/logd_reg_final_data.csv\"\n",
    "            data = pd.read_csv(csv_path)\n",
    "            scaffold_training = data[data['scaffold_train_test_label'] == 'train']\n",
    "            scaffold_training = scaffold_training.reset_index()\n",
    "            pd_data = scaffold_training\n",
    "        elif mode == \"bbb_test\":\n",
    "            db_path = \"/home/jovyan/prompts_learning/results/bbb_test.lmdb\"\n",
    "            csv_path = \"/home/jovyan/prompts_learning/bbb_cls_final_data_multi_class.csv\"\n",
    "            data = pd.read_csv(csv_path)\n",
    "            scaffold_training = data[data['scaffold_train_test_label'] == 'test']\n",
    "            scaffold_training = scaffold_training.reset_index()\n",
    "            pd_data = scaffold_training\n",
    "        \n",
    "        \n",
    "        self.db_path = db_path\n",
    "        assert os.path.isfile(self.db_path), \"{} not found\".format(\n",
    "            self.db_path\n",
    "        )\n",
    "        env = self.connect_db(self.db_path)\n",
    "        with env.begin() as txn:\n",
    "            self._keys = list(txn.cursor().iternext(values=False))\n",
    "        \n",
    "        # get correponding label from csv\n",
    "        self.value_list = []\n",
    "        self.max_len = 0\n",
    "\n",
    "        \n",
    "        self.max_value = 0.\n",
    "        self.min_value = 999.\n",
    "        for i in range(len(self._keys)):\n",
    "            datapoint_pickled = env.begin().get(self._keys[i])\n",
    "            data = pickle.loads(datapoint_pickled)\n",
    "            current_len = len(data['atoms'])\n",
    "            if self.max_len < current_len:\n",
    "                self.max_len = current_len\n",
    "            if mode == \"herg\":\n",
    "                idx = pd_data['smiles'][pd_data['smiles']==data['smi']].index[0]\n",
    "                self.value_list.append(pd_data['class'][idx])\n",
    "            else:\n",
    "                idx = pd_data['Smiles_unify'][pd_data['Smiles_unify']==data['smi']].index[0]\n",
    "                self.value_list.append(pd_data['value'][idx])\n",
    "            \n",
    "        self.atom_dict = {'Br': 0, 'C': 1, 'H': 2, 'N': 3, 'O': 4, 'F': 5, 'Cl': 6, 'S': 7, 'P': 8, 'I': 9, 'B': 10, 'Se': 11, 'Ar': 12, 'Kr': 13, 'Li': 14, 'Ne': 15, 'Xe': 16, 'Si': 17, 'Na': 18, 'Mask': 19, \"Pad\": 20}\n",
    "        print(self.atom_dict)\n",
    "        self.mask_token_id = self.atom_dict['Mask']\n",
    "        self.pad_token_id = self.atom_dict[\"Pad\"]\n",
    "        \n",
    "        self.pad_len = pad_len\n",
    "        self.conf = conf # conformation\n",
    "        self.mode = mode\n",
    "        print(f\"{mode} set is initialized successfully. The max length of the atom is {self.max_len}. The number of dataset is {len(self._keys)}. Padding length is {self.pad_len}.\")\n",
    "        \n",
    "                    \n",
    "\n",
    "    def connect_db(self, lmdb_path, save_to_self=False):\n",
    "        env = lmdb.open(\n",
    "            lmdb_path,\n",
    "            subdir=False,\n",
    "            readonly=True,\n",
    "            lock=False,\n",
    "            readahead=False,\n",
    "            meminit=False,\n",
    "            max_readers=256,\n",
    "        )\n",
    "        if not save_to_self:\n",
    "            return env\n",
    "        else:\n",
    "            self.env = env\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._keys)\n",
    "    \n",
    "    def min_max_norm(self, x):\n",
    "        _min = x.min()\n",
    "        _max = x.max()\n",
    "        x = (x - _min) / (_max - _min)\n",
    "        return x\n",
    "    \n",
    "    def random_mask(self, atoms, dist, mask_ratio=0.75):\n",
    "        # print(\"before\", atoms)\n",
    "        len_masked = int(len(atoms) * mask_ratio)\n",
    "        noise = np.random.rand(len(atoms))  # noise in [0, 1]\n",
    "        \n",
    "        # sort noise for each sample\n",
    "        ids_masked = np.argsort(noise)[:len_masked]  # ascend: small is keep, large is remove\n",
    "        output_label = []\n",
    "        \n",
    "        output_label = atoms[ids_masked]\n",
    "        atoms[ids_masked] = self.mask_token_id\n",
    "        dist[ids_masked, :] = 0.\n",
    "        # print(\"masked\", atoms)\n",
    "        # print(\"lab\", output_label)\n",
    "\n",
    "        return atoms, dist, output_label, ids_masked\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if not hasattr(self, 'env'):\n",
    "            self.connect_db(self.db_path, save_to_self=True)\n",
    "        datapoint_pickled = self.env.begin().get(self._keys[idx])\n",
    "        data = pickle.loads(datapoint_pickled)\n",
    "        coordinates = torch.tensor(np.array(data['coordinates']), dtype=torch.float32)[:self.conf, :, :]\n",
    "        emb_idx = torch.tensor([self.atom_dict[atom] for atom in data['atoms']], dtype=torch.long)\n",
    "        cur_len = len(emb_idx)\n",
    "        \n",
    "        # shortest path distance\n",
    "        spd = torch.tensor(np.array(data[\"SPD\"]), dtype=torch.float32)\n",
    "        edge = torch.tensor(np.array(data[\"edge\"]), dtype=torch.float32) + torch.eye(cur_len)\n",
    "        \n",
    "        # random dropout atoms\n",
    "        if cur_len > 100:\n",
    "            num_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "            num = random.choice(num_list)\n",
    "            emb_idx = emb_idx[-num:]\n",
    "            coordinates = coordinates[:, -num:,:]\n",
    "            spd = spd[-num:, -num:]\n",
    "            edge = edge[-num:, -num:]\n",
    "            cur_len = len(emb_idx)\n",
    "            \n",
    "        \n",
    "        # padding\n",
    "        if cur_len < self.pad_len:\n",
    "            new_emb = torch.full(size=(self.pad_len,), fill_value=self.pad_token_id, dtype=torch.long)\n",
    "            new_emb[:cur_len] = emb_idx\n",
    "            \n",
    "            new_cor = torch.full(size=(self.conf, self.pad_len, 3), fill_value=0, dtype=torch.float32)\n",
    "            new_cor[:, :cur_len, :] = coordinates\n",
    "            \n",
    "            new_spd = torch.full(size=(self.pad_len, self.pad_len), fill_value=0, dtype=torch.float32)\n",
    "            new_spd[:cur_len, :cur_len] = spd\n",
    "            new_edge = torch.full(size=(self.pad_len, self.pad_len), fill_value=0, dtype=torch.float32)\n",
    "            new_edge[:cur_len, :cur_len] = edge\n",
    "        elif cur_len >= self.pad_len:\n",
    "            new_emb = emb_idx[:self.pad_len]\n",
    "            new_cor = coordinates[:, :self.pad_len, :]\n",
    "            new_spd = spd[:self.pad_len, :self.pad_len]\n",
    "            new_edge = edge[:self.pad_len, :self.pad_len]\n",
    "        \n",
    "        # Normalize and augment coordination\n",
    "      # for random augmentation\n",
    "        weight_list = [0.1, 0.2, 0.3, 0.5, 0.7, 0.01, 0.001]\n",
    "        # weight_list = [0.1, 0.2, 0.3, 0.01, 0.001]\n",
    "        scale = random.choice(weight_list) \n",
    "        noise = scale * torch.randn_like(new_cor)\n",
    "        if np.random.rand() < 0.5:  # for add noise locally\n",
    "            mask = torch.randint_like(noise, 0, 2, dtype=torch.float32)\n",
    "            noise = noise * mask\n",
    "        new_cor = new_cor + noise\n",
    "            \n",
    "            \n",
    "        # to compute the pair relative distance\n",
    "        atom_expanded = new_cor.unsqueeze(2)  # shape (conf, pad_len, 1, 3)\n",
    "        coor_expanded = new_cor.unsqueeze(1)   # shape (conf, 1, pad_len, 3)\n",
    "        # distance = atom_expanded - coor_expanded\n",
    "        # distance = distance.permute(1, 0, 2, 3).reshape(-1, conf*pad_len*3)   # xyz \n",
    "        distance = torch.sqrt((atom_expanded - coor_expanded).pow(2).sum(dim=-1))   # x+y+z\n",
    "        distance = distance.permute(1, 0, 2).reshape(-1, self.conf*self.pad_len)\n",
    "        distance = (self.min_max_norm(distance) - 0.5) / 0.5\n",
    "        \n",
    "        label = torch.tensor(self.value_list[idx], dtype=torch.float32)\n",
    "        \n",
    "        new_cor = (self.min_max_norm(new_cor) - 0.5) / 0.5\n",
    "        new_cor = new_cor.permute(1, 0, 2).reshape(-1, self.conf*3)\n",
    "        \n",
    "        # for pre-training task 1: atom prediction\n",
    "        new_emb, masked_distance, masked_label, ids_masked = self.random_mask(new_emb, distance)\n",
    "        # for pre-training task 2: noised distance prediction\n",
    "        # weight_list = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "        weight_list = [0]\n",
    "        scale = random.choice(weight_list) \n",
    "        noise_gt = scale * torch.randn_like(distance)\n",
    "        distance = distance # + noise_gt\n",
    "       \n",
    "        return {\"atoms\": new_emb, \"coordinate\": new_cor, \"masked_distance\": masked_distance, \"distance\": distance, \"SPD\": new_spd, \"edge\": new_edge, \"label\": label,\\\n",
    "               \"masked_label\": masked_label, \"ids_masked\": ids_masked, \"noise_gt\": noise_gt}\n",
    "\n",
    "        \n",
    "\n",
    "    def worker_init_fn(self, worker_id):\n",
    "        np.random.seed(np.random.get_state()[1][0] + worker_id)\n",
    "\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    train_dataset = Pretrain_LMDBDataset(conf=10, pad_len=150, mode=\"bbb_test\")\n",
    "    train_set = torch.utils.data.DataLoader(train_dataset,\n",
    "                                                batch_size=2,\n",
    "                                                shuffle=False,\n",
    "                                                pin_memory=True,\n",
    "                                                num_workers=0,\n",
    "                                                # collate_fn=train_dataset.collate_fn,\n",
    "                                                worker_init_fn=train_dataset.worker_init_fn\n",
    "                                                )\n",
    "    for datas in train_set:\n",
    "        print(datas[\"atoms\"].shape)\n",
    "        # print(datas[\"coordinate\"].shape)\n",
    "        # print(datas[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fa73f8-a648-4d1b-b01c-59bfaadaf8d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fcb",
   "language": "python",
   "name": "fcb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
