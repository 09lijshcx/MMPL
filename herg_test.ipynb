{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae03dd2f-3140-4295-9ae6-89d0a6f4028a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import gc\n",
    "from sklearn import metrics\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "# torch.autograd.set_detect_anomaly(True)\n",
    "CUDA_LAUNCH_BLOCKING=1\n",
    "\n",
    "\n",
    "# fixed random seed for reproduction\n",
    "seed = 10\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.enabled = False\n",
    "print('Random seed :', seed)\n",
    "\n",
    "from collections import OrderedDict\n",
    "@torch.no_grad()\n",
    "def update_ema(ema_model, model, decay=0.9999):\n",
    "    \"\"\"\n",
    "    Step the EMA model towards the current model.\n",
    "    \"\"\"\n",
    "    ema_params = OrderedDict(ema_model.named_parameters())\n",
    "    model_params = OrderedDict(model.named_parameters())\n",
    "\n",
    "    for name, param in model_params.items():\n",
    "        # TODO: Consider applying only to params that require_grad to avoid small numerical changes of pos_embed\n",
    "        ema_params[name].mul_(decay).add_(param.data, alpha=1 - decay)\n",
    "\n",
    "def requires_grad(model, flag=True):\n",
    "    \"\"\"\n",
    "    Set requires_grad flag for all parameters in a model.\n",
    "    \"\"\"\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = flag\n",
    "\n",
    "from sklearn import metrics\n",
    "def compute_AUC(y, pred, n_class=1):\n",
    "    # compute one score\n",
    "    if n_class == 1:\n",
    "        auc = metrics.roc_auc_score(y, pred)\n",
    "\n",
    "    # compute two-class\n",
    "    elif n_class == 2:\n",
    "        # pos = pred[:, 1]\n",
    "        auc = metrics.roc_auc_score(y, pred)\n",
    "    return auc\n",
    "\n",
    "pad_len = 150 # best\n",
    "conf = 10 # best# \n",
    "\n",
    "##########################################\n",
    "#########  construct dataloader  ######### \n",
    "##########################################\n",
    "\n",
    "from herg_cls_datapipeline import HERG_LMDBDataset \n",
    "# test_dataset = HERG_LMDBDataset(conf=conf, pad_len=pad_len, mode=\"week1\")\n",
    "# test_dataset = HERG_LMDBDataset(conf=conf, pad_len=pad_len, mode=\"week2\")\n",
    "# test_dataset = HERG_LMDBDataset(conf=conf, pad_len=pad_len, mode=\"week3\")\n",
    "test_dataset = HERG_LMDBDataset(conf=conf, pad_len=pad_len, mode=\"week4\")\n",
    "test_set = DataLoader(test_dataset,\n",
    "                    batch_size=32,\n",
    "                    drop_last=False,\n",
    "                    shuffle=False,\n",
    "                    pin_memory=True,\n",
    "                    num_workers=4,\n",
    "                    # collate_fn=train_dataset.collate_fn,\n",
    "                    worker_init_fn=test_dataset.worker_init_fn\n",
    "                    )\n",
    "\n",
    "\n",
    "##########################################\n",
    "######  build model and optimizer  ####### \n",
    "##########################################\n",
    "dev = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "from CLIP import clip\n",
    "clip_model, preprocess = clip.load(name=\"ViT-B/16\", device=\"cpu\", download_root=\"/home/jovyan/clip_download_root\")\n",
    "from model_zoo import CLIP_Protein\n",
    "model = CLIP_Protein(clip_model, conf, pad_len=pad_len).to(dev)\n",
    "\n",
    "# path = \"/home/jovyan/prompts_learning/trained_weight/HERG_cliP_Epoch14_val_auc_0.82784.pth\" # conf 200 prompts 1\n",
    "path = \"/home/jovyan/prompts_learning/trained_weight/A_10_9_HERG_prompts2_cliP_Epoch18_val_auc_0.82123.pth\" # 2 prompts\n",
    "# path = \"/home/jovyan/prompts_learning/trained_weight/HERG_cliP_Epoch18_val_auc_0.82686.pth\" # 4 prompts\n",
    "# path = \"/home/jovyan/prompts_learning/trained_weight/A_10_9_HERG_cliP_Epoch18_val_auc_0.82286.pth\"\n",
    "path  = \"/home/jovyan/prompts_learning/trained_weight/A_10_10_HERG_cliPm_Epoch43_val_auc_0.85830.pth\" # multi prompts\n",
    "# path = \"/home/jovyan/prompts_learning/trained_weight/HERG_cliPm_Epoch9_val_auc_0.86128.pth\"\n",
    "sd = torch.load(path)\n",
    "model.load_state_dict(sd)\n",
    "print(\"pre-trained weights loaded...\")\n",
    "\n",
    "                \n",
    "    \n",
    "##########################################\n",
    "####### start evaluating our model #######\n",
    "##########################################\n",
    "model.eval()\n",
    "print(\"evaluating...\")\n",
    "with torch.no_grad():\n",
    "    all_pred = None \n",
    "    all_lab = None\n",
    "    for step_id, datas in enumerate(test_set):\n",
    "            atoms = datas[\"atoms\"].to(dev, non_blocking=True).long()\n",
    "            # coord = datas[\"coordinate\"].to(dev, non_blocking=True).float()\n",
    "            pair = datas[\"distance\"].to(dev, non_blocking=True).float()\n",
    "            spd = datas[\"SPD\"].to(dev, non_blocking=True).float()\n",
    "            edge = datas[\"edge\"].to(dev, non_blocking=True).float()\n",
    "            label = datas[\"label\"].to(dev, non_blocking=True).float()\n",
    "\n",
    "            pred = model(atoms, pair, spd, edge)\n",
    "            pred = torch.sigmoid(pred)\n",
    "            # pred = torch.softmax(pred, dim=-1)[:, 1]\n",
    "\n",
    "            all_pred = pred if all_pred is None else torch.cat([all_pred, pred], dim=0)\n",
    "            all_lab = label if all_lab is None else torch.cat([all_lab, label], dim=0)\n",
    "auc = compute_AUC(all_lab.cpu().detach(), all_pred.cpu().detach())\n",
    "print(f\"test AUC: {auc:.5f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a748406-c1f7-4ae6-b3d5-1f954ff744fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'dict'>, {'x': tensor([       0,       14,       21,  ..., 10549705, 10549732, 10549760]), 'edge_index': tensor([       0,       26,       38,  ..., 22561532, 22561592, 22561658]), 'edge_attr': tensor([       0,       26,       38,  ..., 22561532, 22561592, 22561658]), 'text': tensor([     0,      1,      2,  ..., 298081, 298082, 298083]), 'smiles': tensor([     0,      1,      2,  ..., 298081, 298082, 298083]), 'cid': tensor([     0,      1,      2,  ..., 298081, 298082, 298083])})\n",
      "298083\n",
      "3-chloro-1,1,1-trifluoropropane appears as a colorless odorless nonflammable liquid. Poisonous by inhalation. Emits toxic fumes of chlorine and fluorine when heated to decomposition.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.data import InMemoryDataset\n",
    "\n",
    "\n",
    "class PubChemDataset(InMemoryDataset):\n",
    "    def __init__(self, path):\n",
    "        super(PubChemDataset, self).__init__()\n",
    "        self.data, self.slices = torch.load(path)\n",
    "        print(self.slices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.get(idx)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    dataset = PubChemDataset('./pretrain_data/PubChem324kV2/pretrain.pt')\n",
    "    print(len(dataset))\n",
    "    # print(dataset[0])\n",
    "    # print(dataset[0]['x'])\n",
    "    # print(dataset[0]['edge_index'])\n",
    "    # print(dataset[0]['edge_attr'])\n",
    "    # print(dataset[0]['smiles'])\n",
    "    print(dataset[1]['text'])\n",
    "    # print(dataset[0]['cid'])\n",
    "    # for i in range(len(dataset)):\n",
    "    #     print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46ac95d0-9962-4020-986b-c690e4c43bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed : 10\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "libcusparse.so.11: cannot open shared object file: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 65\u001b[0m\n\u001b[1;32m     60\u001b[0m conf \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;66;03m# best# \u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m##########################################\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m#########  construct dataloader  ######### \u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m##########################################\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtune_cls_pipeline\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MoleculeHERGDataset, MoleculeHERGTestDataset\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mKPGT\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeaturizer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Vocab, N_BOND_TYPES, N_ATOM_TYPES\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mKPGT\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcollator_tune\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Collator_pretrain, Collator_tune\n",
      "File \u001b[0;32m~/prompts_learning/tune_cls_pipeline.py:9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DistributedSampler\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdgl\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/fcb/lib/python3.8/site-packages/dgl/__init__.py:14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Backend and logging should be imported before other modules.\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogging\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m enable_verbose_logging  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend_name, load_backend  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     17\u001b[0m     container,\n\u001b[1;32m     18\u001b[0m     cuda,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m     storages,\n\u001b[1;32m     25\u001b[0m )\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_ffi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__, DGLError\n",
      "File \u001b[0;32m~/.conda/fcb/lib/python3.8/site-packages/dgl/backend/__init__.py:122\u001b[0m\n\u001b[1;32m    118\u001b[0m         set_default_backend(default_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpytorch\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    119\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpytorch\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 122\u001b[0m \u001b[43mload_backend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_preferred_backend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_enabled\u001b[39m(api):\n\u001b[1;32m    126\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return true if the api is enabled by the current backend.\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03m        True if the API is enabled by the current backend.\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/fcb/lib/python3.8/site-packages/dgl/backend/__init__.py:51\u001b[0m, in \u001b[0;36mload_backend\u001b[0;34m(mod_name)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported backend: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m mod_name)\n\u001b[0;32m---> 51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_ffi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_tensor_adapter  \u001b[38;5;66;03m# imports DGL C library\u001b[39;00m\n\u001b[1;32m     53\u001b[0m version \u001b[38;5;241m=\u001b[39m mod\u001b[38;5;241m.\u001b[39m__version__\n\u001b[1;32m     54\u001b[0m load_tensor_adapter(mod_name, version)\n",
      "File \u001b[0;32m~/.conda/fcb/lib/python3.8/site-packages/dgl/_ffi/base.py:50\u001b[0m\n\u001b[1;32m     48\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m libinfo\u001b[38;5;241m.\u001b[39m__version__\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# library instance of nnvm\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m _LIB, _LIB_NAME, _DIR_NAME \u001b[38;5;241m=\u001b[39m \u001b[43m_load_lib\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# The FFI mode of DGL\u001b[39;00m\n\u001b[1;32m     53\u001b[0m _FFI_MODE \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDGL_FFI\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/fcb/lib/python3.8/site-packages/dgl/_ffi/base.py:39\u001b[0m, in \u001b[0;36m_load_lib\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load libary by searching possible path.\"\"\"\u001b[39;00m\n\u001b[1;32m     38\u001b[0m lib_path \u001b[38;5;241m=\u001b[39m libinfo\u001b[38;5;241m.\u001b[39mfind_lib_path()\n\u001b[0;32m---> 39\u001b[0m lib \u001b[38;5;241m=\u001b[39m \u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCDLL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlib_path\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m dirname \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(lib_path[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     41\u001b[0m basename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(lib_path[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/.conda/fcb/lib/python3.8/ctypes/__init__.py:373\u001b[0m, in \u001b[0;36mCDLL.__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_FuncPtr \u001b[38;5;241m=\u001b[39m _FuncPtr\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 373\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m \u001b[43m_dlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m handle\n",
      "\u001b[0;31mOSError\u001b[0m: libcusparse.so.11: cannot open shared object file: No such file or directory"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import gc\n",
    "from sklearn import metrics\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "# torch.autograd.set_detect_anomaly(True)\n",
    "CUDA_LAUNCH_BLOCKING=1\n",
    "\n",
    "\n",
    "# fixed random seed for reproduction\n",
    "seed = 10\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.enabled = False\n",
    "print('Random seed :', seed)\n",
    "\n",
    "from collections import OrderedDict\n",
    "@torch.no_grad()\n",
    "def update_ema(ema_model, model, decay=0.9999):\n",
    "    \"\"\"\n",
    "    Step the EMA model towards the current model.\n",
    "    \"\"\"\n",
    "    ema_params = OrderedDict(ema_model.named_parameters())\n",
    "    model_params = OrderedDict(model.named_parameters())\n",
    "\n",
    "    for name, param in model_params.items():\n",
    "        # TODO: Consider applying only to params that require_grad to avoid small numerical changes of pos_embed\n",
    "        ema_params[name].mul_(decay).add_(param.data, alpha=1 - decay)\n",
    "\n",
    "def requires_grad(model, flag=True):\n",
    "    \"\"\"\n",
    "    Set requires_grad flag for all parameters in a model.\n",
    "    \"\"\"\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = flag\n",
    "\n",
    "from sklearn import metrics\n",
    "def compute_AUC(y, pred, n_class=1):\n",
    "    # compute one score\n",
    "    if n_class == 1:\n",
    "        auc = metrics.roc_auc_score(y, pred)\n",
    "\n",
    "    # compute two-class\n",
    "    elif n_class == 2:\n",
    "        # pos = pred[:, 1]\n",
    "        auc = metrics.roc_auc_score(y, pred)\n",
    "    return auc\n",
    "\n",
    "pad_len = 150 # best\n",
    "conf = 10 # best# \n",
    "\n",
    "##########################################\n",
    "#########  construct dataloader  ######### \n",
    "##########################################\n",
    "from tune_cls_pipeline import MoleculeHERGDataset, MoleculeHERGTestDataset\n",
    "from KPGT.src.data.featurizer import Vocab, N_BOND_TYPES, N_ATOM_TYPES\n",
    "from KPGT.src.data.collator_tune import Collator_pretrain, Collator_tune\n",
    "from KPGT.src.model_config import config_dict\n",
    "config = config_dict['base']\n",
    "\n",
    "\n",
    "test_dataset = MoleculeHERGTestDataset(\"week1\")\n",
    "# test_dataset = MoleculeHERGTestDataset(\"week2\")\n",
    "# test_dataset = MoleculeHERGTestDataset(\"week3\")\n",
    "# test_dataset = MoleculeHERGTestDataset(\"week4\")\n",
    "collator = Collator_tune(vocab, max_length=config['path_length'], n_virtual_nodes=2, candi_rate=config['candi_rate'], fp_disturb_rate=config['fp_disturb_rate'], md_disturb_rate=config['md_disturb_rate'])\n",
    "test_set = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=8, drop_last=False, collate_fn=collator)\n",
    "\n",
    "\n",
    "##########################################\n",
    "######  build model and optimizer  ####### \n",
    "##########################################\n",
    "dev = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "from CLIP import clip\n",
    "clip_model, preprocess = clip.load(name=\"ViT-B/16\", device=\"cpu\", download_root=\"/home/jovyan/clip_download_root\")\n",
    "\n",
    "from KPGT.src.model.light import LiGhTPredictor as LiGhT\n",
    "kpgt = LiGhT(\n",
    "        d_node_feats=config['d_node_feats'],\n",
    "        d_edge_feats=config['d_edge_feats'],\n",
    "        d_g_feats=config['d_g_feats'],\n",
    "        d_fp_feats=train_dataset.d_fps,\n",
    "        d_md_feats=train_dataset.d_mds,\n",
    "        d_hpath_ratio=config['d_hpath_ratio'],\n",
    "        n_mol_layers=config['n_mol_layers'],\n",
    "        path_length=config['path_length'],\n",
    "        n_heads=config['n_heads'],\n",
    "        n_ffn_dense_layers=config['n_ffn_dense_layers'],\n",
    "        input_drop=config['input_drop'],\n",
    "        attn_drop=config['attn_drop'],\n",
    "        feat_drop=config['feat_drop'],\n",
    "        # input_drop=0.,\n",
    "        # attn_drop=0.,\n",
    "        # feat_drop=0.,\n",
    "        n_node_types=vocab.vocab_size\n",
    "    )# .to(\"cuda\")\n",
    "    # model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[local_rank], output_device=local_rank, find_unused_parameters=True)\n",
    "# kpgt.load_state_dict({k.replace('module.', ''): v for k, v in torch.load(\"/home/jovyan/prompts_learning/KPGT/src/models/base.pth\").items()})\n",
    "# print(\"Pre-trained weights of KPGT were loaded successfully!\")\n",
    "\n",
    "from model_zoo import CLIPM\n",
    "model = CLIPM(kpgt, clip_model).to(dev)\n",
    "\n",
    "path = \"/home/jovyan/prompts_learning/trained_weight/cliPM_tune_herg_Epoch33_val_auc_0.87989.pth\"\n",
    "sd = torch.load(path)\n",
    "model.load_state_dict(sd)\n",
    "print(\"pre-trained weights loaded...\")\n",
    "\n",
    "                \n",
    "    \n",
    "##########################################\n",
    "####### start evaluating our model #######\n",
    "##########################################\n",
    "model.eval()\n",
    "print(\"evaluating...\")\n",
    "with torch.no_grad():\n",
    "    all_pred = None \n",
    "    all_lab = None\n",
    "    for step_id, datas in enumerate(test_set):\n",
    "            (_, batched_graph, fps, mds, _, _, _, label, logd, logp, pka, pkb, logsol, wlogsol) = batched_data\n",
    "            batched_graph = batched_graph.to(dev)\n",
    "            fps = fps.to(dev)\n",
    "            mds = mds.to(dev)\n",
    "            \n",
    "            pred = model([batched_graph, fps, mds])\n",
    "            # pred = ema([batched_graph, fps, mds])\n",
    "\n",
    "            pred = torch.sigmoid(pred)\n",
    "\n",
    "            all_pred = pred if all_pred is None else torch.cat([all_pred, pred], dim=0)\n",
    "            all_lab = label if all_lab is None else torch.cat([all_lab, label], dim=0)\n",
    "auc = compute_AUC(all_lab.cpu().detach(), all_pred.cpu().detach())\n",
    "print(f\"test AUC: {auc:.5f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f9dc6d-bb33-46e0-b34f-81f67a35f9a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fcb",
   "language": "python",
   "name": "fcb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
