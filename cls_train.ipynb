{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bac61f26-ff92-477b-96a0-836186307af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed : 10\n",
      "number of postive/negtive samples 4599/2042.\n",
      "{'Br': 0, 'C': 1, 'H': 2, 'N': 3, 'O': 4, 'F': 5, 'Cl': 6, 'S': 7, 'P': 8, 'I': 9, 'B': 10, 'Se': 11, 'Ar': 12, 'Kr': 13, 'Li': 14, 'Ne': 15, 'Xe': 16, 'Si': 17, 'Na': 18}\n",
      "train set is initialized successfully. The max length of the atom is 276. The number of dataset is 6641. Padding length is 150.\n",
      "number of postive/negtive samples 1120/540.\n",
      "{'Br': 0, 'C': 1, 'H': 2, 'N': 3, 'O': 4, 'F': 5, 'Cl': 6, 'S': 7, 'P': 8, 'I': 9, 'B': 10, 'Se': 11, 'Ar': 12, 'Kr': 13, 'Li': 14, 'Ne': 15, 'Xe': 16, 'Si': 17, 'Na': 18}\n",
      "test set is initialized successfully. The max length of the atom is 196. The number of dataset is 1660. Padding length is 150.\n",
      "Layer number of backbone is 12.\n",
      "The initial prompts are: ['The protein has the blood-brain barrier.', 'The protein has the X X X X X']\n",
      "Set of Optimizer: lr:0.0002, weight_decay:0.0\n",
      "Current Optimizer is adan\n",
      "Current learning rate is [6.666666666666667e-05, 6.666666666666667e-05, 6.666666666666667e-05, 6.666666666666667e-05, 6.666666666666667e-05, 6.666666666666667e-05].\n",
      "Let's start training!\n",
      "epoch: 1 / 1000,step 39 / 415, loss: 0.3131\n",
      "epoch: 1 / 1000,step 79 / 415, loss: 0.2280\n",
      "epoch: 1 / 1000,step 119 / 415, loss: 0.1449\n",
      "epoch: 1 / 1000,step 159 / 415, loss: 0.2189\n",
      "epoch: 1 / 1000,step 199 / 415, loss: 0.1822\n",
      "epoch: 1 / 1000,step 239 / 415, loss: 0.3128\n",
      "epoch: 1 / 1000,step 279 / 415, loss: 0.1438\n",
      "epoch: 1 / 1000,step 319 / 415, loss: 0.2320\n",
      "epoch: 1 / 1000,step 359 / 415, loss: 0.2635\n",
      "epoch: 1 / 1000,step 399 / 415, loss: 0.1156\n",
      "evaluating...\n",
      "epoch: 1 / 1000, test AUC: 0.83397\n",
      "epoch: 1 end ; cost time: 1.8864 min\n",
      "Current learning rate is [9.333333333333334e-05, 9.333333333333334e-05, 9.333333333333334e-05, 9.333333333333334e-05, 9.333333333333334e-05, 9.333333333333334e-05].\n",
      "epoch: 2 / 1000,step 39 / 415, loss: 0.3263\n",
      "epoch: 2 / 1000,step 79 / 415, loss: 0.2094\n",
      "epoch: 2 / 1000,step 119 / 415, loss: 0.1920\n",
      "epoch: 2 / 1000,step 159 / 415, loss: 0.2605\n",
      "epoch: 2 / 1000,step 199 / 415, loss: 0.2155\n",
      "epoch: 2 / 1000,step 239 / 415, loss: 0.2029\n",
      "epoch: 2 / 1000,step 279 / 415, loss: 0.1914\n",
      "epoch: 2 / 1000,step 319 / 415, loss: 0.1267\n",
      "epoch: 2 / 1000,step 359 / 415, loss: 0.2048\n",
      "epoch: 2 / 1000,step 399 / 415, loss: 0.1979\n",
      "evaluating...\n",
      "epoch: 2 / 1000, test AUC: 0.84953\n",
      "epoch: 2 end ; cost time: 1.8852 min\n",
      "Current learning rate is [0.00012000000000000002, 0.00012000000000000002, 0.00012000000000000002, 0.00012000000000000002, 0.00012000000000000002, 0.00012000000000000002].\n",
      "epoch: 3 / 1000,step 39 / 415, loss: 0.1463\n",
      "epoch: 3 / 1000,step 79 / 415, loss: 0.1698\n",
      "epoch: 3 / 1000,step 119 / 415, loss: 0.2316\n",
      "epoch: 3 / 1000,step 159 / 415, loss: 0.1138\n",
      "epoch: 3 / 1000,step 199 / 415, loss: 0.1949\n",
      "epoch: 3 / 1000,step 239 / 415, loss: 0.1535\n",
      "epoch: 3 / 1000,step 279 / 415, loss: 0.1036\n",
      "epoch: 3 / 1000,step 319 / 415, loss: 0.2288\n",
      "epoch: 3 / 1000,step 359 / 415, loss: 0.2217\n",
      "epoch: 3 / 1000,step 399 / 415, loss: 0.1131\n",
      "evaluating...\n",
      "epoch: 3 / 1000, test AUC: 0.85729\n",
      "epoch: 3 end ; cost time: 1.8848 min\n",
      "Current learning rate is [0.0001466666666666667, 0.0001466666666666667, 0.0001466666666666667, 0.0001466666666666667, 0.0001466666666666667, 0.0001466666666666667].\n",
      "epoch: 4 / 1000,step 39 / 415, loss: 0.2321\n",
      "epoch: 4 / 1000,step 79 / 415, loss: 0.2090\n",
      "epoch: 4 / 1000,step 119 / 415, loss: 0.0937\n",
      "epoch: 4 / 1000,step 159 / 415, loss: 0.1541\n",
      "epoch: 4 / 1000,step 199 / 415, loss: 0.2248\n",
      "epoch: 4 / 1000,step 239 / 415, loss: 0.0686\n",
      "epoch: 4 / 1000,step 279 / 415, loss: 0.1566\n",
      "epoch: 4 / 1000,step 319 / 415, loss: 0.4584\n",
      "epoch: 4 / 1000,step 359 / 415, loss: 0.1155\n",
      "epoch: 4 / 1000,step 399 / 415, loss: 0.2502\n",
      "evaluating...\n",
      "epoch: 4 / 1000, test AUC: 0.86765\n",
      "epoch: 4 end ; cost time: 1.8867 min\n",
      "Current learning rate is [0.00017333333333333336, 0.00017333333333333336, 0.00017333333333333336, 0.00017333333333333336, 0.00017333333333333336, 0.00017333333333333336].\n",
      "epoch: 5 / 1000,step 39 / 415, loss: 0.1718\n",
      "epoch: 5 / 1000,step 79 / 415, loss: 0.2799\n",
      "epoch: 5 / 1000,step 119 / 415, loss: 0.2261\n",
      "epoch: 5 / 1000,step 159 / 415, loss: 0.2079\n",
      "epoch: 5 / 1000,step 199 / 415, loss: 0.1712\n",
      "epoch: 5 / 1000,step 239 / 415, loss: 0.2788\n",
      "epoch: 5 / 1000,step 279 / 415, loss: 0.3000\n",
      "epoch: 5 / 1000,step 319 / 415, loss: 0.2425\n",
      "epoch: 5 / 1000,step 359 / 415, loss: 0.1516\n",
      "epoch: 5 / 1000,step 399 / 415, loss: 0.2159\n",
      "evaluating...\n",
      "epoch: 5 / 1000, test AUC: 0.87616\n",
      "epoch: 5 end ; cost time: 1.8856 min\n",
      "Current learning rate is [0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.conda/fcb/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 / 1000,step 39 / 415, loss: 0.2212\n",
      "epoch: 6 / 1000,step 79 / 415, loss: 0.0990\n",
      "epoch: 6 / 1000,step 119 / 415, loss: 0.1664\n",
      "epoch: 6 / 1000,step 159 / 415, loss: 0.2543\n",
      "epoch: 6 / 1000,step 199 / 415, loss: 0.2065\n",
      "epoch: 6 / 1000,step 239 / 415, loss: 0.2340\n",
      "epoch: 6 / 1000,step 279 / 415, loss: 0.1506\n",
      "epoch: 6 / 1000,step 319 / 415, loss: 0.2507\n",
      "epoch: 6 / 1000,step 359 / 415, loss: 0.1402\n",
      "epoch: 6 / 1000,step 399 / 415, loss: 0.2645\n",
      "evaluating...\n",
      "epoch: 6 / 1000, test AUC: 0.86664\n",
      "epoch: 6 end ; cost time: 1.8863 min\n",
      "Current learning rate is [0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002].\n",
      "epoch: 7 / 1000,step 39 / 415, loss: 0.1211\n",
      "epoch: 7 / 1000,step 79 / 415, loss: 0.1180\n",
      "epoch: 7 / 1000,step 119 / 415, loss: 0.2402\n",
      "epoch: 7 / 1000,step 159 / 415, loss: 0.1206\n",
      "epoch: 7 / 1000,step 199 / 415, loss: 0.1765\n",
      "epoch: 7 / 1000,step 239 / 415, loss: 0.1627\n",
      "epoch: 7 / 1000,step 279 / 415, loss: 0.1799\n",
      "epoch: 7 / 1000,step 319 / 415, loss: 0.1218\n",
      "epoch: 7 / 1000,step 359 / 415, loss: 0.2182\n",
      "epoch: 7 / 1000,step 399 / 415, loss: 0.1180\n",
      "evaluating...\n",
      "epoch: 7 / 1000, test AUC: 0.86348\n",
      "epoch: 7 end ; cost time: 1.8889 min\n",
      "Current learning rate is [0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002].\n",
      "epoch: 8 / 1000,step 39 / 415, loss: 0.2118\n",
      "epoch: 8 / 1000,step 79 / 415, loss: 0.1366\n",
      "epoch: 8 / 1000,step 119 / 415, loss: 0.1979\n",
      "epoch: 8 / 1000,step 159 / 415, loss: 0.2612\n",
      "epoch: 8 / 1000,step 199 / 415, loss: 0.1780\n",
      "epoch: 8 / 1000,step 239 / 415, loss: 0.1723\n",
      "epoch: 8 / 1000,step 279 / 415, loss: 0.1770\n",
      "epoch: 8 / 1000,step 319 / 415, loss: 0.1350\n",
      "epoch: 8 / 1000,step 359 / 415, loss: 0.1956\n",
      "epoch: 8 / 1000,step 399 / 415, loss: 0.2318\n",
      "evaluating...\n",
      "epoch: 8 / 1000, test AUC: 0.86407\n",
      "epoch: 8 end ; cost time: 1.8874 min\n",
      "Current learning rate is [0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import gc\n",
    "from sklearn import metrics\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "# torch.autograd.set_detect_anomaly(True)\n",
    "CUDA_LAUNCH_BLOCKING=1\n",
    "\n",
    "\n",
    "# fixed random seed for reproduction\n",
    "seed = 10\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.enabled = False\n",
    "print('Random seed :', seed)\n",
    "\n",
    "from collections import OrderedDict\n",
    "@torch.no_grad()\n",
    "def update_ema(ema_model, model, decay=0.9999):\n",
    "    \"\"\"\n",
    "    Step the EMA model towards the current model.\n",
    "    \"\"\"\n",
    "    ema_params = OrderedDict(ema_model.named_parameters())\n",
    "    model_params = OrderedDict(model.named_parameters())\n",
    "\n",
    "    for name, param in model_params.items():\n",
    "        # TODO: Consider applying only to params that require_grad to avoid small numerical changes of pos_embed\n",
    "        ema_params[name].mul_(decay).add_(param.data, alpha=1 - decay)\n",
    "\n",
    "def requires_grad(model, flag=True):\n",
    "    \"\"\"\n",
    "    Set requires_grad flag for all parameters in a model.\n",
    "    \"\"\"\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = flag\n",
    "\n",
    "from sklearn import metrics\n",
    "def compute_AUC(y, pred, n_class=1):\n",
    "    # compute one score\n",
    "    if n_class == 1:\n",
    "        auc = metrics.roc_auc_score(y, pred)\n",
    "\n",
    "    # compute two-class\n",
    "    elif n_class == 2:\n",
    "        # pos = pred[:, 1]\n",
    "        auc = metrics.roc_auc_score(y, pred)\n",
    "    return auc\n",
    "\n",
    "pad_len = 150 # best\n",
    "conf = 10 # best# \n",
    "\n",
    "##########################################\n",
    "#########  construct dataloader  ######### \n",
    "##########################################\n",
    "# from lmdb_datapipeline import LMDBDataset  \n",
    "from cls_lmdb_datapipeline import LMDBDataset \n",
    "# lmdb_file = './results/logd_train.lmdb'\n",
    "lmdb_file = './results/bbb_train.lmdb'\n",
    "train_dataset = LMDBDataset(lmdb_file, conf=conf, pad_len=pad_len, mode=\"train\")\n",
    "train_set = DataLoader(train_dataset,\n",
    "                    batch_size=16,  # 16 best\n",
    "                    drop_last=True,\n",
    "                    shuffle=True,\n",
    "                    pin_memory=True,\n",
    "                    num_workers=4,\n",
    "                    # collate_fn=train_dataset.collate_fn,\n",
    "                    worker_init_fn=train_dataset.worker_init_fn\n",
    "                    )\n",
    "\n",
    "# lmdb_file = './results/logd_test.lmdb'\n",
    "lmdb_file = './results/bbb_test.lmdb'\n",
    "val_dataset = LMDBDataset(lmdb_file, conf=conf, pad_len=pad_len, mode=\"test\")\n",
    "val_set = DataLoader(val_dataset,\n",
    "                    batch_size=32,\n",
    "                    drop_last=False,\n",
    "                    shuffle=False,\n",
    "                    pin_memory=True,\n",
    "                    num_workers=4,\n",
    "                    # collate_fn=train_dataset.collate_fn,\n",
    "                    worker_init_fn=val_dataset.worker_init_fn\n",
    "                    )\n",
    "\n",
    "\n",
    "##########################################\n",
    "######  build model and optimizer  ####### \n",
    "##########################################\n",
    "dev = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "from CLIP import clip\n",
    "clip_model, preprocess = clip.load(name=\"ViT-B/16\", device=\"cpu\", download_root=\"/home/jovyan/clip_download_root\")\n",
    "from model_zoo import CLIP_Protein\n",
    "model = CLIP_Protein(clip_model, conf, pad_len=pad_len).to(dev)\n",
    "\n",
    "# path = \"/home/jovyan/prompts_learning/trained_weight/A_10_7_cliP_Epoch13_val_auc_0.90789.pth\"\n",
    "# sd = torch.load(path)\n",
    "# model.load_state_dict(sd)\n",
    "# print(\"pre-trained weights loaded...\")\n",
    "\n",
    "from copy import deepcopy\n",
    "ema = deepcopy(model).to(dev)  # Create an EMA of the model for use after training\n",
    "update_ema(ema, model, decay=0)  # Ensure EMA is initialized with synced weights\n",
    "requires_grad(ema, False)\n",
    "ema.eval()\n",
    "\n",
    "# best\n",
    "lr = 5e-5\n",
    "wd = 0.\n",
    "\n",
    "# lr = 2e-4\n",
    "# wd = 0.\n",
    "\n",
    "print(f'Set of Optimizer: lr:{lr}, weight_decay:{wd}')\n",
    "model_params = [# {'params': model.parameters(), 'lr': lr},\n",
    "                {'params': model.atom_encoder.parameters(), 'lr': lr},\n",
    "                {'params': model.coor_encoder.parameters(), 'lr': lr},\n",
    "                {'params': model.fusion_blocks.parameters(), 'lr': lr},\n",
    "                {'params': model.head.parameters(), 'lr': 1e-3}, # best\n",
    "                # {'params': model.head.parameters(), 'lr': lr},\n",
    "                {'params': model.prompts_processor.parameters(), 'lr': lr},\n",
    "                {'params': model.ppim.parameters(), 'lr': lr},\n",
    "                # {'params': model.qformer.parameters(), 'lr':1e-5}\n",
    "               ]\n",
    "\n",
    "optims = 'adan'\n",
    "# optims = \"sgd\"\n",
    "if optims == 'adan':\n",
    "    from adan import Adan\n",
    "    optimizer = Adan(model_params, betas=(0.98, 0.92, 0.99),weight_decay=wd, max_grad_norm=0.)\n",
    "elif optims == 'sgd':\n",
    "    optimizer = optim.SGD(model_params, momentum=0.9, weight_decay=wd)\n",
    "elif optims == 'adamw':\n",
    "    optimizer = optim.AdamW(model_params, betas=(0.9, 0.999), weight_decay=wd)\n",
    "elif optims == 'adam':\n",
    "    optimizer = optim.Adam(model_params, betas=(0.9, 0.999), weight_decay=wd)\n",
    "print('Current Optimizer is', optims)\n",
    "\n",
    "\n",
    "###################################################\n",
    "########   build learning rate scheduler   ######## \n",
    "###################################################\n",
    "# best\n",
    "scheduler1 = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=1./3., total_iters=5) \n",
    "scheduler2 = torch.optim.lr_scheduler.ConstantLR(optimizer, factor=1., total_iters=5)  \n",
    "scheduler3 = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=15, eta_min=1e-6)\n",
    "scheduler = torch.optim.lr_scheduler.SequentialLR(optimizer, schedulers=[scheduler1, scheduler2, scheduler3], milestones=[5, 10]) \n",
    "\n",
    "# scheduler1 = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=1./3., total_iters=5)\n",
    "# scheduler2 = torch.optim.lr_scheduler.ConstantLR(optimizer, factor=1., total_iters=10)\n",
    "# scheduler3 = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=15, eta_min=1e-6)\n",
    "# scheduler = torch.optim.lr_scheduler.SequentialLR(optimizer, schedulers=[scheduler1, scheduler2, scheduler3], milestones=[5, 15]) \n",
    "cur_lr = scheduler.get_last_lr() \n",
    "print(f\"Current learning rate is {cur_lr}.\")\n",
    "\n",
    "\n",
    "##########################################\n",
    "########   build loss criterion   ######## \n",
    "##########################################\n",
    "cri_mse = nn.MSELoss()\n",
    "cri_mae = nn.L1Loss()\n",
    "weight = torch.tensor([6641./2042., 6641./4599.], dtype=torch.float32, device=dev)\n",
    "cri_ce = nn.CrossEntropyLoss(weight)\n",
    "\n",
    "\n",
    "##########################################\n",
    "######## start training our model ######## \n",
    "##########################################\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "Epoch = 1000\n",
    "best_val = 0.91\n",
    "print(\"Let's start training!\")\n",
    "\n",
    "for e in range(0, Epoch):\n",
    "    start = time.time()\n",
    "    model.train()\n",
    "    for step_id, datas in enumerate(train_set):\n",
    "        # break\n",
    "        # print(datas[\"atoms\"].shape)\n",
    "        # print(datas[\"coordinate\"].shape)\n",
    "        # print(datas[\"label\"])\n",
    "        atoms = datas[\"atoms\"].to(dev, non_blocking=True).long()\n",
    "        # coord = datas[\"coordinate\"].to(dev, non_blocking=True).float()\n",
    "        pair = datas[\"distance\"].to(dev, non_blocking=True).float()\n",
    "        spd = datas[\"SPD\"].to(dev, non_blocking=True).float()\n",
    "        edge = datas[\"edge\"].to(dev, non_blocking=True).float()\n",
    "        label = datas[\"label\"].to(dev, non_blocking=True).float()\n",
    "        # label = torch.tanh(label)\n",
    "\n",
    "        pred = model(atoms, pair, spd, edge)\n",
    "        # pred = torch.tanh(pred)\n",
    "\n",
    "        # loss = cri_mae(pred, label.unsqueeze(-1))\n",
    "\n",
    "        # weighted BCE loss\n",
    "        # loss = F.binary_cross_entropy_with_logits(pred, label.unsqueeze(-1))\n",
    "        loss = F.binary_cross_entropy_with_logits(pred, label.unsqueeze(-1), reduction='none')\n",
    "        for i, lab in enumerate(label):\n",
    "            if lab == 0:\n",
    "                # loss[i] = loss[i] * (6641./2042.)\n",
    "                loss[i] = loss[i] * (4599./6641)  # best\n",
    "            else:\n",
    "                # loss[i] = loss[i] * (6641./4599.)\n",
    "                loss[i] = loss[i] * (2042./6641)\n",
    "        loss = loss.mean()\n",
    "\n",
    "        # CE loss\n",
    "        # loss = cri_ce(pred, label.long())\n",
    "\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        update_ema(ema, model, 0.997) # 0.997 best\n",
    "\n",
    "        if not (step_id+1) % 40:\n",
    "            print(f\"epoch: {e+1} / {Epoch},step {step_id} / {len(train_set)}, loss: {loss.detach().cpu().numpy():.4f}\")\n",
    "                \n",
    "    \n",
    "    ##########################################\n",
    "    ####### start evaluating our model #######\n",
    "    ##########################################\n",
    "    model.eval()\n",
    "    print(\"evaluating...\")\n",
    "    with torch.no_grad():\n",
    "        all_pred = None \n",
    "        all_lab = None\n",
    "        for step_id, datas in enumerate(val_set):\n",
    "                atoms = datas[\"atoms\"].to(dev, non_blocking=True).long()\n",
    "                # coord = datas[\"coordinate\"].to(dev, non_blocking=True).float()\n",
    "                pair = datas[\"distance\"].to(dev, non_blocking=True).float()\n",
    "                spd = datas[\"SPD\"].to(dev, non_blocking=True).float()\n",
    "                edge = datas[\"edge\"].to(dev, non_blocking=True).float()\n",
    "                label = datas[\"label\"].to(dev, non_blocking=True).float()\n",
    "                # label = torch.tanh(label)\n",
    "                # pred = model(atoms, pair, spd, edge)\n",
    "                pred = ema(atoms, pair, spd, edge)\n",
    "                \n",
    "                pred = torch.sigmoid(pred)\n",
    "                # pred = torch.softmax(pred, dim=-1)[:, 1]\n",
    "                # total_loss += cri_mae(pred, label.unsqueeze(-1))\n",
    "                all_pred = pred if all_pred is None else torch.cat([all_pred, pred], dim=0)\n",
    "                all_lab = label if all_lab is None else torch.cat([all_lab, label], dim=0)\n",
    "    auc = compute_AUC(all_lab.cpu().detach(), all_pred.cpu().detach())\n",
    "    print(f\"epoch: {e+1} / {Epoch}, test AUC: {auc:.5f}\")\n",
    "    if auc > best_val:\n",
    "        best_val = auc\n",
    "        # torch.save(model.state_dict(), f'./trained_weight/cliP_Epoch{e+1}_val_auc_{best_val:.5f}.pth') \n",
    "        torch.save(ema.state_dict(), f'./trained_weight/cliP_Epoch{e+1}_val_auc_{best_val:.5f}.pth') \n",
    "\n",
    "\n",
    "    end = time.time()\n",
    "    print(f\"epoch: {e+1} end ; cost time: {(end - start)/60.:.4f} min\")\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    scheduler.step()\n",
    "    cur_lr = scheduler.get_last_lr() \n",
    "    print(f\"Current learning rate is {cur_lr}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a929c24-3573-4523-8f87-e303fb3b3f79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fcb",
   "language": "python",
   "name": "fcb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
