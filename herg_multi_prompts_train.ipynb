{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36c35dd4-5eb9-44fa-865c-3c148d085a17",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed : 10\n",
      "number of positive/negative samples 2809/2878.\n",
      "{'Br': 0, 'C': 1, 'H': 2, 'N': 3, 'O': 4, 'F': 5, 'Cl': 6, 'S': 7, 'P': 8, 'I': 9, 'B': 10, 'Se': 11, 'Ar': 12, 'Kr': 13, 'Li': 14, 'Ne': 15, 'Xe': 16, 'Si': 17, 'Na': 18, 'Mask': 19, 'Pad': 20}\n",
      "train set is initialized successfully. The max length of the atom is 135. The number of dataset is 5687. Padding length is 150.\n",
      "number of positive/negative samples 696/726.\n",
      "{'Br': 0, 'C': 1, 'H': 2, 'N': 3, 'O': 4, 'F': 5, 'Cl': 6, 'S': 7, 'P': 8, 'I': 9, 'B': 10, 'Se': 11, 'Ar': 12, 'Kr': 13, 'Li': 14, 'Ne': 15, 'Xe': 16, 'Si': 17, 'Na': 18, 'Mask': 19, 'Pad': 20}\n",
      "val set is initialized successfully. The max length of the atom is 134. The number of dataset is 1422. Padding length is 150.\n",
      "Layer number of backbone is 12.\n",
      "The initial prompts are: ['The molecule has the X X X X X', 'The LogD of the molecule is A A A A', 'The LogP of the molecule is B B B B', 'The pKa of the molecule is C C C C', 'The pKb of the molecule is D D D D', 'The LogSol of the molecule is E E E E', 'The wLogSol of the molecule is F F F F']\n",
      "Set of Optimizer: lr:5e-05, weight_decay:0.0\n",
      "Current Optimizer is adan\n",
      "Current learning rate is [1.6666666666666667e-05, 1.6666666666666667e-05, 1.6666666666666667e-05, 0.0003333333333333333, 1.6666666666666667e-05, 1.6666666666666667e-05, 1.6666666666666667e-05, 1.6666666666666667e-05, 1.6666666666666667e-05].\n",
      "attribution loss is bce, and reduction method is mean.\n",
      "Let's start training!\n",
      "epoch: 1 / 1000,step 39 / 355, loss: 3.4571\n",
      "HERG_cls: 0.3449; LogD: 0.6967; LogP: 0.6925; pKa: 0.4574; pKb: 0.1463; LogSol: 0.5571; wLogSol: 0.5622; \n",
      "epoch: 1 / 1000,step 79 / 355, loss: 3.9266\n",
      "HERG_cls: 0.3453; LogD: 0.6918; LogP: 0.6716; pKa: 0.7211; pKb: 0.3492; LogSol: 0.5729; wLogSol: 0.5747; \n",
      "epoch: 1 / 1000,step 119 / 355, loss: 3.6948\n",
      "HERG_cls: 0.3412; LogD: 0.6804; LogP: 0.6717; pKa: 0.4721; pKb: 0.3484; LogSol: 0.5865; wLogSol: 0.5945; \n",
      "epoch: 1 / 1000,step 159 / 355, loss: 3.3288\n",
      "HERG_cls: 0.3200; LogD: 0.6819; LogP: 0.6684; pKa: 0.3329; pKb: 0.2873; LogSol: 0.5190; wLogSol: 0.5193; \n",
      "epoch: 1 / 1000,step 199 / 355, loss: 3.2211\n",
      "HERG_cls: 0.3109; LogD: 0.6893; LogP: 0.6694; pKa: 0.2737; pKb: 0.2176; LogSol: 0.5335; wLogSol: 0.5267; \n",
      "epoch: 1 / 1000,step 239 / 355, loss: 3.4213\n",
      "HERG_cls: 0.3756; LogD: 0.6945; LogP: 0.6795; pKa: 0.3052; pKb: 0.2865; LogSol: 0.5374; wLogSol: 0.5425; \n",
      "epoch: 1 / 1000,step 279 / 355, loss: 3.2140\n",
      "HERG_cls: 0.3640; LogD: 0.6834; LogP: 0.6729; pKa: 0.3681; pKb: 0.1194; LogSol: 0.5011; wLogSol: 0.5050; \n",
      "epoch: 1 / 1000,step 319 / 355, loss: 3.3640\n",
      "HERG_cls: 0.3570; LogD: 0.6751; LogP: 0.6491; pKa: 0.1508; pKb: 0.3220; LogSol: 0.6015; wLogSol: 0.6085; \n",
      "evaluating...\n",
      "epoch: 1 / 1000, test AUC: 0.71341\n",
      "epoch: 1 end ; cost time: 1.6546 min\n",
      "Current learning rate is [2.3333333333333336e-05, 2.3333333333333336e-05, 2.3333333333333336e-05, 0.0004666666666666667, 2.3333333333333336e-05, 2.3333333333333336e-05, 2.3333333333333336e-05, 2.3333333333333336e-05, 2.3333333333333336e-05].\n",
      "epoch: 2 / 1000,step 39 / 355, loss: 3.3749\n",
      "HERG_cls: 0.3341; LogD: 0.6620; LogP: 0.6696; pKa: 0.3192; pKb: 0.2830; LogSol: 0.5550; wLogSol: 0.5520; \n",
      "epoch: 2 / 1000,step 79 / 355, loss: 2.7442\n",
      "HERG_cls: 0.1830; LogD: 0.6503; LogP: 0.6423; pKa: 0.1620; pKb: 0.1094; LogSol: 0.4970; wLogSol: 0.5002; \n",
      "epoch: 2 / 1000,step 119 / 355, loss: 2.9802\n",
      "HERG_cls: 0.3546; LogD: 0.6650; LogP: 0.6448; pKa: 0.2093; pKb: 0.0905; LogSol: 0.5070; wLogSol: 0.5091; \n",
      "epoch: 2 / 1000,step 159 / 355, loss: 2.9579\n",
      "HERG_cls: 0.4259; LogD: 0.6599; LogP: 0.6510; pKa: 0.1386; pKb: 0.1081; LogSol: 0.4877; wLogSol: 0.4868; \n",
      "epoch: 2 / 1000,step 199 / 355, loss: 2.9147\n",
      "HERG_cls: 0.2508; LogD: 0.6777; LogP: 0.6810; pKa: 0.2940; pKb: 0.1353; LogSol: 0.4386; wLogSol: 0.4373; \n",
      "epoch: 2 / 1000,step 239 / 355, loss: 2.9496\n",
      "HERG_cls: 0.2559; LogD: 0.6686; LogP: 0.6473; pKa: 0.2050; pKb: 0.1579; LogSol: 0.5063; wLogSol: 0.5087; \n",
      "epoch: 2 / 1000,step 279 / 355, loss: 3.3306\n",
      "HERG_cls: 0.3063; LogD: 0.6728; LogP: 0.6939; pKa: 0.2739; pKb: 0.2815; LogSol: 0.5500; wLogSol: 0.5521; \n",
      "epoch: 2 / 1000,step 319 / 355, loss: 3.1975\n",
      "HERG_cls: 0.5742; LogD: 0.6583; LogP: 0.6642; pKa: 0.1660; pKb: 0.0337; LogSol: 0.5551; wLogSol: 0.5460; \n",
      "evaluating...\n",
      "epoch: 2 / 1000, test AUC: 0.78445\n",
      "epoch: 2 end ; cost time: 1.6430 min\n",
      "Current learning rate is [3.0000000000000004e-05, 3.0000000000000004e-05, 3.0000000000000004e-05, 0.0006000000000000001, 3.0000000000000004e-05, 3.0000000000000004e-05, 3.0000000000000004e-05, 3.0000000000000004e-05, 3.0000000000000004e-05].\n",
      "epoch: 3 / 1000,step 39 / 355, loss: 2.8464\n",
      "HERG_cls: 0.1517; LogD: 0.6636; LogP: 0.6740; pKa: 0.1373; pKb: 0.1500; LogSol: 0.5344; wLogSol: 0.5354; \n",
      "epoch: 3 / 1000,step 79 / 355, loss: 2.6163\n",
      "HERG_cls: 0.2448; LogD: 0.6807; LogP: 0.6670; pKa: 0.0541; pKb: 0.0243; LogSol: 0.4714; wLogSol: 0.4741; \n",
      "epoch: 3 / 1000,step 119 / 355, loss: 2.8365\n",
      "HERG_cls: 0.1994; LogD: 0.6388; LogP: 0.6796; pKa: 0.1426; pKb: 0.1212; LogSol: 0.5269; wLogSol: 0.5280; \n",
      "epoch: 3 / 1000,step 159 / 355, loss: 2.8945\n",
      "HERG_cls: 0.3201; LogD: 0.6639; LogP: 0.6377; pKa: 0.1224; pKb: 0.1001; LogSol: 0.5273; wLogSol: 0.5230; \n",
      "epoch: 3 / 1000,step 199 / 355, loss: 2.9457\n",
      "HERG_cls: 0.2251; LogD: 0.6675; LogP: 0.6673; pKa: 0.1494; pKb: 0.1837; LogSol: 0.5229; wLogSol: 0.5298; \n",
      "epoch: 3 / 1000,step 239 / 355, loss: 2.7875\n",
      "HERG_cls: 0.2718; LogD: 0.6224; LogP: 0.6419; pKa: 0.2192; pKb: 0.0724; LogSol: 0.4786; wLogSol: 0.4812; \n",
      "epoch: 3 / 1000,step 279 / 355, loss: 3.0298\n",
      "HERG_cls: 0.2518; LogD: 0.6654; LogP: 0.6708; pKa: 0.2015; pKb: 0.1606; LogSol: 0.5419; wLogSol: 0.5377; \n",
      "epoch: 3 / 1000,step 319 / 355, loss: 2.8943\n",
      "HERG_cls: 0.3318; LogD: 0.6705; LogP: 0.6848; pKa: 0.1284; pKb: 0.0779; LogSol: 0.4985; wLogSol: 0.5023; \n",
      "evaluating...\n",
      "epoch: 3 / 1000, test AUC: 0.82250\n",
      "epoch: 3 end ; cost time: 1.6440 min\n",
      "Current learning rate is [3.666666666666667e-05, 3.666666666666667e-05, 3.666666666666667e-05, 0.0007333333333333334, 3.666666666666667e-05, 3.666666666666667e-05, 3.666666666666667e-05, 3.666666666666667e-05, 3.666666666666667e-05].\n",
      "epoch: 4 / 1000,step 39 / 355, loss: 2.8667\n",
      "HERG_cls: 0.2798; LogD: 0.6396; LogP: 0.6485; pKa: 0.1897; pKb: 0.1322; LogSol: 0.4884; wLogSol: 0.4885; \n",
      "epoch: 4 / 1000,step 79 / 355, loss: 2.7183\n",
      "HERG_cls: 0.2143; LogD: 0.6551; LogP: 0.6718; pKa: 0.0693; pKb: 0.1113; LogSol: 0.4990; wLogSol: 0.4975; \n",
      "epoch: 4 / 1000,step 119 / 355, loss: 2.7721\n",
      "HERG_cls: 0.2643; LogD: 0.6616; LogP: 0.6653; pKa: 0.0859; pKb: 0.0890; LogSol: 0.5029; wLogSol: 0.5032; \n",
      "epoch: 4 / 1000,step 159 / 355, loss: 2.8029\n",
      "HERG_cls: 0.1976; LogD: 0.6812; LogP: 0.6715; pKa: 0.0664; pKb: 0.1058; LogSol: 0.5397; wLogSol: 0.5407; \n",
      "epoch: 4 / 1000,step 199 / 355, loss: 2.7318\n",
      "HERG_cls: 0.0837; LogD: 0.6745; LogP: 0.6779; pKa: 0.2130; pKb: 0.0916; LogSol: 0.4956; wLogSol: 0.4954; \n",
      "epoch: 4 / 1000,step 239 / 355, loss: 2.7092\n",
      "HERG_cls: 0.2014; LogD: 0.6574; LogP: 0.6770; pKa: 0.0656; pKb: 0.0313; LogSol: 0.5384; wLogSol: 0.5380; \n",
      "epoch: 4 / 1000,step 279 / 355, loss: 2.7270\n",
      "HERG_cls: 0.1851; LogD: 0.6442; LogP: 0.6813; pKa: 0.1409; pKb: 0.0246; LogSol: 0.5272; wLogSol: 0.5236; \n",
      "epoch: 4 / 1000,step 319 / 355, loss: 2.5996\n",
      "HERG_cls: 0.1204; LogD: 0.6502; LogP: 0.6579; pKa: 0.0680; pKb: 0.0424; LogSol: 0.5317; wLogSol: 0.5289; \n",
      "evaluating...\n",
      "epoch: 4 / 1000, test AUC: 0.84126\n",
      "epoch: 4 end ; cost time: 1.6422 min\n",
      "Current learning rate is [4.333333333333334e-05, 4.333333333333334e-05, 4.333333333333334e-05, 0.0008666666666666668, 4.333333333333334e-05, 4.333333333333334e-05, 4.333333333333334e-05, 4.333333333333334e-05, 4.333333333333334e-05].\n",
      "epoch: 5 / 1000,step 39 / 355, loss: 2.7861\n",
      "HERG_cls: 0.2449; LogD: 0.6593; LogP: 0.6550; pKa: 0.1044; pKb: 0.1248; LogSol: 0.5012; wLogSol: 0.4964; \n",
      "epoch: 5 / 1000,step 79 / 355, loss: 2.8329\n",
      "HERG_cls: 0.2451; LogD: 0.6297; LogP: 0.6700; pKa: 0.2214; pKb: 0.1362; LogSol: 0.4690; wLogSol: 0.4614; \n",
      "epoch: 5 / 1000,step 119 / 355, loss: 2.6453\n",
      "HERG_cls: 0.1364; LogD: 0.6617; LogP: 0.6747; pKa: 0.0886; pKb: 0.0354; LogSol: 0.5232; wLogSol: 0.5254; \n",
      "epoch: 5 / 1000,step 159 / 355, loss: 2.9244\n",
      "HERG_cls: 0.3280; LogD: 0.6507; LogP: 0.6147; pKa: 0.1599; pKb: 0.1356; LogSol: 0.5181; wLogSol: 0.5175; \n",
      "epoch: 5 / 1000,step 199 / 355, loss: 2.7518\n",
      "HERG_cls: 0.2844; LogD: 0.6750; LogP: 0.6587; pKa: 0.0719; pKb: 0.0715; LogSol: 0.4946; wLogSol: 0.4958; \n",
      "epoch: 5 / 1000,step 239 / 355, loss: 2.8265\n",
      "HERG_cls: 0.1612; LogD: 0.6680; LogP: 0.6483; pKa: 0.2747; pKb: 0.0398; LogSol: 0.5185; wLogSol: 0.5159; \n",
      "epoch: 5 / 1000,step 279 / 355, loss: 3.1095\n",
      "HERG_cls: 0.3355; LogD: 0.6473; LogP: 0.6503; pKa: 0.2742; pKb: 0.1393; LogSol: 0.5318; wLogSol: 0.5311; \n",
      "epoch: 5 / 1000,step 319 / 355, loss: 2.8797\n",
      "HERG_cls: 0.1265; LogD: 0.6435; LogP: 0.6512; pKa: 0.1124; pKb: 0.2541; LogSol: 0.5447; wLogSol: 0.5474; \n",
      "evaluating...\n",
      "epoch: 5 / 1000, test AUC: 0.85485\n",
      "epoch: 5 end ; cost time: 1.6843 min\n",
      "Current learning rate is [5e-05, 5e-05, 5e-05, 0.001, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.conda/fcb/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 / 1000,step 39 / 355, loss: 2.6118\n",
      "HERG_cls: 0.1136; LogD: 0.6519; LogP: 0.6439; pKa: 0.1212; pKb: 0.0489; LogSol: 0.5171; wLogSol: 0.5152; \n",
      "epoch: 6 / 1000,step 79 / 355, loss: 2.7400\n",
      "HERG_cls: 0.0351; LogD: 0.6046; LogP: 0.6702; pKa: 0.1919; pKb: 0.1916; LogSol: 0.5272; wLogSol: 0.5195; \n",
      "epoch: 6 / 1000,step 119 / 355, loss: 2.6465\n",
      "HERG_cls: 0.1028; LogD: 0.6690; LogP: 0.6721; pKa: 0.0680; pKb: 0.0352; LogSol: 0.5497; wLogSol: 0.5497; \n",
      "epoch: 6 / 1000,step 159 / 355, loss: 3.0092\n",
      "HERG_cls: 0.2896; LogD: 0.6430; LogP: 0.6671; pKa: 0.2536; pKb: 0.1857; LogSol: 0.4848; wLogSol: 0.4854; \n",
      "epoch: 6 / 1000,step 199 / 355, loss: 2.6415\n",
      "HERG_cls: 0.1438; LogD: 0.6556; LogP: 0.6437; pKa: 0.0919; pKb: 0.0760; LogSol: 0.5165; wLogSol: 0.5140; \n",
      "epoch: 6 / 1000,step 239 / 355, loss: 2.8076\n",
      "HERG_cls: 0.2302; LogD: 0.6723; LogP: 0.6652; pKa: 0.2035; pKb: 0.0764; LogSol: 0.4793; wLogSol: 0.4807; \n",
      "epoch: 6 / 1000,step 279 / 355, loss: 3.1075\n",
      "HERG_cls: 0.3365; LogD: 0.6584; LogP: 0.6614; pKa: 0.1531; pKb: 0.1808; LogSol: 0.5555; wLogSol: 0.5618; \n",
      "epoch: 6 / 1000,step 319 / 355, loss: 2.7679\n",
      "HERG_cls: 0.1750; LogD: 0.6247; LogP: 0.6729; pKa: 0.1296; pKb: 0.1126; LogSol: 0.5266; wLogSol: 0.5265; \n",
      "evaluating...\n",
      "epoch: 6 / 1000, test AUC: 0.85482\n",
      "epoch: 6 end ; cost time: 1.6401 min\n",
      "Current learning rate is [5e-05, 5e-05, 5e-05, 0.001, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05].\n",
      "epoch: 7 / 1000,step 39 / 355, loss: 2.5177\n",
      "HERG_cls: 0.0742; LogD: 0.6498; LogP: 0.6377; pKa: 0.1459; pKb: 0.0874; LogSol: 0.4600; wLogSol: 0.4628; \n",
      "epoch: 7 / 1000,step 79 / 355, loss: 2.8440\n",
      "HERG_cls: 0.1511; LogD: 0.6483; LogP: 0.6607; pKa: 0.1715; pKb: 0.1898; LogSol: 0.5120; wLogSol: 0.5107; \n",
      "epoch: 7 / 1000,step 119 / 355, loss: 2.8688\n",
      "HERG_cls: 0.1183; LogD: 0.6688; LogP: 0.6686; pKa: 0.2209; pKb: 0.0543; LogSol: 0.5697; wLogSol: 0.5682; \n",
      "epoch: 7 / 1000,step 159 / 355, loss: 2.4945\n",
      "HERG_cls: 0.0830; LogD: 0.6276; LogP: 0.6599; pKa: 0.1539; pKb: 0.0374; LogSol: 0.4658; wLogSol: 0.4668; \n",
      "epoch: 7 / 1000,step 199 / 355, loss: 2.6301\n",
      "HERG_cls: 0.1950; LogD: 0.6437; LogP: 0.6671; pKa: 0.1981; pKb: 0.0257; LogSol: 0.4499; wLogSol: 0.4506; \n",
      "epoch: 7 / 1000,step 239 / 355, loss: 2.6703\n",
      "HERG_cls: 0.1596; LogD: 0.6643; LogP: 0.6729; pKa: 0.2005; pKb: 0.0540; LogSol: 0.4594; wLogSol: 0.4596; \n",
      "epoch: 7 / 1000,step 279 / 355, loss: 2.7212\n",
      "HERG_cls: 0.0631; LogD: 0.6822; LogP: 0.6656; pKa: 0.0911; pKb: 0.0285; LogSol: 0.5963; wLogSol: 0.5945; \n",
      "epoch: 7 / 1000,step 319 / 355, loss: 2.6134\n",
      "HERG_cls: 0.0777; LogD: 0.6643; LogP: 0.6773; pKa: 0.1064; pKb: 0.0745; LogSol: 0.5068; wLogSol: 0.5065; \n",
      "evaluating...\n",
      "epoch: 7 / 1000, test AUC: 0.85446\n",
      "epoch: 7 end ; cost time: 1.6422 min\n",
      "Current learning rate is [5e-05, 5e-05, 5e-05, 0.001, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05].\n",
      "epoch: 8 / 1000,step 39 / 355, loss: 2.7428\n",
      "HERG_cls: 0.1762; LogD: 0.6601; LogP: 0.6731; pKa: 0.1684; pKb: 0.1073; LogSol: 0.4787; wLogSol: 0.4790; \n",
      "epoch: 8 / 1000,step 79 / 355, loss: 2.6919\n",
      "HERG_cls: 0.1706; LogD: 0.6528; LogP: 0.6268; pKa: 0.1281; pKb: 0.0662; LogSol: 0.5259; wLogSol: 0.5215; \n",
      "epoch: 8 / 1000,step 119 / 355, loss: 2.7509\n",
      "HERG_cls: 0.0471; LogD: 0.6554; LogP: 0.6731; pKa: 0.2213; pKb: 0.1363; LogSol: 0.5085; wLogSol: 0.5092; \n",
      "epoch: 8 / 1000,step 159 / 355, loss: 2.6265\n",
      "HERG_cls: 0.1166; LogD: 0.6531; LogP: 0.6743; pKa: 0.1600; pKb: 0.0631; LogSol: 0.4798; wLogSol: 0.4796; \n",
      "epoch: 8 / 1000,step 199 / 355, loss: 2.8516\n",
      "HERG_cls: 0.0800; LogD: 0.6845; LogP: 0.6717; pKa: 0.2180; pKb: 0.2116; LogSol: 0.4921; wLogSol: 0.4939; \n",
      "epoch: 8 / 1000,step 239 / 355, loss: 2.7834\n",
      "HERG_cls: 0.1352; LogD: 0.6589; LogP: 0.6546; pKa: 0.1559; pKb: 0.1935; LogSol: 0.4931; wLogSol: 0.4921; \n",
      "epoch: 8 / 1000,step 279 / 355, loss: 2.5144\n",
      "HERG_cls: 0.0298; LogD: 0.6736; LogP: 0.6547; pKa: 0.1083; pKb: 0.0283; LogSol: 0.5120; wLogSol: 0.5077; \n",
      "epoch: 8 / 1000,step 319 / 355, loss: 2.7284\n",
      "HERG_cls: 0.1022; LogD: 0.6454; LogP: 0.6477; pKa: 0.1944; pKb: 0.1895; LogSol: 0.4737; wLogSol: 0.4756; \n",
      "evaluating...\n",
      "epoch: 8 / 1000, test AUC: 0.85536\n",
      "epoch: 8 end ; cost time: 1.6902 min\n",
      "Current learning rate is [5e-05, 5e-05, 5e-05, 0.001, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05].\n",
      "epoch: 9 / 1000,step 39 / 355, loss: 2.5139\n",
      "HERG_cls: 0.0139; LogD: 0.6775; LogP: 0.6536; pKa: 0.0625; pKb: 0.0379; LogSol: 0.5350; wLogSol: 0.5335; \n",
      "epoch: 9 / 1000,step 79 / 355, loss: 2.6150\n",
      "HERG_cls: 0.0890; LogD: 0.6709; LogP: 0.6704; pKa: 0.1259; pKb: 0.0995; LogSol: 0.4803; wLogSol: 0.4790; \n",
      "epoch: 9 / 1000,step 119 / 355, loss: 2.7820\n",
      "HERG_cls: 0.2080; LogD: 0.6463; LogP: 0.6609; pKa: 0.1189; pKb: 0.1416; LogSol: 0.5029; wLogSol: 0.5034; \n",
      "epoch: 9 / 1000,step 159 / 355, loss: 2.4761\n",
      "HERG_cls: 0.0592; LogD: 0.6492; LogP: 0.6294; pKa: 0.1172; pKb: 0.0643; LogSol: 0.4789; wLogSol: 0.4778; \n",
      "epoch: 9 / 1000,step 199 / 355, loss: 2.6074\n",
      "HERG_cls: 0.0748; LogD: 0.6619; LogP: 0.6799; pKa: 0.0697; pKb: 0.0813; LogSol: 0.5198; wLogSol: 0.5200; \n",
      "epoch: 9 / 1000,step 239 / 355, loss: 3.0781\n",
      "HERG_cls: 0.0951; LogD: 0.6901; LogP: 0.6595; pKa: 0.2413; pKb: 0.2540; LogSol: 0.5699; wLogSol: 0.5682; \n",
      "epoch: 9 / 1000,step 279 / 355, loss: 2.6533\n",
      "HERG_cls: 0.0865; LogD: 0.6809; LogP: 0.6612; pKa: 0.1499; pKb: 0.0751; LogSol: 0.4997; wLogSol: 0.5000; \n",
      "epoch: 9 / 1000,step 319 / 355, loss: 2.6526\n",
      "HERG_cls: 0.0750; LogD: 0.6769; LogP: 0.6633; pKa: 0.0934; pKb: 0.1182; LogSol: 0.5141; wLogSol: 0.5118; \n",
      "evaluating...\n",
      "epoch: 9 / 1000, test AUC: 0.86128\n",
      "epoch: 9 end ; cost time: 1.6870 min\n",
      "Current learning rate is [5e-05, 5e-05, 5e-05, 0.001, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05].\n",
      "epoch: 10 / 1000,step 39 / 355, loss: 2.5025\n",
      "HERG_cls: 0.0824; LogD: 0.6622; LogP: 0.6791; pKa: 0.0545; pKb: 0.0083; LogSol: 0.5064; wLogSol: 0.5096; \n",
      "epoch: 10 / 1000,step 79 / 355, loss: 2.5860\n",
      "HERG_cls: 0.0718; LogD: 0.6717; LogP: 0.6586; pKa: 0.1830; pKb: 0.0678; LogSol: 0.4663; wLogSol: 0.4667; \n",
      "epoch: 10 / 1000,step 119 / 355, loss: 2.5828\n",
      "HERG_cls: 0.0408; LogD: 0.6655; LogP: 0.6726; pKa: 0.1806; pKb: 0.0368; LogSol: 0.4937; wLogSol: 0.4928; \n",
      "epoch: 10 / 1000,step 159 / 355, loss: 2.6534\n",
      "HERG_cls: 0.0568; LogD: 0.6545; LogP: 0.6641; pKa: 0.1123; pKb: 0.2294; LogSol: 0.4722; wLogSol: 0.4641; \n",
      "epoch: 10 / 1000,step 199 / 355, loss: 2.5922\n",
      "HERG_cls: 0.0195; LogD: 0.6736; LogP: 0.6867; pKa: 0.1546; pKb: 0.0615; LogSol: 0.4990; wLogSol: 0.4973; \n",
      "epoch: 10 / 1000,step 239 / 355, loss: 2.6707\n",
      "HERG_cls: 0.1157; LogD: 0.6559; LogP: 0.6699; pKa: 0.1683; pKb: 0.0441; LogSol: 0.5083; wLogSol: 0.5086; \n",
      "epoch: 10 / 1000,step 279 / 355, loss: 2.5705\n",
      "HERG_cls: 0.2838; LogD: 0.6750; LogP: 0.6406; pKa: 0.0537; pKb: 0.0163; LogSol: 0.4510; wLogSol: 0.4502; \n",
      "epoch: 10 / 1000,step 319 / 355, loss: 2.4672\n",
      "HERG_cls: 0.1149; LogD: 0.6755; LogP: 0.6543; pKa: 0.0187; pKb: 0.0328; LogSol: 0.4859; wLogSol: 0.4851; \n",
      "evaluating...\n",
      "epoch: 10 / 1000, test AUC: 0.86071\n",
      "epoch: 10 end ; cost time: 1.6416 min\n",
      "Current learning rate is [5e-05, 5e-05, 5e-05, 0.001, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.conda/fcb/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 11 / 1000,step 39 / 355, loss: 2.5329\n",
      "HERG_cls: 0.0082; LogD: 0.6113; LogP: 0.6374; pKa: 0.1376; pKb: 0.1282; LogSol: 0.5055; wLogSol: 0.5047; \n",
      "epoch: 11 / 1000,step 79 / 355, loss: 2.6116\n",
      "HERG_cls: 0.1391; LogD: 0.6556; LogP: 0.6570; pKa: 0.0505; pKb: 0.0262; LogSol: 0.5420; wLogSol: 0.5412; \n",
      "epoch: 11 / 1000,step 119 / 355, loss: 2.6544\n",
      "HERG_cls: 0.0475; LogD: 0.6589; LogP: 0.6680; pKa: 0.2026; pKb: 0.1224; LogSol: 0.4767; wLogSol: 0.4783; \n",
      "epoch: 11 / 1000,step 159 / 355, loss: 2.5980\n",
      "HERG_cls: 0.1064; LogD: 0.6264; LogP: 0.6626; pKa: 0.1363; pKb: 0.1138; LogSol: 0.4762; wLogSol: 0.4763; \n",
      "epoch: 11 / 1000,step 199 / 355, loss: 2.5885\n",
      "HERG_cls: 0.0423; LogD: 0.6475; LogP: 0.6705; pKa: 0.2131; pKb: 0.0171; LogSol: 0.5021; wLogSol: 0.4959; \n",
      "epoch: 11 / 1000,step 239 / 355, loss: 2.4302\n",
      "HERG_cls: 0.0788; LogD: 0.6693; LogP: 0.6682; pKa: 0.0665; pKb: 0.0424; LogSol: 0.4526; wLogSol: 0.4523; \n",
      "epoch: 11 / 1000,step 279 / 355, loss: 2.9908\n",
      "HERG_cls: 0.2736; LogD: 0.6419; LogP: 0.6552; pKa: 0.2227; pKb: 0.1375; LogSol: 0.5309; wLogSol: 0.5291; \n",
      "epoch: 11 / 1000,step 319 / 355, loss: 2.7974\n",
      "HERG_cls: 0.0734; LogD: 0.6630; LogP: 0.6739; pKa: 0.1836; pKb: 0.2226; LogSol: 0.4861; wLogSol: 0.4947; \n",
      "evaluating...\n",
      "epoch: 11 / 1000, test AUC: 0.85618\n",
      "epoch: 11 end ; cost time: 1.6460 min\n",
      "Current learning rate is [4.946461621797824e-05, 4.946461621797824e-05, 4.946461621797824e-05, 0.000989084726566536, 4.946461621797824e-05, 4.946461621797824e-05, 4.946461621797824e-05, 4.946461621797824e-05, 4.946461621797824e-05].\n",
      "epoch: 12 / 1000,step 39 / 355, loss: 2.6197\n",
      "HERG_cls: 0.1051; LogD: 0.6637; LogP: 0.6781; pKa: 0.1270; pKb: 0.0668; LogSol: 0.4874; wLogSol: 0.4916; \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import gc\n",
    "from sklearn import metrics\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "# torch.autograd.set_detect_anomaly(True)\n",
    "CUDA_LAUNCH_BLOCKING=1\n",
    "\n",
    "\n",
    "# fixed random seed for reproduction\n",
    "seed = 10\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.enabled = False\n",
    "print('Random seed :', seed)\n",
    "\n",
    "from collections import OrderedDict\n",
    "@torch.no_grad()\n",
    "def update_ema(ema_model, model, decay=0.9999):\n",
    "    \"\"\"\n",
    "    Step the EMA model towards the current model.\n",
    "    \"\"\"\n",
    "    ema_params = OrderedDict(ema_model.named_parameters())\n",
    "    model_params = OrderedDict(model.named_parameters())\n",
    "\n",
    "    for name, param in model_params.items():\n",
    "        # TODO: Consider applying only to params that require_grad to avoid small numerical changes of pos_embed\n",
    "        ema_params[name].mul_(decay).add_(param.data, alpha=1 - decay)\n",
    "\n",
    "def requires_grad(model, flag=True):\n",
    "    \"\"\"\n",
    "    Set requires_grad flag for all parameters in a model.\n",
    "    \"\"\"\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = flag\n",
    "\n",
    "from sklearn import metrics\n",
    "def compute_AUC(y, pred, n_class=1):\n",
    "    # compute one score\n",
    "    if n_class == 1:\n",
    "        auc = metrics.roc_auc_score(y, pred)\n",
    "\n",
    "    # compute two-class\n",
    "    elif n_class == 2:\n",
    "        # pos = pred[:, 1]\n",
    "        auc = metrics.roc_auc_score(y, pred)\n",
    "    return auc\n",
    "\n",
    "def print_loss(loss, loss_name):\n",
    "    print(f\"{loss_name}: {loss.detach().cpu().numpy():.4f}; \", end='', flush=True)\n",
    "    # print('\\r', end='', flush=True)\n",
    "\n",
    "pad_len = 150 # best\n",
    "conf = 10 # best\n",
    "\n",
    "\n",
    "##########################################\n",
    "#########  construct dataloader  ######### \n",
    "##########################################\n",
    "from herg_cls_datapipeline import HERG_LMDBDataset, HERG_Multi_Class_LMDBDataset\n",
    "# train_dataset = HERG_LMDBDataset(conf=conf, pad_len=pad_len, mode=\"train\")\n",
    "train_dataset = HERG_Multi_Class_LMDBDataset(conf=conf, pad_len=pad_len, mode=\"train\")\n",
    "train_set = DataLoader(train_dataset,\n",
    "                    batch_size=16,  # 16 best\n",
    "                    drop_last=True,\n",
    "                    shuffle=True,\n",
    "                    pin_memory=True,\n",
    "                    num_workers=4,\n",
    "                    # collate_fn=train_dataset.collate_fn,\n",
    "                    worker_init_fn=train_dataset.worker_init_fn\n",
    "                    )\n",
    "\n",
    "val_dataset = HERG_LMDBDataset(conf=conf, pad_len=pad_len, mode=\"val\")\n",
    "val_set = DataLoader(val_dataset,\n",
    "                    batch_size=32,\n",
    "                    drop_last=False,\n",
    "                    shuffle=False,\n",
    "                    pin_memory=True,\n",
    "                    num_workers=4,\n",
    "                    # collate_fn=train_dataset.collate_fn,\n",
    "                    worker_init_fn=val_dataset.worker_init_fn\n",
    "                    )\n",
    "\n",
    "\n",
    "##########################################\n",
    "######  build model and optimizer  ####### \n",
    "##########################################\n",
    "dev = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "from CLIP import clip\n",
    "clip_model, preprocess = clip.load(name=\"ViT-B/16\", device=\"cpu\", download_root=\"/home/jovyan/clip_download_root\")\n",
    "from model_zoo import CLIP_Protein\n",
    "model = CLIP_Protein(clip_model, conf, pad_len=pad_len).to(dev)\n",
    "\n",
    "# path = \"/home/jovyan/prompts_learning/trained_weight/A_10_7_cliP_Epoch13_val_auc_0.90789.pth\"\n",
    "# sd = torch.load(path)\n",
    "# model.load_state_dict(sd)\n",
    "# print(\"pre-trained weights loaded...\")\n",
    "\n",
    "from copy import deepcopy\n",
    "ema = deepcopy(model).to(dev)  # Create an EMA of the model for use after training\n",
    "update_ema(ema, model, decay=0)  # Ensure EMA is initialized with synced weights\n",
    "requires_grad(ema, False)\n",
    "ema.eval()\n",
    "\n",
    "# # best\n",
    "# lr = 5e-5\n",
    "# wd = 0.\n",
    "\n",
    "# print(f'Set of Optimizer: lr:{lr}, weight_decay:{wd}')\n",
    "# model_params = [# {'params': model.parameters(), 'lr': lr},\n",
    "#                 {'params': model.atom_encoder.parameters(), 'lr': lr},\n",
    "#                 {'params': model.coor_encoder.parameters(), 'lr': lr},\n",
    "#                 {'params': model.fusion_blocks.parameters(), 'lr': lr},\n",
    "#                 {'params': model.head.parameters(), 'lr': 1e-3},\n",
    "#                 {'params': model.prompts_processor.parameters(), 'lr': lr},\n",
    "#                 {'params': model.ppim.parameters(), 'lr': lr},\n",
    "#                 # {'params': model.qformer.parameters(), 'lr':1e-5}\n",
    "#                ]\n",
    "\n",
    "# best\n",
    "lr = 5e-5\n",
    "wd = 0.\n",
    "\n",
    "print(f'Set of Optimizer: lr:{lr}, weight_decay:{wd}')\n",
    "model_params = [\n",
    "                {'params': model.atom_encoder.parameters(), 'lr': lr, \"weight_decay\": wd},\n",
    "                {'params': model.coor_encoder.parameters(), 'lr': lr, \"weight_decay\": wd},\n",
    "                {'params': model.fusion_blocks.parameters(), 'lr': lr, \"weight_decay\": wd},\n",
    "                \n",
    "                {'params': model.head.parameters(), 'lr': 1e-3, \"weight_decay\": wd},\n",
    "                \n",
    "                {'params': model.prompts_processor.parameters(), 'lr': 5e-5, \"weight_decay\": wd},\n",
    "                # {'params': model.ppim.parameters(), 'lr': lr},\n",
    "                {'params': model.ppim.model.parameters(), 'lr': 5e-5, \"weight_decay\": wd},\n",
    "                {'params': model.ppim.atom_emb.parameters(), 'lr': 5e-5, \"weight_decay\": wd},\n",
    "                {'params': model.ppim.label_emb.parameters(), 'lr': 5e-5, \"weight_decay\": wd},\n",
    "                {'params': model.ppim.attr_heads.parameters(), 'lr': 5e-5, \"weight_decay\": wd},\n",
    "               ]\n",
    "\n",
    "optims = 'adan'\n",
    "# optims = \"sgd\"\n",
    "if optims == 'adan':\n",
    "    from adan import Adan\n",
    "    optimizer = Adan(model_params, betas=(0.98, 0.92, 0.99),weight_decay=wd, max_grad_norm=0.)\n",
    "elif optims == 'sgd':\n",
    "    optimizer = optim.SGD(model_params, momentum=0.9, weight_decay=wd)\n",
    "elif optims == 'adamw':\n",
    "    optimizer = optim.AdamW(model_params, betas=(0.9, 0.999), weight_decay=wd)\n",
    "elif optims == 'adam':\n",
    "    optimizer = optim.Adam(model_params, betas=(0.9, 0.999), weight_decay=wd)\n",
    "print('Current Optimizer is', optims)\n",
    "\n",
    "\n",
    "###################################################\n",
    "########   build learning rate scheduler   ######## \n",
    "###################################################\n",
    "scheduler1 = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=1./3., total_iters=5) # best\n",
    "scheduler2 = torch.optim.lr_scheduler.ConstantLR(optimizer, factor=1., total_iters=5)\n",
    "scheduler3 = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=15, eta_min=1e-6)\n",
    "scheduler = torch.optim.lr_scheduler.SequentialLR(optimizer, schedulers=[scheduler1, scheduler2, scheduler3], milestones=[5, 10])\n",
    "cur_lr = scheduler.get_last_lr() \n",
    "print(f\"Current learning rate is {cur_lr}.\")\n",
    "\n",
    "\n",
    "##########################################\n",
    "########   build loss criterion   ######## \n",
    "##########################################\n",
    "# attr_loss = 'l1'\n",
    "# attr_loss = 'mse'\n",
    "attr_loss = 'bce'\n",
    "# attr_loss = 'ce'\n",
    "\n",
    "# red = 'sum'\n",
    "red = 'mean'\n",
    "\n",
    "print(f\"attribution loss is {attr_loss}, and reduction method is {red}.\")\n",
    "if attr_loss == 'l1':\n",
    "    # cri_attr = nn.L1Loss(reduction=red)\n",
    "    cri_attr = nn.SmoothL1Loss(reduction=red)\n",
    "elif attr_loss == 'mse':\n",
    "    cri_attr = nn.MSELoss(reduction=red)\n",
    "elif attr_loss == 'bce':\n",
    "    cri_attr = nn.BCEWithLogitsLoss(reduction=red)\n",
    "elif attr_loss == 'ce':\n",
    "    cri_attr = nn.CrossEntropyLoss(reduction=red)\n",
    "\n",
    "\n",
    "##########################################\n",
    "######## start training our model ######## \n",
    "##########################################\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "Epoch = 1000\n",
    "best_val = 0.85\n",
    "print(\"Let's start training!\")\n",
    "\n",
    "for e in range(0, Epoch):\n",
    "    start = time.time()\n",
    "    model.train()\n",
    "    for step_id, datas in enumerate(train_set):\n",
    "            # break\n",
    "            # print(datas[\"atoms\"].shape)\n",
    "            # print(datas[\"coordinate\"].shape)\n",
    "            # print(datas[\"label\"])\n",
    "            atoms = datas[\"atoms\"].to(dev, non_blocking=True).long()\n",
    "            # coord = datas[\"coordinate\"].to(dev, non_blocking=True).float()\n",
    "            pair = datas[\"distance\"].to(dev, non_blocking=True).float()\n",
    "            spd = datas[\"SPD\"].to(dev, non_blocking=True).float()\n",
    "            edge = datas[\"edge\"].to(dev, non_blocking=True).float()\n",
    "            label = datas[\"label\"].to(dev, non_blocking=True).float()\n",
    "            logd_gt = datas[\"logd\"].to(dev, non_blocking=True).float()\n",
    "            logp_gt = datas[\"logp\"].to(dev, non_blocking=True).float()\n",
    "            pka_gt = datas[\"pka\"].to(dev, non_blocking=True).float()\n",
    "            pkb_gt = datas[\"pkb\"].to(dev, non_blocking=True).float()\n",
    "            logsol_gt = datas[\"logsol\"].to(dev, non_blocking=True).float()\n",
    "            wlogsol_gt = datas[\"wlogsol\"].to(dev, non_blocking=True).float()\n",
    "            # label = torch.tanh(label)\n",
    "            \n",
    "            pred, attr_list = model(atoms, pair, spd, edge)\n",
    "            # pred = torch.tanh(pred)\n",
    "\n",
    "            # loss = cri_mae(pred, label.unsqueeze(-1))\n",
    "            \n",
    "            # weighted BCE loss\n",
    "            # loss = F.binary_cross_entropy_with_logits(pred, label.unsqueeze(-1))\n",
    "            loss = F.binary_cross_entropy_with_logits(pred, label.unsqueeze(-1), reduction='none')\n",
    "            for i, lab in enumerate(label):\n",
    "                if lab == 0:\n",
    "                    loss[i] = loss[i] * (2809./5687.)  # best\n",
    "                else:\n",
    "                    loss[i] = loss[i] * (2878./5687.)\n",
    "            loss_cls = loss.mean()\n",
    "            \n",
    "            # loss_pka = F.binary_cross_entropy_with_logits(attr_list[2], pka_gt.unsqueeze(-1), reduction=red)\n",
    "            # loss_pkb = F.binary_cross_entropy_with_logits(attr_list[3], pkb_gt.unsqueeze(-1), reduction=red)\n",
    "            loss_logd = cri_attr(attr_list[0], logd_gt.unsqueeze(-1))\n",
    "            loss_logp = cri_attr(attr_list[1], logp_gt.unsqueeze(-1))\n",
    "            loss_pka = cri_attr(attr_list[2], pka_gt.unsqueeze(-1))\n",
    "            loss_pkb = cri_attr(attr_list[3], pkb_gt.unsqueeze(-1))\n",
    "            loss_logsol = cri_attr(attr_list[4], logsol_gt.unsqueeze(-1))\n",
    "            loss_wlogsol = cri_attr(attr_list[5], wlogsol_gt.unsqueeze(-1))\n",
    "            # loss_logd = cri_attr(attr_list[0], logd_gt)\n",
    "            # loss_logp = cri_attr(attr_list[1], logp_gt)\n",
    "            # loss_logsol = cri_attr(attr_list[4], logsol_gt)\n",
    "            # loss_wlogsol = cri_attr(attr_list[5], wlogsol_gt)\n",
    "            loss_attr = loss_logd + loss_logp + loss_pka + loss_pkb + loss_logsol + loss_wlogsol\n",
    "            \n",
    "            loss = loss_cls + loss_attr\n",
    "            \n",
    "            \n",
    "            # CE loss\n",
    "            # loss = cri_ce(pred, label.long())\n",
    "            \n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            update_ema(ema, model, 0.997) # 0.997 best\n",
    "\n",
    "            if not (step_id+1) % 40:\n",
    "                print(f\"epoch: {e+1} / {Epoch},step {step_id} / {len(train_set)}, loss: {loss.detach().cpu().numpy():.4f}\")\n",
    "                print_loss(loss_cls, \"HERG_cls\")\n",
    "                print_loss(loss_logd, \"LogD\")\n",
    "                print_loss(loss_logp, \"LogP\")\n",
    "                print_loss(loss_pka, \"pKa\")\n",
    "                print_loss(loss_pkb, \"pKb\")\n",
    "                print_loss(loss_logsol, \"LogSol\")\n",
    "                print_loss(loss_wlogsol, \"wLogSol\")\n",
    "                print()\n",
    "                \n",
    "    \n",
    "    ##########################################\n",
    "    ####### start evaluating our model #######\n",
    "    ##########################################\n",
    "    model.eval()\n",
    "    print(\"evaluating...\")\n",
    "    with torch.no_grad():\n",
    "        all_pred = None \n",
    "        all_lab = None\n",
    "        for step_id, datas in enumerate(val_set):\n",
    "                atoms = datas[\"atoms\"].to(dev, non_blocking=True).long()\n",
    "                # coord = datas[\"coordinate\"].to(dev, non_blocking=True).float()\n",
    "                pair = datas[\"distance\"].to(dev, non_blocking=True).float()\n",
    "                spd = datas[\"SPD\"].to(dev, non_blocking=True).float()\n",
    "                edge = datas[\"edge\"].to(dev, non_blocking=True).float()\n",
    "                label = datas[\"label\"].to(dev, non_blocking=True).float()\n",
    "                # label = torch.tanh(label)\n",
    "                # pred = model(atoms, pair, spd, edge)\n",
    "                pred = ema(atoms, pair, spd, edge)\n",
    "                \n",
    "                pred = torch.sigmoid(pred)\n",
    "                # pred = torch.softmax(pred, dim=-1)[:, 1]\n",
    "                # total_loss += cri_mae(pred, label.unsqueeze(-1))\n",
    "                all_pred = pred if all_pred is None else torch.cat([all_pred, pred], dim=0)\n",
    "                all_lab = label if all_lab is None else torch.cat([all_lab, label], dim=0)\n",
    "    auc = compute_AUC(all_lab.cpu().detach(), all_pred.cpu().detach())\n",
    "    print(f\"epoch: {e+1} / {Epoch}, test AUC: {auc:.5f}\")\n",
    "    if auc > best_val:\n",
    "        best_val = auc\n",
    "        # torch.save(model.state_dict(), f'./trained_weight/cliP_Epoch{e+1}_val_auc_{best_val:.5f}.pth') \n",
    "        torch.save(ema.state_dict(), f'./trained_weight/HERG_cliPm_Epoch{e+1}_val_auc_{best_val:.5f}.pth') \n",
    "\n",
    "\n",
    "    end = time.time()\n",
    "    print(f\"epoch: {e+1} end ; cost time: {(end - start)/60.:.4f} min\")\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    scheduler.step()\n",
    "    cur_lr = scheduler.get_last_lr() \n",
    "    print(f\"Current learning rate is {cur_lr}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdc6b18-bdab-4e34-8c17-6ca69275b138",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a2fabe-ed81-47e4-b3a8-2f3c5fce7758",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fcb",
   "language": "python",
   "name": "fcb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
