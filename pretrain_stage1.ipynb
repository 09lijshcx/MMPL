{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f04f710-7b5a-4af2-9270-6836a034e37c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import gc\n",
    "from sklearn import metrics\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "# torch.autograd.set_detect_anomaly(True)\n",
    "CUDA_LAUNCH_BLOCKING=1\n",
    "\n",
    "# fixed random seed for reproduction\n",
    "seed = 10\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.enabled = False\n",
    "print('Random seed :', seed)\n",
    "\n",
    "from collections import OrderedDict\n",
    "@torch.no_grad()\n",
    "def update_ema(ema_model, model, decay=0.9999):\n",
    "    \"\"\"\n",
    "    Step the EMA model towards the current model.\n",
    "    \"\"\"\n",
    "    ema_params = OrderedDict(ema_model.named_parameters())\n",
    "    model_params = OrderedDict(model.named_parameters())\n",
    "\n",
    "    for name, param in model_params.items():\n",
    "        # TODO: Consider applying only to params that require_grad to avoid small numerical changes of pos_embed\n",
    "        ema_params[name].mul_(decay).add_(param.data, alpha=1 - decay)\n",
    "\n",
    "def requires_grad(model, flag=True):\n",
    "    \"\"\"\n",
    "    Set requires_grad flag for all parameters in a model.\n",
    "    \"\"\"\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = flag\n",
    "\n",
    "from sklearn import metrics\n",
    "def compute_AUC(y, pred, n_class=1):\n",
    "    # compute one score\n",
    "    if n_class == 1:\n",
    "        auc = metrics.roc_auc_score(y, pred)\n",
    "\n",
    "    # compute two-class\n",
    "    elif n_class == 2:\n",
    "        # pos = pred[:, 1]\n",
    "        auc = metrics.roc_auc_score(y, pred)\n",
    "    return auc\n",
    "\n",
    "\n",
    "def print_loss(loss, loss_name):\n",
    "    print(f\"{loss_name}: {loss.detach().cpu().numpy():.4f}; \", end='', flush=True)\n",
    "    # print('\\r', end='', flush=True)\n",
    "\n",
    "    \n",
    "pad_len = 150 # best\n",
    "conf = 10 # best# \n",
    "\n",
    "##########################################\n",
    "#########  construct dataloader  ######### \n",
    "##########################################\n",
    "train_bz = 64\n",
    " \n",
    "from pretrain_datapipeline import Pretrain_LMDBDataset\n",
    "train_dataset = Pretrain_LMDBDataset(conf=conf, pad_len=pad_len, mode=\"logd\")\n",
    "train_set = DataLoader(train_dataset,\n",
    "                    batch_size=train_bz,  # 16 best\n",
    "                    drop_last=True,\n",
    "                    shuffle=True,\n",
    "                    pin_memory=True,\n",
    "                    num_workers=4,\n",
    "                    # collate_fn=train_dataset.collate_fn,\n",
    "                    worker_init_fn=train_dataset.worker_init_fn\n",
    "                    )\n",
    "\n",
    "train_dataset3 = Pretrain_LMDBDataset(conf=conf, pad_len=pad_len, mode=\"herg\")\n",
    "train_set3 = DataLoader(train_dataset3,\n",
    "                    batch_size=train_bz,  # 16 best\n",
    "                    drop_last=True,\n",
    "                    shuffle=True,\n",
    "                    pin_memory=True,\n",
    "                    num_workers=4,\n",
    "                    # collate_fn=train_dataset.collate_fn,\n",
    "                    worker_init_fn=train_dataset3.worker_init_fn\n",
    "                    )\n",
    "\n",
    "train_dataset2 = Pretrain_LMDBDataset(conf=conf, pad_len=pad_len, mode=\"bbb\")\n",
    "train_set2 = DataLoader(train_dataset2,\n",
    "                    batch_size=train_bz,  # 16 best\n",
    "                    drop_last=True,\n",
    "                    shuffle=True,\n",
    "                    pin_memory=True,\n",
    "                    num_workers=4,\n",
    "                    # collate_fn=train_dataset.collate_fn,\n",
    "                    worker_init_fn=train_dataset2.worker_init_fn\n",
    "                    )\n",
    "\n",
    "val_dataset = Pretrain_LMDBDataset(conf=conf, pad_len=pad_len, mode=\"bbb_test\")\n",
    "val_set = DataLoader(val_dataset,\n",
    "                    batch_size=32,\n",
    "                    drop_last=False,\n",
    "                    shuffle=False,\n",
    "                    pin_memory=True,\n",
    "                    num_workers=4,\n",
    "                    # collate_fn=train_dataset.collate_fn,\n",
    "                    worker_init_fn=val_dataset.worker_init_fn\n",
    "                    )\n",
    "\n",
    "\n",
    "##########################################\n",
    "######  build model and optimizer  ####### \n",
    "##########################################\n",
    "dev = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "from CLIP import clip\n",
    "clip_model, preprocess = clip.load(name=\"ViT-B/16\", device=\"cpu\", download_root=\"/home/jovyan/clip_download_root\")\n",
    "from model_zoo import CLIPM_Stage1\n",
    "model = CLIPM_Stage1(clip_model, conf, pad_len=pad_len).to(dev)\n",
    "\n",
    "\n",
    "from copy import deepcopy\n",
    "ema = deepcopy(model).to(dev)  # Create an EMA of the model for use after training\n",
    "update_ema(ema, model, decay=0)  # Ensure EMA is initialized with synced weights\n",
    "requires_grad(ema, False)\n",
    "ema.eval()\n",
    "\n",
    "# best\n",
    "lr = 5e-5\n",
    "wd = 0.\n",
    "\n",
    "print(f'Set of Optimizer: lr:{lr}, weight_decay:{wd}')\n",
    "model_params = [\n",
    "    {'params': model.parameters(), 'lr': lr},\n",
    "                # {'params': model.atom_encoder.parameters(), 'lr': lr},\n",
    "                # {'params': model.coor_encoder.parameters(), 'lr': lr},\n",
    "                # {'params': model.fusion_blocks.parameters(), 'lr': lr},\n",
    "                # {'params': model.pretrain_head1.parameters(), 'lr': lr},\n",
    "               ]\n",
    "\n",
    "\n",
    "optims = 'adan'\n",
    "# optims = \"sgd\"\n",
    "if optims == 'adan':\n",
    "    from adan import Adan\n",
    "    optimizer = Adan(model_params, betas=(0.98, 0.92, 0.99),weight_decay=wd, max_grad_norm=0.)\n",
    "elif optims == 'sgd':\n",
    "    optimizer = optim.SGD(model_params, momentum=0.9, weight_decay=wd)\n",
    "elif optims == 'adamw':\n",
    "    optimizer = optim.AdamW(model_params, betas=(0.9, 0.999), weight_decay=wd)\n",
    "elif optims == 'adam':\n",
    "    optimizer = optim.Adam(model_params, betas=(0.9, 0.999), weight_decay=wd)\n",
    "print('Current Optimizer is', optims)\n",
    "\n",
    "\n",
    "###################################################\n",
    "########   build learning rate scheduler   ######## \n",
    "###################################################\n",
    "# scheduler1 = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=1./3., total_iters=5) # best\n",
    "# scheduler2 = torch.optim.lr_scheduler.ConstantLR(optimizer, factor=1., total_iters=95)\n",
    "# scheduler3 = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=15, eta_min=1e-6)\n",
    "# scheduler = torch.optim.lr_scheduler.SequentialLR(optimizer, schedulers=[scheduler1, scheduler2, scheduler3], milestones=[5, 100])\n",
    "# cur_lr = scheduler.get_last_lr() \n",
    "# print(f\"Current learning rate is {cur_lr}.\")\n",
    "\n",
    "\n",
    "\n",
    "##########################################\n",
    "######## start training our model ######## \n",
    "##########################################\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "Epoch = 100\n",
    "print(\"Let's start training!\")\n",
    "all_len = len(train_set)+len(train_set2)+len(train_set3)\n",
    "for e in range(0, Epoch):\n",
    "    start = time.time()\n",
    "    model.train()\n",
    "    for step_id, datas in enumerate(train_set):\n",
    "            atoms = datas[\"atoms\"].to(dev, non_blocking=True).long()\n",
    "            coord = datas[\"coordinate\"].to(dev, non_blocking=True).float()\n",
    "            pair = datas[\"distance\"].to(dev, non_blocking=True).float()\n",
    "            spd = datas[\"SPD\"].to(dev, non_blocking=True).float()\n",
    "            edge = datas[\"edge\"].to(dev, non_blocking=True).float()\n",
    "            masked_label = datas[\"masked_label\"].to(dev, non_blocking=True).long()\n",
    "            ids_masked = datas[\"ids_masked\"].to(dev, non_blocking=True).long()\n",
    "            n_gt = datas[\"noise_gt\"].to(dev, non_blocking=True).float()\n",
    "            \n",
    "            pre_atom, pre_dist = model(atoms, pair, spd, edge, ids_masked)\n",
    "            loss_atom = F.cross_entropy(pre_atom.reshape(-1, 21), masked_label.reshape(-1))\n",
    "            loss_dist = F.mse_loss(pre_dist.permute(1, 0, 2), coord)\n",
    "            loss = loss_atom + loss_dist\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            update_ema(ema, model, 0.997) # 0.997 best\n",
    "\n",
    "            if not (step_id+1) % 40:\n",
    "                # print(f\"epoch: {e+1} / {Epoch},step {step_id} / {len(train_set)}, loss: {loss.detach().cpu().numpy():.4f}\")\n",
    "                # print(f\"epoch: {e+1} / {Epoch},step {step_id} / {len(train_set)+len(train_set2)}, loss: {loss.detach().cpu().numpy():.4f}\")\n",
    "                print(f\"epoch: {e+1} / {Epoch},step {step_id} / {all_len}, loss: {loss.detach().cpu().numpy():.4f}\")\n",
    "                print_loss(loss_atom, \"Atom\")\n",
    "                print_loss(loss_dist, \"Dist\")\n",
    "                print()\n",
    "    \n",
    "    for step_id, datas in enumerate(train_set2):\n",
    "            atoms = datas[\"atoms\"].to(dev, non_blocking=True).long()\n",
    "            coord = datas[\"coordinate\"].to(dev, non_blocking=True).float()\n",
    "            pair = datas[\"distance\"].to(dev, non_blocking=True).float()\n",
    "            spd = datas[\"SPD\"].to(dev, non_blocking=True).float()\n",
    "            edge = datas[\"edge\"].to(dev, non_blocking=True).float()\n",
    "            masked_label = datas[\"masked_label\"].to(dev, non_blocking=True).long()\n",
    "            ids_masked = datas[\"ids_masked\"].to(dev, non_blocking=True).long()\n",
    "            n_gt = datas[\"noise_gt\"].to(dev, non_blocking=True).float()\n",
    "            \n",
    "            pre_atom, pre_dist = model(atoms, pair, spd, edge, ids_masked)\n",
    "            loss_atom = F.cross_entropy(pre_atom.reshape(-1, 21), masked_label.reshape(-1))\n",
    "            loss_dist = F.mse_loss(pre_dist.permute(1, 0, 2), coord)\n",
    "            loss = loss_atom + loss_dist\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            update_ema(ema, model, 0.997) # 0.997 best\n",
    "\n",
    "            if not (step_id+1) % 40:\n",
    "                print(f\"epoch: {e+1} / {Epoch},step {step_id+len(train_set)} / {all_len}, loss: {loss.detach().cpu().numpy():.4f}\")\n",
    "                print_loss(loss_atom, \"Atom\")\n",
    "                print_loss(loss_dist, \"Dist\")\n",
    "                print()\n",
    "                \n",
    "    for step_id, datas in enumerate(train_set3):\n",
    "            atoms = datas[\"atoms\"].to(dev, non_blocking=True).long()\n",
    "            coord = datas[\"coordinate\"].to(dev, non_blocking=True).float()\n",
    "            pair = datas[\"distance\"].to(dev, non_blocking=True).float()\n",
    "            spd = datas[\"SPD\"].to(dev, non_blocking=True).float()\n",
    "            edge = datas[\"edge\"].to(dev, non_blocking=True).float()\n",
    "            masked_label = datas[\"masked_label\"].to(dev, non_blocking=True).long()\n",
    "            ids_masked = datas[\"ids_masked\"].to(dev, non_blocking=True).long()\n",
    "            n_gt = datas[\"noise_gt\"].to(dev, non_blocking=True).float()\n",
    "            \n",
    "            pre_atom, pre_dist = model(atoms, pair, spd, edge, ids_masked)\n",
    "            loss_atom = F.cross_entropy(pre_atom.reshape(-1, 21), masked_label.reshape(-1))\n",
    "            loss_dist = F.mse_loss(pre_dist.permute(1, 0, 2), coord)\n",
    "            loss = loss_atom + loss_dist\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            update_ema(ema, model, 0.997) # 0.997 best\n",
    "\n",
    "            if not (step_id+1) % 40:\n",
    "                print(f\"epoch: {e+1} / {Epoch},step {step_id+len(train_set)+len(train_set2)} / {all_len}, loss: {loss.detach().cpu().numpy():.4f}\")\n",
    "                print_loss(loss_atom, \"Atom\")\n",
    "                print_loss(loss_dist, \"Dist\")\n",
    "                print()\n",
    "                \n",
    "    \n",
    "    ##########################################\n",
    "    ####### start evaluating our model #######\n",
    "    ##########################################\n",
    "    model.eval()\n",
    "    print(\"evaluating...\")\n",
    "    with torch.no_grad():\n",
    "        all_atom_loss = None \n",
    "        all_dist_loss = None\n",
    "        for step_id, datas in enumerate(val_set):\n",
    "                atoms = datas[\"atoms\"].to(dev, non_blocking=True).long()\n",
    "                coord = datas[\"coordinate\"].to(dev, non_blocking=True).float()\n",
    "                pair = datas[\"distance\"].to(dev, non_blocking=True).float()\n",
    "                spd = datas[\"SPD\"].to(dev, non_blocking=True).float()\n",
    "                edge = datas[\"edge\"].to(dev, non_blocking=True).float()\n",
    "                masked_label = datas[\"masked_label\"].to(dev, non_blocking=True).long()\n",
    "                ids_masked = datas[\"ids_masked\"].to(dev, non_blocking=True).long()\n",
    "                n_gt = datas[\"noise_gt\"].to(dev, non_blocking=True).float()\n",
    "                \n",
    "                pre_atom, pre_dist = model(atoms, pair, spd, edge, ids_masked)\n",
    "                loss_atom = F.cross_entropy(pre_atom.reshape(-1, 21), masked_label.reshape(-1)).unsqueeze(0)\n",
    "                loss_dist = F.mse_loss(pre_dist.permute(1, 0, 2), coord).unsqueeze(0)\n",
    "                \n",
    "                all_atom_loss = loss_atom if all_atom_loss is None else torch.cat([all_atom_loss, loss_atom], dim=0)\n",
    "                all_dist_loss = loss_dist if all_dist_loss is None else torch.cat([all_dist_loss, loss_dist], dim=0)\n",
    "                \n",
    "    all_atom_loss = all_atom_loss.mean()\n",
    "    all_dist_loss = all_dist_loss.mean()\n",
    "    print(f\"epoch: {e+1} / {Epoch}, test atom loss: {all_atom_loss:.5f}; test dist loss: {all_dist_loss:.5f}\")\n",
    "    if not (e+1) % 1:\n",
    "        check_point = {\n",
    "            'atom_encoder': model.atom_encoder.state_dict(),\n",
    "            'coor_encoder': model.coor_encoder.state_dict(),\n",
    "            'fusion_blocks': model.fusion_blocks.state_dict(),\n",
    "            'pretrain_head1': model.pretrain_head1.state_dict(),\n",
    "            'pretrain_head2': model.pretrain_head2.state_dict(),\n",
    "        }\n",
    "        \n",
    "        torch.save(check_point, f'./trained_weight/cliPM_pre_stage1_Epoch{e+1}.pth') \n",
    "\n",
    "\n",
    "    end = time.time()\n",
    "    print(f\"epoch: {e+1} end ; cost time: {(end - start)/60.:.4f} min\")\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    # scheduler.step()\n",
    "    # cur_lr = scheduler.get_last_lr() \n",
    "    # print(f\"Current learning rate is {cur_lr}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ffce76-7c56-45a6-8df6-1daad212fe11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fcb",
   "language": "python",
   "name": "fcb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
